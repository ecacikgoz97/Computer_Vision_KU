{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5f1acb",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by Comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cffde0e",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5427248",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b086c7",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38856cc1",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51825095",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c073405",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da85fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        \n",
    "        self.W_O = nn.Linear(self.proj_dims,self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        attn_out = None\n",
    "        \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ## Compute attention logits. Note that this operation is implemented as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        h = q.shape[1]\n",
    "        #print(f\"B: {b}, H: {h}, N: {n}, D: {d}\")\n",
    "        attention_logits = (q @ k.transpose(-2, -1)) * (1/np.sqrt(b//h))       \n",
    "        #print(f\"attention_logits: {attention_logits.shape}\")\n",
    "        \n",
    "        ## Compute attention Weights. Recall that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        normalized_attention_logits = F.softmax(attention_logits, dim=-1)\n",
    "        #print(f\"normalized_attention_logits: {normalized_attention_logits.shape}\")\n",
    "        \n",
    "        ## Compute attention output values. Bear in mind that this operation is applied as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        ##       (or any other equivalent torch operations)\n",
    "        \n",
    "        #print(f\"V: {v.shape}\")\n",
    "        attention_output_logits = normalized_attention_logits @ v\n",
    "        #print(f\"attention_output_logits: {attention_output_logits.shape}\")\n",
    "        attention_output_logits = attention_output_logits.permute(0, 2, 1, 3).reshape(b, n, -1)\n",
    "        #print(f\"attention_output_logits: {attention_output_logits.shape}\")\n",
    "        \n",
    "        ## Compute output feature map. This operation is just passing the concatenated attention \n",
    "        ## output that we have just obtained through a final projection layer W_O.\n",
    "        ## Both the input and the output should be of size [B, N, D]\n",
    "        \n",
    "        attn_out = self.W_O(attention_output_logits)\n",
    "        #print(f\"attn_out: {attn_out.shape}\")\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704e058",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9362cf31",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93873da6",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51dc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.gelu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29786cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.attention_norm_layer = nn.LayerNorm(hidden_dims)\n",
    "        self.multihead_attention  = SelfAttention(hidden_dims, head_dims=32, num_heads=num_heads,  bias=False)\n",
    "        self.mlp_norm_layer       = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp                  = MLP(hidden_dims, hidden_dims*4, hidden_dims, bias=True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        res1 = x\n",
    "        #print(f\"res1: {res1.shape}\")\n",
    "        x = self.attention_norm_layer(x)\n",
    "        #print(f\"attention_norm_layer: {x.shape}\")\n",
    "        x = self.multihead_attention(x)\n",
    "        #print(f\"multihead_attention: {x.shape}\")\n",
    "        x = x + res1\n",
    "        #print(f\"x: {x.shape}\")\n",
    "        \n",
    "        res2 = x\n",
    "        #print(f\"res2: {res2.shape}\")\n",
    "        x = self.mlp_norm_layer(x)\n",
    "        #print(f\"mlp_norm_layer: {x.shape}\")\n",
    "        x = self.mlp(x)\n",
    "        #print(f\"mlp: {x.shape}\")\n",
    "        x = x + res2\n",
    "        #print(f\"out: {x.shape}\")\n",
    "        return x\n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a8b8f",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f275004",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432192b4",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb9b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3907e",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c588dd4",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66a49c",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77609f17",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335c31fd",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d33937",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba8e6e1",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cce17",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b492bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f00ef9",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ebc65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcfc4173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 2.960 | Acc: 8.000% (2/25)\n",
      "Loss: 4.362 | Acc: 6.000% (3/50)\n",
      "Loss: 4.136 | Acc: 10.667% (8/75)\n",
      "Loss: 4.041 | Acc: 10.000% (10/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 10.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 4.693 | Acc: 8.000% (2/25)\n",
      "Loss: 4.698 | Acc: 10.000% (5/50)\n",
      "Loss: 4.532 | Acc: 13.333% (10/75)\n",
      "Loss: 4.712 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 4.411 | Acc: 8.000% (2/25)\n",
      "Loss: 3.688 | Acc: 12.000% (6/50)\n",
      "Loss: 3.299 | Acc: 10.667% (8/75)\n",
      "Loss: 3.188 | Acc: 11.000% (11/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 11.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.376 | Acc: 28.000% (7/25)\n",
      "Loss: 2.488 | Acc: 22.000% (11/50)\n",
      "Loss: 2.653 | Acc: 20.000% (15/75)\n",
      "Loss: 2.662 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.266 | Acc: 24.000% (6/25)\n",
      "Loss: 2.228 | Acc: 28.000% (14/50)\n",
      "Loss: 2.189 | Acc: 28.000% (21/75)\n",
      "Loss: 2.110 | Acc: 31.000% (31/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 31.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.405 | Acc: 20.000% (5/25)\n",
      "Loss: 2.459 | Acc: 16.000% (8/50)\n",
      "Loss: 2.465 | Acc: 13.333% (10/75)\n",
      "Loss: 2.419 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.846 | Acc: 28.000% (7/25)\n",
      "Loss: 1.757 | Acc: 42.000% (21/50)\n",
      "Loss: 1.779 | Acc: 38.667% (29/75)\n",
      "Loss: 1.753 | Acc: 45.000% (45/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 45.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.074 | Acc: 20.000% (5/25)\n",
      "Loss: 2.174 | Acc: 26.000% (13/50)\n",
      "Loss: 2.235 | Acc: 22.667% (17/75)\n",
      "Loss: 2.219 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.267 | Acc: 60.000% (15/25)\n",
      "Loss: 1.149 | Acc: 68.000% (34/50)\n",
      "Loss: 1.208 | Acc: 64.000% (48/75)\n",
      "Loss: 1.191 | Acc: 66.000% (66/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 66.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.233 | Acc: 28.000% (7/25)\n",
      "Loss: 2.208 | Acc: 28.000% (14/50)\n",
      "Loss: 2.327 | Acc: 24.000% (18/75)\n",
      "Loss: 2.348 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 0.833 | Acc: 76.000% (19/25)\n",
      "Loss: 0.846 | Acc: 70.000% (35/50)\n",
      "Loss: 0.910 | Acc: 69.333% (52/75)\n",
      "Loss: 0.913 | Acc: 71.000% (71/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 71.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.461 | Acc: 24.000% (6/25)\n",
      "Loss: 2.363 | Acc: 24.000% (12/50)\n",
      "Loss: 2.434 | Acc: 22.667% (17/75)\n",
      "Loss: 2.448 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 0.597 | Acc: 88.000% (22/25)\n",
      "Loss: 0.646 | Acc: 86.000% (43/50)\n",
      "Loss: 0.642 | Acc: 85.333% (64/75)\n",
      "Loss: 0.620 | Acc: 83.000% (83/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 83.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.399 | Acc: 24.000% (6/25)\n",
      "Loss: 2.583 | Acc: 24.000% (12/50)\n",
      "Loss: 2.717 | Acc: 22.667% (17/75)\n",
      "Loss: 2.707 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.319 | Acc: 88.000% (22/25)\n",
      "Loss: 0.433 | Acc: 84.000% (42/50)\n",
      "Loss: 0.372 | Acc: 89.333% (67/75)\n",
      "Loss: 0.349 | Acc: 91.000% (91/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 91.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.573 | Acc: 20.000% (5/25)\n",
      "Loss: 2.529 | Acc: 20.000% (10/50)\n",
      "Loss: 2.529 | Acc: 20.000% (15/75)\n",
      "Loss: 2.605 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.167 | Acc: 100.000% (25/25)\n",
      "Loss: 0.174 | Acc: 100.000% (50/50)\n",
      "Loss: 0.151 | Acc: 100.000% (75/75)\n",
      "Loss: 0.146 | Acc: 100.000% (100/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.470 | Acc: 28.000% (7/25)\n",
      "Loss: 2.624 | Acc: 28.000% (14/50)\n",
      "Loss: 2.692 | Acc: 26.667% (20/75)\n",
      "Loss: 2.698 | Acc: 26.000% (26/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 26.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.119 | Acc: 100.000% (25/25)\n",
      "Loss: 0.092 | Acc: 100.000% (50/50)\n",
      "Loss: 0.098 | Acc: 100.000% (75/75)\n",
      "Loss: 0.089 | Acc: 100.000% (100/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.564 | Acc: 20.000% (5/25)\n",
      "Loss: 2.806 | Acc: 20.000% (10/50)\n",
      "Loss: 2.933 | Acc: 21.333% (16/75)\n",
      "Loss: 2.913 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.046 | Acc: 100.000% (25/25)\n",
      "Loss: 0.043 | Acc: 100.000% (50/50)\n",
      "Loss: 0.042 | Acc: 100.000% (75/75)\n",
      "Loss: 0.038 | Acc: 100.000% (100/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.736 | Acc: 20.000% (5/25)\n",
      "Loss: 2.915 | Acc: 20.000% (10/50)\n",
      "Loss: 3.041 | Acc: 22.667% (17/75)\n",
      "Loss: 2.987 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.021 | Acc: 100.000% (25/25)\n",
      "Loss: 0.022 | Acc: 100.000% (50/50)\n",
      "Loss: 0.023 | Acc: 100.000% (75/75)\n",
      "Loss: 0.023 | Acc: 100.000% (100/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.865 | Acc: 16.000% (4/25)\n",
      "Loss: 2.974 | Acc: 18.000% (9/50)\n",
      "Loss: 3.080 | Acc: 20.000% (15/75)\n",
      "Loss: 3.013 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.011 | Acc: 100.000% (25/25)\n",
      "Loss: 0.016 | Acc: 100.000% (50/50)\n",
      "Loss: 0.015 | Acc: 100.000% (75/75)\n",
      "Loss: 0.015 | Acc: 100.000% (100/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.898 | Acc: 20.000% (5/25)\n",
      "Loss: 3.010 | Acc: 20.000% (10/50)\n",
      "Loss: 3.102 | Acc: 20.000% (15/75)\n",
      "Loss: 3.026 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.018 | Acc: 100.000% (25/25)\n",
      "Loss: 0.013 | Acc: 100.000% (50/50)\n",
      "Loss: 0.011 | Acc: 100.000% (75/75)\n",
      "Loss: 0.011 | Acc: 100.000% (100/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.948 | Acc: 20.000% (5/25)\n",
      "Loss: 3.079 | Acc: 20.000% (10/50)\n",
      "Loss: 3.164 | Acc: 18.667% (14/75)\n",
      "Loss: 3.075 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.010 | Acc: 100.000% (25/25)\n",
      "Loss: 0.009 | Acc: 100.000% (50/50)\n",
      "Loss: 0.008 | Acc: 100.000% (75/75)\n",
      "Loss: 0.008 | Acc: 100.000% (100/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.012 | Acc: 28.000% (7/25)\n",
      "Loss: 3.180 | Acc: 24.000% (12/50)\n",
      "Loss: 3.265 | Acc: 22.667% (17/75)\n",
      "Loss: 3.159 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Final train set accuracy is 100.0\n",
      "Final val set accuracy is 22.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers=num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, \n",
    "            bias=False).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e046e1e",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c02695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 4.624 | Acc: 3.125% (2/64)\n",
      "Loss: 4.849 | Acc: 4.688% (6/128)\n",
      "Loss: 4.562 | Acc: 7.292% (14/192)\n",
      "Loss: 4.581 | Acc: 7.812% (20/256)\n",
      "Loss: 4.197 | Acc: 9.375% (30/320)\n",
      "Loss: 4.044 | Acc: 9.115% (35/384)\n",
      "Loss: 3.892 | Acc: 9.821% (44/448)\n",
      "Loss: 3.749 | Acc: 10.547% (54/512)\n",
      "Loss: 3.625 | Acc: 11.111% (64/576)\n",
      "Loss: 3.514 | Acc: 12.031% (77/640)\n",
      "Loss: 3.397 | Acc: 13.210% (93/704)\n",
      "Loss: 3.307 | Acc: 13.932% (107/768)\n",
      "Loss: 3.252 | Acc: 14.423% (120/832)\n",
      "Loss: 3.180 | Acc: 14.732% (132/896)\n",
      "Loss: 3.104 | Acc: 15.208% (146/960)\n",
      "Loss: 3.060 | Acc: 15.039% (154/1024)\n",
      "Loss: 3.020 | Acc: 15.257% (166/1088)\n",
      "Loss: 2.977 | Acc: 15.538% (179/1152)\n",
      "Loss: 2.954 | Acc: 15.872% (193/1216)\n",
      "Loss: 2.924 | Acc: 15.938% (204/1280)\n",
      "Loss: 2.895 | Acc: 16.146% (217/1344)\n",
      "Loss: 2.858 | Acc: 16.832% (237/1408)\n",
      "Loss: 2.837 | Acc: 16.984% (250/1472)\n",
      "Loss: 2.811 | Acc: 16.992% (261/1536)\n",
      "Loss: 2.786 | Acc: 17.312% (277/1600)\n",
      "Loss: 2.766 | Acc: 17.728% (295/1664)\n",
      "Loss: 2.749 | Acc: 17.535% (303/1728)\n",
      "Loss: 2.727 | Acc: 17.411% (312/1792)\n",
      "Loss: 2.707 | Acc: 17.511% (325/1856)\n",
      "Loss: 2.691 | Acc: 17.552% (337/1920)\n",
      "Loss: 2.672 | Acc: 17.692% (351/1984)\n",
      "Loss: 2.650 | Acc: 17.969% (368/2048)\n",
      "Loss: 2.634 | Acc: 17.945% (379/2112)\n",
      "Loss: 2.622 | Acc: 18.107% (394/2176)\n",
      "Loss: 2.614 | Acc: 18.125% (406/2240)\n",
      "Loss: 2.609 | Acc: 18.099% (417/2304)\n",
      "Loss: 2.597 | Acc: 18.201% (431/2368)\n",
      "Loss: 2.583 | Acc: 18.298% (445/2432)\n",
      "Loss: 2.576 | Acc: 18.269% (456/2496)\n",
      "Loss: 2.566 | Acc: 18.555% (475/2560)\n",
      "Loss: 2.553 | Acc: 18.902% (496/2624)\n",
      "Loss: 2.541 | Acc: 19.010% (511/2688)\n",
      "Loss: 2.530 | Acc: 19.222% (529/2752)\n",
      "Loss: 2.518 | Acc: 19.354% (545/2816)\n",
      "Loss: 2.504 | Acc: 19.653% (566/2880)\n",
      "Loss: 2.491 | Acc: 19.905% (586/2944)\n",
      "Loss: 2.486 | Acc: 19.947% (600/3008)\n",
      "Loss: 2.476 | Acc: 20.117% (618/3072)\n",
      "Loss: 2.469 | Acc: 20.217% (634/3136)\n",
      "Loss: 2.459 | Acc: 20.250% (648/3200)\n",
      "Loss: 2.448 | Acc: 20.282% (662/3264)\n",
      "Loss: 2.442 | Acc: 20.252% (674/3328)\n",
      "Loss: 2.435 | Acc: 20.342% (690/3392)\n",
      "Loss: 2.432 | Acc: 20.399% (705/3456)\n",
      "Loss: 2.430 | Acc: 20.312% (715/3520)\n",
      "Loss: 2.418 | Acc: 20.480% (734/3584)\n",
      "Loss: 2.409 | Acc: 20.532% (749/3648)\n",
      "Loss: 2.402 | Acc: 20.582% (764/3712)\n",
      "Loss: 2.399 | Acc: 20.630% (779/3776)\n",
      "Loss: 2.390 | Acc: 20.964% (805/3840)\n",
      "Loss: 2.385 | Acc: 21.055% (822/3904)\n",
      "Loss: 2.381 | Acc: 21.043% (835/3968)\n",
      "Loss: 2.376 | Acc: 21.156% (853/4032)\n",
      "Loss: 2.372 | Acc: 21.338% (874/4096)\n",
      "Loss: 2.368 | Acc: 21.370% (889/4160)\n",
      "Loss: 2.361 | Acc: 21.473% (907/4224)\n",
      "Loss: 2.352 | Acc: 21.642% (928/4288)\n",
      "Loss: 2.347 | Acc: 21.783% (948/4352)\n",
      "Loss: 2.344 | Acc: 21.739% (960/4416)\n",
      "Loss: 2.342 | Acc: 21.875% (980/4480)\n",
      "Loss: 2.337 | Acc: 22.007% (1000/4544)\n",
      "Loss: 2.330 | Acc: 22.114% (1019/4608)\n",
      "Loss: 2.329 | Acc: 22.132% (1034/4672)\n",
      "Loss: 2.324 | Acc: 22.318% (1057/4736)\n",
      "Loss: 2.315 | Acc: 22.583% (1084/4800)\n",
      "Loss: 2.314 | Acc: 22.656% (1102/4864)\n",
      "Loss: 2.310 | Acc: 22.727% (1120/4928)\n",
      "Loss: 2.303 | Acc: 22.877% (1142/4992)\n",
      "Loss: 2.299 | Acc: 22.943% (1160/5056)\n",
      "Loss: 2.294 | Acc: 22.969% (1176/5120)\n",
      "Loss: 2.287 | Acc: 23.090% (1197/5184)\n",
      "Loss: 2.281 | Acc: 23.171% (1216/5248)\n",
      "Loss: 2.278 | Acc: 23.193% (1232/5312)\n",
      "Loss: 2.273 | Acc: 23.307% (1253/5376)\n",
      "Loss: 2.272 | Acc: 23.272% (1266/5440)\n",
      "Loss: 2.273 | Acc: 23.183% (1276/5504)\n",
      "Loss: 2.270 | Acc: 23.240% (1294/5568)\n",
      "Loss: 2.265 | Acc: 23.295% (1312/5632)\n",
      "Loss: 2.263 | Acc: 23.332% (1329/5696)\n",
      "Loss: 2.259 | Acc: 23.351% (1345/5760)\n",
      "Loss: 2.255 | Acc: 23.455% (1366/5824)\n",
      "Loss: 2.252 | Acc: 23.454% (1381/5888)\n",
      "Loss: 2.247 | Acc: 23.606% (1405/5952)\n",
      "Loss: 2.244 | Acc: 23.687% (1425/6016)\n",
      "Loss: 2.241 | Acc: 23.717% (1442/6080)\n",
      "Loss: 2.240 | Acc: 23.763% (1460/6144)\n",
      "Loss: 2.236 | Acc: 23.824% (1479/6208)\n",
      "Loss: 2.233 | Acc: 23.932% (1501/6272)\n",
      "Loss: 2.231 | Acc: 23.958% (1518/6336)\n",
      "Loss: 2.229 | Acc: 24.016% (1537/6400)\n",
      "Loss: 2.226 | Acc: 24.087% (1557/6464)\n",
      "Loss: 2.222 | Acc: 24.173% (1578/6528)\n",
      "Loss: 2.219 | Acc: 24.211% (1596/6592)\n",
      "Loss: 2.216 | Acc: 24.309% (1618/6656)\n",
      "Loss: 2.214 | Acc: 24.330% (1635/6720)\n",
      "Loss: 2.210 | Acc: 24.366% (1653/6784)\n",
      "Loss: 2.206 | Acc: 24.489% (1677/6848)\n",
      "Loss: 2.203 | Acc: 24.537% (1696/6912)\n",
      "Loss: 2.203 | Acc: 24.513% (1710/6976)\n",
      "Loss: 2.199 | Acc: 24.560% (1729/7040)\n",
      "Loss: 2.196 | Acc: 24.592% (1747/7104)\n",
      "Loss: 2.192 | Acc: 24.665% (1768/7168)\n",
      "Loss: 2.189 | Acc: 24.751% (1790/7232)\n",
      "Loss: 2.186 | Acc: 24.836% (1812/7296)\n",
      "Loss: 2.184 | Acc: 24.864% (1830/7360)\n",
      "Loss: 2.179 | Acc: 25.013% (1857/7424)\n",
      "Loss: 2.177 | Acc: 25.067% (1877/7488)\n",
      "Loss: 2.173 | Acc: 25.159% (1900/7552)\n",
      "Loss: 2.167 | Acc: 25.289% (1926/7616)\n",
      "Loss: 2.167 | Acc: 25.299% (1943/7680)\n",
      "Loss: 2.164 | Acc: 25.336% (1962/7744)\n",
      "Loss: 2.163 | Acc: 25.295% (1975/7808)\n",
      "Loss: 2.162 | Acc: 25.254% (1988/7872)\n",
      "Loss: 2.161 | Acc: 25.290% (2007/7936)\n",
      "Loss: 2.158 | Acc: 25.387% (2031/8000)\n",
      "Loss: 2.156 | Acc: 25.434% (2051/8064)\n",
      "Loss: 2.153 | Acc: 25.517% (2074/8128)\n",
      "Loss: 2.150 | Acc: 25.537% (2092/8192)\n",
      "Loss: 2.147 | Acc: 25.642% (2117/8256)\n",
      "Loss: 2.145 | Acc: 25.613% (2131/8320)\n",
      "Loss: 2.143 | Acc: 25.644% (2150/8384)\n",
      "Loss: 2.139 | Acc: 25.769% (2177/8448)\n",
      "Loss: 2.138 | Acc: 25.775% (2194/8512)\n",
      "Loss: 2.134 | Acc: 25.851% (2217/8576)\n",
      "Loss: 2.132 | Acc: 25.856% (2234/8640)\n",
      "Loss: 2.129 | Acc: 25.919% (2256/8704)\n",
      "Loss: 2.127 | Acc: 25.958% (2276/8768)\n",
      "Loss: 2.124 | Acc: 26.042% (2300/8832)\n",
      "Loss: 2.123 | Acc: 26.079% (2320/8896)\n",
      "Loss: 2.122 | Acc: 26.083% (2337/8960)\n",
      "Loss: 2.121 | Acc: 26.064% (2352/9024)\n",
      "Loss: 2.117 | Acc: 26.177% (2379/9088)\n",
      "Loss: 2.114 | Acc: 26.257% (2403/9152)\n",
      "Loss: 2.112 | Acc: 26.270% (2421/9216)\n",
      "Loss: 2.110 | Acc: 26.358% (2446/9280)\n",
      "Loss: 2.109 | Acc: 26.327% (2460/9344)\n",
      "Loss: 2.107 | Acc: 26.371% (2481/9408)\n",
      "Loss: 2.104 | Acc: 26.489% (2509/9472)\n",
      "Loss: 2.102 | Acc: 26.562% (2533/9536)\n",
      "Loss: 2.100 | Acc: 26.604% (2554/9600)\n",
      "Loss: 2.098 | Acc: 26.645% (2575/9664)\n",
      "Loss: 2.097 | Acc: 26.655% (2593/9728)\n",
      "Loss: 2.096 | Acc: 26.654% (2610/9792)\n",
      "Loss: 2.092 | Acc: 26.806% (2642/9856)\n",
      "Loss: 2.091 | Acc: 26.875% (2666/9920)\n",
      "Loss: 2.089 | Acc: 26.963% (2692/9984)\n",
      "Loss: 2.089 | Acc: 26.990% (2712/10048)\n",
      "Loss: 2.086 | Acc: 27.126% (2743/10112)\n",
      "Loss: 2.084 | Acc: 27.241% (2772/10176)\n",
      "Loss: 2.082 | Acc: 27.305% (2796/10240)\n",
      "Loss: 2.082 | Acc: 27.300% (2813/10304)\n",
      "Loss: 2.082 | Acc: 27.324% (2833/10368)\n",
      "Loss: 2.081 | Acc: 27.339% (2852/10432)\n",
      "Loss: 2.079 | Acc: 27.410% (2877/10496)\n",
      "Loss: 2.076 | Acc: 27.528% (2907/10560)\n",
      "Loss: 2.073 | Acc: 27.617% (2934/10624)\n",
      "Loss: 2.072 | Acc: 27.695% (2960/10688)\n",
      "Loss: 2.069 | Acc: 27.772% (2986/10752)\n",
      "Loss: 2.068 | Acc: 27.801% (3007/10816)\n",
      "Loss: 2.067 | Acc: 27.812% (3026/10880)\n",
      "Loss: 2.066 | Acc: 27.833% (3046/10944)\n",
      "Loss: 2.065 | Acc: 27.852% (3066/11008)\n",
      "Loss: 2.064 | Acc: 27.935% (3093/11072)\n",
      "Loss: 2.060 | Acc: 28.089% (3128/11136)\n",
      "Loss: 2.058 | Acc: 28.089% (3146/11200)\n",
      "Loss: 2.059 | Acc: 28.072% (3162/11264)\n",
      "Loss: 2.058 | Acc: 28.107% (3184/11328)\n",
      "Loss: 2.057 | Acc: 28.099% (3201/11392)\n",
      "Loss: 2.055 | Acc: 28.151% (3225/11456)\n",
      "Loss: 2.053 | Acc: 28.247% (3254/11520)\n",
      "Loss: 2.052 | Acc: 28.263% (3274/11584)\n",
      "Loss: 2.050 | Acc: 28.314% (3298/11648)\n",
      "Loss: 2.048 | Acc: 28.356% (3321/11712)\n",
      "Loss: 2.047 | Acc: 28.337% (3337/11776)\n",
      "Loss: 2.045 | Acc: 28.421% (3365/11840)\n",
      "Loss: 2.045 | Acc: 28.453% (3387/11904)\n",
      "Loss: 2.045 | Acc: 28.476% (3408/11968)\n",
      "Loss: 2.043 | Acc: 28.482% (3427/12032)\n",
      "Loss: 2.041 | Acc: 28.497% (3447/12096)\n",
      "Loss: 2.040 | Acc: 28.536% (3470/12160)\n",
      "Loss: 2.039 | Acc: 28.575% (3493/12224)\n",
      "Loss: 2.038 | Acc: 28.581% (3512/12288)\n",
      "Loss: 2.036 | Acc: 28.619% (3535/12352)\n",
      "Loss: 2.035 | Acc: 28.697% (3563/12416)\n",
      "Loss: 2.034 | Acc: 28.678% (3579/12480)\n",
      "Loss: 2.034 | Acc: 28.699% (3600/12544)\n",
      "Loss: 2.032 | Acc: 28.783% (3629/12608)\n",
      "Loss: 2.031 | Acc: 28.780% (3647/12672)\n",
      "Loss: 2.029 | Acc: 28.824% (3671/12736)\n",
      "Loss: 2.028 | Acc: 28.867% (3695/12800)\n",
      "Loss: 2.027 | Acc: 28.871% (3714/12864)\n",
      "Loss: 2.025 | Acc: 28.960% (3744/12928)\n",
      "Loss: 2.023 | Acc: 29.018% (3770/12992)\n",
      "Loss: 2.021 | Acc: 29.036% (3791/13056)\n",
      "Loss: 2.020 | Acc: 29.101% (3818/13120)\n",
      "Loss: 2.020 | Acc: 29.103% (3837/13184)\n",
      "Loss: 2.018 | Acc: 29.129% (3859/13248)\n",
      "Loss: 2.017 | Acc: 29.207% (3888/13312)\n",
      "Loss: 2.015 | Acc: 29.284% (3917/13376)\n",
      "Loss: 2.012 | Acc: 29.345% (3944/13440)\n",
      "Loss: 2.011 | Acc: 29.369% (3966/13504)\n",
      "Loss: 2.009 | Acc: 29.415% (3991/13568)\n",
      "Loss: 2.007 | Acc: 29.423% (4011/13632)\n",
      "Loss: 2.006 | Acc: 29.461% (4035/13696)\n",
      "Loss: 2.005 | Acc: 29.506% (4060/13760)\n",
      "Loss: 2.003 | Acc: 29.550% (4085/13824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.002 | Acc: 29.572% (4107/13888)\n",
      "Loss: 2.001 | Acc: 29.652% (4137/13952)\n",
      "Loss: 1.998 | Acc: 29.688% (4161/14016)\n",
      "Loss: 1.997 | Acc: 29.709% (4183/14080)\n",
      "Loss: 1.996 | Acc: 29.695% (4200/14144)\n",
      "Loss: 1.995 | Acc: 29.744% (4226/14208)\n",
      "Loss: 1.993 | Acc: 29.793% (4252/14272)\n",
      "Loss: 1.992 | Acc: 29.792% (4271/14336)\n",
      "Loss: 1.990 | Acc: 29.826% (4295/14400)\n",
      "Loss: 1.989 | Acc: 29.833% (4315/14464)\n",
      "Loss: 1.987 | Acc: 29.901% (4344/14528)\n",
      "Loss: 1.986 | Acc: 29.941% (4369/14592)\n",
      "Loss: 1.985 | Acc: 29.960% (4391/14656)\n",
      "Loss: 1.984 | Acc: 29.980% (4413/14720)\n",
      "Loss: 1.982 | Acc: 29.999% (4435/14784)\n",
      "Loss: 1.982 | Acc: 30.011% (4456/14848)\n",
      "Loss: 1.981 | Acc: 30.036% (4479/14912)\n",
      "Loss: 1.979 | Acc: 30.088% (4506/14976)\n",
      "Loss: 1.977 | Acc: 30.160% (4536/15040)\n",
      "Loss: 1.977 | Acc: 30.184% (4559/15104)\n",
      "Loss: 1.976 | Acc: 30.228% (4585/15168)\n",
      "Loss: 1.975 | Acc: 30.219% (4603/15232)\n",
      "Loss: 1.974 | Acc: 30.263% (4629/15296)\n",
      "Loss: 1.973 | Acc: 30.286% (4652/15360)\n",
      "Loss: 1.973 | Acc: 30.316% (4676/15424)\n",
      "Loss: 1.970 | Acc: 30.430% (4713/15488)\n",
      "Loss: 1.968 | Acc: 30.491% (4742/15552)\n",
      "Loss: 1.968 | Acc: 30.469% (4758/15616)\n",
      "Loss: 1.968 | Acc: 30.472% (4778/15680)\n",
      "Loss: 1.966 | Acc: 30.532% (4807/15744)\n",
      "Loss: 1.965 | Acc: 30.605% (4838/15808)\n",
      "Loss: 1.963 | Acc: 30.645% (4864/15872)\n",
      "Loss: 1.963 | Acc: 30.679% (4889/15936)\n",
      "Loss: 1.962 | Acc: 30.712% (4914/16000)\n",
      "Loss: 1.961 | Acc: 30.740% (4938/16064)\n",
      "Loss: 1.959 | Acc: 30.773% (4963/16128)\n",
      "Loss: 1.958 | Acc: 30.793% (4986/16192)\n",
      "Loss: 1.958 | Acc: 30.770% (5002/16256)\n",
      "Loss: 1.958 | Acc: 30.778% (5023/16320)\n",
      "Loss: 1.957 | Acc: 30.835% (5052/16384)\n",
      "Loss: 1.956 | Acc: 30.879% (5079/16448)\n",
      "Loss: 1.955 | Acc: 30.881% (5099/16512)\n",
      "Loss: 1.953 | Acc: 30.918% (5125/16576)\n",
      "Loss: 1.952 | Acc: 30.962% (5152/16640)\n",
      "Loss: 1.951 | Acc: 30.987% (5176/16704)\n",
      "Loss: 1.951 | Acc: 30.934% (5187/16768)\n",
      "Loss: 1.950 | Acc: 30.947% (5209/16832)\n",
      "Loss: 1.949 | Acc: 30.996% (5237/16896)\n",
      "Loss: 1.948 | Acc: 31.032% (5263/16960)\n",
      "Loss: 1.947 | Acc: 31.062% (5288/17024)\n",
      "Loss: 1.946 | Acc: 31.104% (5315/17088)\n",
      "Loss: 1.945 | Acc: 31.128% (5339/17152)\n",
      "Loss: 1.944 | Acc: 31.140% (5361/17216)\n",
      "Loss: 1.943 | Acc: 31.169% (5386/17280)\n",
      "Loss: 1.943 | Acc: 31.181% (5408/17344)\n",
      "Loss: 1.942 | Acc: 31.198% (5431/17408)\n",
      "Loss: 1.940 | Acc: 31.267% (5463/17472)\n",
      "Loss: 1.940 | Acc: 31.273% (5484/17536)\n",
      "Loss: 1.939 | Acc: 31.335% (5515/17600)\n",
      "Loss: 1.939 | Acc: 31.324% (5533/17664)\n",
      "Loss: 1.938 | Acc: 31.289% (5547/17728)\n",
      "Loss: 1.938 | Acc: 31.289% (5567/17792)\n",
      "Loss: 1.936 | Acc: 31.312% (5591/17856)\n",
      "Loss: 1.935 | Acc: 31.362% (5620/17920)\n",
      "Loss: 1.934 | Acc: 31.395% (5646/17984)\n",
      "Loss: 1.932 | Acc: 31.422% (5671/18048)\n",
      "Loss: 1.932 | Acc: 31.449% (5696/18112)\n",
      "Loss: 1.931 | Acc: 31.470% (5720/18176)\n",
      "Loss: 1.930 | Acc: 31.480% (5742/18240)\n",
      "Loss: 1.929 | Acc: 31.523% (5770/18304)\n",
      "Loss: 1.928 | Acc: 31.571% (5799/18368)\n",
      "Loss: 1.926 | Acc: 31.630% (5830/18432)\n",
      "Loss: 1.926 | Acc: 31.618% (5848/18496)\n",
      "Loss: 1.924 | Acc: 31.654% (5875/18560)\n",
      "Loss: 1.924 | Acc: 31.680% (5900/18624)\n",
      "Loss: 1.922 | Acc: 31.726% (5929/18688)\n",
      "Loss: 1.922 | Acc: 31.757% (5955/18752)\n",
      "Loss: 1.921 | Acc: 31.776% (5979/18816)\n",
      "Loss: 1.919 | Acc: 31.817% (6007/18880)\n",
      "Loss: 1.918 | Acc: 31.862% (6036/18944)\n",
      "Loss: 1.918 | Acc: 31.860% (6056/19008)\n",
      "Loss: 1.918 | Acc: 31.884% (6081/19072)\n",
      "Loss: 1.916 | Acc: 31.898% (6104/19136)\n",
      "Loss: 1.916 | Acc: 31.906% (6126/19200)\n",
      "Loss: 1.915 | Acc: 31.930% (6151/19264)\n",
      "Loss: 1.914 | Acc: 31.995% (6184/19328)\n",
      "Loss: 1.914 | Acc: 31.998% (6205/19392)\n",
      "Loss: 1.913 | Acc: 32.011% (6228/19456)\n",
      "Loss: 1.912 | Acc: 32.044% (6255/19520)\n",
      "Loss: 1.911 | Acc: 32.093% (6285/19584)\n",
      "Loss: 1.909 | Acc: 32.120% (6311/19648)\n",
      "Loss: 1.909 | Acc: 32.123% (6332/19712)\n",
      "Loss: 1.908 | Acc: 32.145% (6357/19776)\n",
      "Loss: 1.906 | Acc: 32.193% (6387/19840)\n",
      "Loss: 1.906 | Acc: 32.240% (6417/19904)\n",
      "Loss: 1.906 | Acc: 32.247% (6439/19968)\n",
      "Loss: 1.904 | Acc: 32.308% (6472/20032)\n",
      "Loss: 1.903 | Acc: 32.325% (6496/20096)\n",
      "Loss: 1.902 | Acc: 32.381% (6528/20160)\n",
      "Loss: 1.901 | Acc: 32.392% (6551/20224)\n",
      "Loss: 1.900 | Acc: 32.418% (6577/20288)\n",
      "Loss: 1.900 | Acc: 32.410% (6596/20352)\n",
      "Loss: 1.899 | Acc: 32.445% (6624/20416)\n",
      "Loss: 1.898 | Acc: 32.432% (6642/20480)\n",
      "Loss: 1.897 | Acc: 32.462% (6669/20544)\n",
      "Loss: 1.896 | Acc: 32.526% (6703/20608)\n",
      "Loss: 1.895 | Acc: 32.551% (6729/20672)\n",
      "Loss: 1.894 | Acc: 32.581% (6756/20736)\n",
      "Loss: 1.893 | Acc: 32.611% (6783/20800)\n",
      "Loss: 1.891 | Acc: 32.678% (6818/20864)\n",
      "Loss: 1.890 | Acc: 32.731% (6850/20928)\n",
      "Loss: 1.889 | Acc: 32.746% (6874/20992)\n",
      "Loss: 1.889 | Acc: 32.775% (6901/21056)\n",
      "Loss: 1.888 | Acc: 32.794% (6926/21120)\n",
      "Loss: 1.888 | Acc: 32.784% (6945/21184)\n",
      "Loss: 1.887 | Acc: 32.794% (6968/21248)\n",
      "Loss: 1.886 | Acc: 32.855% (7002/21312)\n",
      "Loss: 1.884 | Acc: 32.911% (7035/21376)\n",
      "Loss: 1.883 | Acc: 32.966% (7068/21440)\n",
      "Loss: 1.883 | Acc: 32.989% (7094/21504)\n",
      "Loss: 1.882 | Acc: 32.993% (7116/21568)\n",
      "Loss: 1.881 | Acc: 33.030% (7145/21632)\n",
      "Loss: 1.880 | Acc: 33.057% (7172/21696)\n",
      "Loss: 1.879 | Acc: 33.097% (7202/21760)\n",
      "Loss: 1.879 | Acc: 33.129% (7230/21824)\n",
      "Loss: 1.878 | Acc: 33.183% (7263/21888)\n",
      "Loss: 1.877 | Acc: 33.227% (7294/21952)\n",
      "Loss: 1.875 | Acc: 33.308% (7333/22016)\n",
      "Loss: 1.875 | Acc: 33.320% (7357/22080)\n",
      "Loss: 1.875 | Acc: 33.314% (7377/22144)\n",
      "Loss: 1.875 | Acc: 33.312% (7398/22208)\n",
      "Loss: 1.874 | Acc: 33.342% (7426/22272)\n",
      "Loss: 1.873 | Acc: 33.354% (7450/22336)\n",
      "Loss: 1.873 | Acc: 33.379% (7477/22400)\n",
      "Loss: 1.871 | Acc: 33.427% (7509/22464)\n",
      "Loss: 1.871 | Acc: 33.452% (7536/22528)\n",
      "Loss: 1.871 | Acc: 33.459% (7559/22592)\n",
      "Loss: 1.870 | Acc: 33.497% (7589/22656)\n",
      "Loss: 1.869 | Acc: 33.530% (7618/22720)\n",
      "Loss: 1.868 | Acc: 33.545% (7643/22784)\n",
      "Loss: 1.867 | Acc: 33.570% (7670/22848)\n",
      "Loss: 1.867 | Acc: 33.585% (7695/22912)\n",
      "Loss: 1.867 | Acc: 33.618% (7724/22976)\n",
      "Loss: 1.866 | Acc: 33.620% (7746/23040)\n",
      "Loss: 1.865 | Acc: 33.639% (7772/23104)\n",
      "Loss: 1.863 | Acc: 33.693% (7806/23168)\n",
      "Loss: 1.863 | Acc: 33.716% (7833/23232)\n",
      "Loss: 1.862 | Acc: 33.714% (7854/23296)\n",
      "Loss: 1.862 | Acc: 33.737% (7881/23360)\n",
      "Loss: 1.861 | Acc: 33.760% (7908/23424)\n",
      "Loss: 1.860 | Acc: 33.783% (7935/23488)\n",
      "Loss: 1.859 | Acc: 33.789% (7958/23552)\n",
      "Loss: 1.859 | Acc: 33.786% (7979/23616)\n",
      "Loss: 1.858 | Acc: 33.813% (8007/23680)\n",
      "Loss: 1.857 | Acc: 33.857% (8039/23744)\n",
      "Loss: 1.858 | Acc: 33.850% (8059/23808)\n",
      "Loss: 1.857 | Acc: 33.902% (8093/23872)\n",
      "Loss: 1.855 | Acc: 33.932% (8122/23936)\n",
      "Loss: 1.854 | Acc: 33.962% (8151/24000)\n",
      "Loss: 1.854 | Acc: 33.980% (8177/24064)\n",
      "Loss: 1.853 | Acc: 33.998% (8203/24128)\n",
      "Loss: 1.853 | Acc: 33.999% (8225/24192)\n",
      "Loss: 1.853 | Acc: 34.025% (8253/24256)\n",
      "Loss: 1.852 | Acc: 34.058% (8283/24320)\n",
      "Loss: 1.852 | Acc: 34.076% (8309/24384)\n",
      "Loss: 1.850 | Acc: 34.142% (8347/24448)\n",
      "Loss: 1.850 | Acc: 34.155% (8372/24512)\n",
      "Loss: 1.850 | Acc: 34.159% (8395/24576)\n",
      "Loss: 1.849 | Acc: 34.184% (8423/24640)\n",
      "Loss: 1.849 | Acc: 34.185% (8445/24704)\n",
      "Loss: 1.848 | Acc: 34.181% (8466/24768)\n",
      "Loss: 1.848 | Acc: 34.198% (8492/24832)\n",
      "Loss: 1.847 | Acc: 34.218% (8519/24896)\n",
      "Loss: 1.846 | Acc: 34.255% (8550/24960)\n",
      "Loss: 1.845 | Acc: 34.287% (8580/25024)\n",
      "Loss: 1.845 | Acc: 34.299% (8605/25088)\n",
      "Loss: 1.844 | Acc: 34.335% (8636/25152)\n",
      "Loss: 1.843 | Acc: 34.355% (8663/25216)\n",
      "Loss: 1.843 | Acc: 34.367% (8688/25280)\n",
      "Loss: 1.842 | Acc: 34.418% (8723/25344)\n",
      "Loss: 1.841 | Acc: 34.438% (8750/25408)\n",
      "Loss: 1.840 | Acc: 34.434% (8771/25472)\n",
      "Loss: 1.839 | Acc: 34.453% (8798/25536)\n",
      "Loss: 1.839 | Acc: 34.461% (8822/25600)\n",
      "Loss: 1.838 | Acc: 34.492% (8852/25664)\n",
      "Loss: 1.837 | Acc: 34.515% (8880/25728)\n",
      "Loss: 1.836 | Acc: 34.549% (8911/25792)\n",
      "Loss: 1.835 | Acc: 34.592% (8944/25856)\n",
      "Loss: 1.834 | Acc: 34.618% (8973/25920)\n",
      "Loss: 1.834 | Acc: 34.637% (9000/25984)\n",
      "Loss: 1.833 | Acc: 34.659% (9028/26048)\n",
      "Loss: 1.832 | Acc: 34.670% (9053/26112)\n",
      "Loss: 1.831 | Acc: 34.704% (9084/26176)\n",
      "Loss: 1.831 | Acc: 34.741% (9116/26240)\n",
      "Loss: 1.830 | Acc: 34.767% (9145/26304)\n",
      "Loss: 1.829 | Acc: 34.785% (9172/26368)\n",
      "Loss: 1.828 | Acc: 34.821% (9204/26432)\n",
      "Loss: 1.828 | Acc: 34.828% (9228/26496)\n",
      "Loss: 1.827 | Acc: 34.857% (9258/26560)\n",
      "Loss: 1.826 | Acc: 34.871% (9284/26624)\n",
      "Loss: 1.826 | Acc: 34.877% (9308/26688)\n",
      "Loss: 1.825 | Acc: 34.891% (9334/26752)\n",
      "Loss: 1.824 | Acc: 34.912% (9362/26816)\n",
      "Loss: 1.823 | Acc: 34.948% (9394/26880)\n",
      "Loss: 1.822 | Acc: 34.991% (9428/26944)\n",
      "Loss: 1.822 | Acc: 35.008% (9455/27008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.821 | Acc: 35.014% (9479/27072)\n",
      "Loss: 1.820 | Acc: 35.027% (9505/27136)\n",
      "Loss: 1.820 | Acc: 35.029% (9528/27200)\n",
      "Loss: 1.819 | Acc: 35.050% (9556/27264)\n",
      "Loss: 1.819 | Acc: 35.067% (9583/27328)\n",
      "Loss: 1.819 | Acc: 35.072% (9607/27392)\n",
      "Loss: 1.818 | Acc: 35.111% (9640/27456)\n",
      "Loss: 1.817 | Acc: 35.131% (9668/27520)\n",
      "Loss: 1.816 | Acc: 35.154% (9697/27584)\n",
      "Loss: 1.816 | Acc: 35.167% (9723/27648)\n",
      "Loss: 1.815 | Acc: 35.191% (9752/27712)\n",
      "Loss: 1.814 | Acc: 35.232% (9786/27776)\n",
      "Loss: 1.814 | Acc: 35.230% (9808/27840)\n",
      "Loss: 1.814 | Acc: 35.257% (9838/27904)\n",
      "Loss: 1.813 | Acc: 35.269% (9864/27968)\n",
      "Loss: 1.812 | Acc: 35.281% (9890/28032)\n",
      "Loss: 1.812 | Acc: 35.311% (9921/28096)\n",
      "Loss: 1.811 | Acc: 35.327% (9948/28160)\n",
      "Loss: 1.810 | Acc: 35.349% (9977/28224)\n",
      "Loss: 1.810 | Acc: 35.365% (10004/28288)\n",
      "Loss: 1.809 | Acc: 35.384% (10032/28352)\n",
      "Loss: 1.809 | Acc: 35.385% (10055/28416)\n",
      "Loss: 1.809 | Acc: 35.390% (10079/28480)\n",
      "Loss: 1.808 | Acc: 35.440% (10116/28544)\n",
      "Loss: 1.807 | Acc: 35.487% (10152/28608)\n",
      "Loss: 1.807 | Acc: 35.491% (10176/28672)\n",
      "Loss: 1.806 | Acc: 35.503% (10202/28736)\n",
      "Loss: 1.806 | Acc: 35.521% (10230/28800)\n",
      "Loss: 1.805 | Acc: 35.522% (10253/28864)\n",
      "Loss: 1.805 | Acc: 35.526% (10277/28928)\n",
      "Loss: 1.803 | Acc: 35.582% (10316/28992)\n",
      "Loss: 1.803 | Acc: 35.590% (10341/29056)\n",
      "Loss: 1.802 | Acc: 35.618% (10372/29120)\n",
      "Loss: 1.802 | Acc: 35.629% (10398/29184)\n",
      "Loss: 1.800 | Acc: 35.678% (10435/29248)\n",
      "Loss: 1.800 | Acc: 35.688% (10461/29312)\n",
      "Loss: 1.800 | Acc: 35.703% (10488/29376)\n",
      "Loss: 1.799 | Acc: 35.710% (10513/29440)\n",
      "Loss: 1.798 | Acc: 35.738% (10544/29504)\n",
      "Loss: 1.798 | Acc: 35.714% (10560/29568)\n",
      "Loss: 1.798 | Acc: 35.722% (10585/29632)\n",
      "Loss: 1.797 | Acc: 35.735% (10612/29696)\n",
      "Loss: 1.797 | Acc: 35.746% (10638/29760)\n",
      "Loss: 1.796 | Acc: 35.743% (10660/29824)\n",
      "Loss: 1.796 | Acc: 35.747% (10684/29888)\n",
      "Loss: 1.796 | Acc: 35.774% (10715/29952)\n",
      "Loss: 1.795 | Acc: 35.784% (10741/30016)\n",
      "Loss: 1.795 | Acc: 35.795% (10767/30080)\n",
      "Loss: 1.794 | Acc: 35.808% (10794/30144)\n",
      "Loss: 1.793 | Acc: 35.825% (10822/30208)\n",
      "Loss: 1.792 | Acc: 35.838% (10849/30272)\n",
      "Loss: 1.792 | Acc: 35.868% (10881/30336)\n",
      "Loss: 1.791 | Acc: 35.885% (10909/30400)\n",
      "Loss: 1.792 | Acc: 35.888% (10933/30464)\n",
      "Loss: 1.791 | Acc: 35.924% (10967/30528)\n",
      "Loss: 1.790 | Acc: 35.951% (10998/30592)\n",
      "Loss: 1.789 | Acc: 35.970% (11027/30656)\n",
      "Loss: 1.789 | Acc: 35.996% (11058/30720)\n",
      "Loss: 1.788 | Acc: 35.999% (11082/30784)\n",
      "Loss: 1.788 | Acc: 35.989% (11102/30848)\n",
      "Loss: 1.787 | Acc: 36.002% (11129/30912)\n",
      "Loss: 1.787 | Acc: 36.018% (11157/30976)\n",
      "Loss: 1.787 | Acc: 36.037% (11186/31040)\n",
      "Loss: 1.786 | Acc: 36.037% (11209/31104)\n",
      "Loss: 1.786 | Acc: 36.069% (11242/31168)\n",
      "Loss: 1.785 | Acc: 36.098% (11274/31232)\n",
      "Loss: 1.785 | Acc: 36.100% (11298/31296)\n",
      "Loss: 1.785 | Acc: 36.113% (11325/31360)\n",
      "Loss: 1.784 | Acc: 36.163% (11364/31424)\n",
      "Loss: 1.783 | Acc: 36.192% (11396/31488)\n",
      "Loss: 1.783 | Acc: 36.191% (11419/31552)\n",
      "Loss: 1.782 | Acc: 36.213% (11449/31616)\n",
      "Loss: 1.783 | Acc: 36.212% (11472/31680)\n",
      "Loss: 1.782 | Acc: 36.230% (11501/31744)\n",
      "Loss: 1.781 | Acc: 36.277% (11539/31808)\n",
      "Loss: 1.781 | Acc: 36.273% (11561/31872)\n",
      "Loss: 1.781 | Acc: 36.257% (11579/31936)\n",
      "Loss: 1.780 | Acc: 36.272% (11607/32000)\n",
      "Loss: 1.780 | Acc: 36.256% (11625/32064)\n",
      "Loss: 1.780 | Acc: 36.292% (11660/32128)\n",
      "Loss: 1.779 | Acc: 36.313% (11690/32192)\n",
      "Loss: 1.779 | Acc: 36.334% (11720/32256)\n",
      "Loss: 1.779 | Acc: 36.340% (11745/32320)\n",
      "Loss: 1.778 | Acc: 36.348% (11771/32384)\n",
      "Loss: 1.778 | Acc: 36.360% (11798/32448)\n",
      "Loss: 1.777 | Acc: 36.368% (11824/32512)\n",
      "Loss: 1.777 | Acc: 36.395% (11856/32576)\n",
      "Loss: 1.776 | Acc: 36.406% (11883/32640)\n",
      "Loss: 1.776 | Acc: 36.415% (11909/32704)\n",
      "Loss: 1.775 | Acc: 36.432% (11938/32768)\n",
      "Loss: 1.775 | Acc: 36.443% (11965/32832)\n",
      "Loss: 1.775 | Acc: 36.448% (11990/32896)\n",
      "Loss: 1.774 | Acc: 36.478% (12023/32960)\n",
      "Loss: 1.774 | Acc: 36.495% (12052/33024)\n",
      "Loss: 1.773 | Acc: 36.518% (12083/33088)\n",
      "Loss: 1.773 | Acc: 36.508% (12103/33152)\n",
      "Loss: 1.773 | Acc: 36.516% (12129/33216)\n",
      "Loss: 1.773 | Acc: 36.523% (12155/33280)\n",
      "Loss: 1.772 | Acc: 36.531% (12181/33344)\n",
      "Loss: 1.772 | Acc: 36.557% (12213/33408)\n",
      "Loss: 1.772 | Acc: 36.577% (12243/33472)\n",
      "Loss: 1.772 | Acc: 36.564% (12262/33536)\n",
      "Loss: 1.771 | Acc: 36.583% (12292/33600)\n",
      "Loss: 1.771 | Acc: 36.591% (12318/33664)\n",
      "Loss: 1.771 | Acc: 36.611% (12348/33728)\n",
      "Loss: 1.770 | Acc: 36.642% (12382/33792)\n",
      "Loss: 1.770 | Acc: 36.646% (12407/33856)\n",
      "Loss: 1.770 | Acc: 36.660% (12435/33920)\n",
      "Loss: 1.769 | Acc: 36.664% (12460/33984)\n",
      "Loss: 1.769 | Acc: 36.663% (12483/34048)\n",
      "Loss: 1.768 | Acc: 36.703% (12520/34112)\n",
      "Loss: 1.768 | Acc: 36.730% (12553/34176)\n",
      "Loss: 1.767 | Acc: 36.744% (12581/34240)\n",
      "Loss: 1.767 | Acc: 36.762% (12611/34304)\n",
      "Loss: 1.767 | Acc: 36.767% (12636/34368)\n",
      "Loss: 1.766 | Acc: 36.780% (12664/34432)\n",
      "Loss: 1.766 | Acc: 36.804% (12696/34496)\n",
      "Loss: 1.765 | Acc: 36.832% (12729/34560)\n",
      "Loss: 1.765 | Acc: 36.853% (12760/34624)\n",
      "Loss: 1.764 | Acc: 36.863% (12787/34688)\n",
      "Loss: 1.764 | Acc: 36.901% (12824/34752)\n",
      "Loss: 1.763 | Acc: 36.917% (12853/34816)\n",
      "Loss: 1.762 | Acc: 36.932% (12882/34880)\n",
      "Loss: 1.762 | Acc: 36.948% (12911/34944)\n",
      "Loss: 1.761 | Acc: 36.946% (12934/35008)\n",
      "Loss: 1.761 | Acc: 36.958% (12962/35072)\n",
      "Loss: 1.761 | Acc: 36.982% (12994/35136)\n",
      "Loss: 1.760 | Acc: 36.991% (13021/35200)\n",
      "Loss: 1.759 | Acc: 37.024% (13056/35264)\n",
      "Loss: 1.759 | Acc: 37.044% (13087/35328)\n",
      "Loss: 1.758 | Acc: 37.051% (13113/35392)\n",
      "Loss: 1.758 | Acc: 37.068% (13143/35456)\n",
      "Loss: 1.757 | Acc: 37.075% (13169/35520)\n",
      "Loss: 1.757 | Acc: 37.098% (13201/35584)\n",
      "Loss: 1.756 | Acc: 37.113% (13230/35648)\n",
      "Loss: 1.756 | Acc: 37.153% (13268/35712)\n",
      "Loss: 1.755 | Acc: 37.173% (13299/35776)\n",
      "Loss: 1.754 | Acc: 37.190% (13329/35840)\n",
      "Loss: 1.754 | Acc: 37.213% (13361/35904)\n",
      "Loss: 1.754 | Acc: 37.219% (13387/35968)\n",
      "Loss: 1.753 | Acc: 37.236% (13417/36032)\n",
      "Loss: 1.752 | Acc: 37.251% (13446/36096)\n",
      "Loss: 1.752 | Acc: 37.254% (13471/36160)\n",
      "Loss: 1.751 | Acc: 37.276% (13503/36224)\n",
      "Loss: 1.751 | Acc: 37.285% (13530/36288)\n",
      "Loss: 1.750 | Acc: 37.296% (13558/36352)\n",
      "Loss: 1.750 | Acc: 37.319% (13590/36416)\n",
      "Loss: 1.749 | Acc: 37.341% (13622/36480)\n",
      "Loss: 1.748 | Acc: 37.369% (13656/36544)\n",
      "Loss: 1.748 | Acc: 37.380% (13684/36608)\n",
      "Loss: 1.748 | Acc: 37.399% (13715/36672)\n",
      "Loss: 1.747 | Acc: 37.421% (13747/36736)\n",
      "Loss: 1.748 | Acc: 37.424% (13772/36800)\n",
      "Loss: 1.747 | Acc: 37.429% (13798/36864)\n",
      "Loss: 1.747 | Acc: 37.440% (13826/36928)\n",
      "Loss: 1.746 | Acc: 37.446% (13852/36992)\n",
      "Loss: 1.746 | Acc: 37.443% (13875/37056)\n",
      "Loss: 1.746 | Acc: 37.457% (13904/37120)\n",
      "Loss: 1.745 | Acc: 37.476% (13935/37184)\n",
      "Loss: 1.745 | Acc: 37.497% (13967/37248)\n",
      "Loss: 1.745 | Acc: 37.516% (13998/37312)\n",
      "Loss: 1.744 | Acc: 37.519% (14023/37376)\n",
      "Loss: 1.744 | Acc: 37.535% (14053/37440)\n",
      "Loss: 1.744 | Acc: 37.535% (14077/37504)\n",
      "Loss: 1.743 | Acc: 37.559% (14110/37568)\n",
      "Loss: 1.743 | Acc: 37.572% (14139/37632)\n",
      "Loss: 1.742 | Acc: 37.577% (14165/37696)\n",
      "Loss: 1.742 | Acc: 37.569% (14186/37760)\n",
      "Loss: 1.742 | Acc: 37.561% (14207/37824)\n",
      "Loss: 1.742 | Acc: 37.563% (14232/37888)\n",
      "Loss: 1.742 | Acc: 37.576% (14261/37952)\n",
      "Loss: 1.741 | Acc: 37.600% (14294/38016)\n",
      "Loss: 1.741 | Acc: 37.616% (14324/38080)\n",
      "Loss: 1.740 | Acc: 37.623% (14351/38144)\n",
      "Loss: 1.740 | Acc: 37.620% (14374/38208)\n",
      "Loss: 1.740 | Acc: 37.625% (14400/38272)\n",
      "Loss: 1.740 | Acc: 37.625% (14424/38336)\n",
      "Loss: 1.740 | Acc: 37.625% (14448/38400)\n",
      "Loss: 1.740 | Acc: 37.638% (14477/38464)\n",
      "Loss: 1.739 | Acc: 37.648% (14505/38528)\n",
      "Loss: 1.739 | Acc: 37.653% (14531/38592)\n",
      "Loss: 1.738 | Acc: 37.668% (14561/38656)\n",
      "Loss: 1.738 | Acc: 37.699% (14597/38720)\n",
      "Loss: 1.737 | Acc: 37.717% (14628/38784)\n",
      "Loss: 1.737 | Acc: 37.750% (14665/38848)\n",
      "Loss: 1.736 | Acc: 37.757% (14692/38912)\n",
      "Loss: 1.736 | Acc: 37.764% (14719/38976)\n",
      "Loss: 1.735 | Acc: 37.779% (14749/39040)\n",
      "Loss: 1.735 | Acc: 37.794% (14779/39104)\n",
      "Loss: 1.734 | Acc: 37.811% (14810/39168)\n",
      "Loss: 1.734 | Acc: 37.816% (14836/39232)\n",
      "Loss: 1.733 | Acc: 37.831% (14866/39296)\n",
      "Loss: 1.733 | Acc: 37.840% (14894/39360)\n",
      "Loss: 1.732 | Acc: 37.855% (14924/39424)\n",
      "Loss: 1.732 | Acc: 37.865% (14952/39488)\n",
      "Loss: 1.732 | Acc: 37.874% (14980/39552)\n",
      "Loss: 1.732 | Acc: 37.876% (15005/39616)\n",
      "Loss: 1.731 | Acc: 37.916% (15045/39680)\n",
      "Loss: 1.730 | Acc: 37.925% (15073/39744)\n",
      "Loss: 1.730 | Acc: 37.935% (15101/39808)\n",
      "Loss: 1.730 | Acc: 37.944% (15129/39872)\n",
      "Loss: 1.730 | Acc: 37.966% (15162/39936)\n",
      "Loss: 1.729 | Acc: 37.977% (15191/40000)\n",
      "Loss: 1.729 | Acc: 37.982% (15217/40064)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.729 | Acc: 37.996% (15247/40128)\n",
      "Loss: 1.728 | Acc: 38.005% (15275/40192)\n",
      "Loss: 1.728 | Acc: 38.022% (15306/40256)\n",
      "Loss: 1.728 | Acc: 38.041% (15338/40320)\n",
      "Loss: 1.727 | Acc: 38.052% (15367/40384)\n",
      "Loss: 1.727 | Acc: 38.056% (15393/40448)\n",
      "Loss: 1.727 | Acc: 38.078% (15426/40512)\n",
      "Loss: 1.726 | Acc: 38.099% (15459/40576)\n",
      "Loss: 1.726 | Acc: 38.100% (15484/40640)\n",
      "Loss: 1.726 | Acc: 38.099% (15508/40704)\n",
      "Loss: 1.725 | Acc: 38.113% (15538/40768)\n",
      "Loss: 1.724 | Acc: 38.137% (15572/40832)\n",
      "Loss: 1.724 | Acc: 38.153% (15603/40896)\n",
      "Loss: 1.723 | Acc: 38.171% (15635/40960)\n",
      "Loss: 1.723 | Acc: 38.180% (15663/41024)\n",
      "Loss: 1.722 | Acc: 38.211% (15700/41088)\n",
      "Loss: 1.721 | Acc: 38.234% (15734/41152)\n",
      "Loss: 1.721 | Acc: 38.242% (15762/41216)\n",
      "Loss: 1.721 | Acc: 38.266% (15796/41280)\n",
      "Loss: 1.720 | Acc: 38.269% (15822/41344)\n",
      "Loss: 1.720 | Acc: 38.273% (15848/41408)\n",
      "Loss: 1.719 | Acc: 38.296% (15882/41472)\n",
      "Loss: 1.719 | Acc: 38.302% (15909/41536)\n",
      "Loss: 1.718 | Acc: 38.327% (15944/41600)\n",
      "Loss: 1.718 | Acc: 38.342% (15975/41664)\n",
      "Loss: 1.717 | Acc: 38.353% (16004/41728)\n",
      "Loss: 1.717 | Acc: 38.361% (16032/41792)\n",
      "Loss: 1.717 | Acc: 38.372% (16061/41856)\n",
      "Loss: 1.717 | Acc: 38.387% (16092/41920)\n",
      "Loss: 1.716 | Acc: 38.415% (16128/41984)\n",
      "Loss: 1.715 | Acc: 38.430% (16159/42048)\n",
      "Loss: 1.715 | Acc: 38.450% (16192/42112)\n",
      "Loss: 1.714 | Acc: 38.477% (16228/42176)\n",
      "Loss: 1.714 | Acc: 38.480% (16254/42240)\n",
      "Loss: 1.714 | Acc: 38.490% (16283/42304)\n",
      "Loss: 1.713 | Acc: 38.491% (16308/42368)\n",
      "Loss: 1.713 | Acc: 38.499% (16336/42432)\n",
      "Loss: 1.713 | Acc: 38.493% (16358/42496)\n",
      "Loss: 1.713 | Acc: 38.517% (16393/42560)\n",
      "Loss: 1.712 | Acc: 38.535% (16425/42624)\n",
      "Loss: 1.712 | Acc: 38.554% (16458/42688)\n",
      "Loss: 1.712 | Acc: 38.567% (16488/42752)\n",
      "Loss: 1.711 | Acc: 38.588% (16522/42816)\n",
      "Loss: 1.711 | Acc: 38.596% (16550/42880)\n",
      "Loss: 1.710 | Acc: 38.620% (16585/42944)\n",
      "Loss: 1.710 | Acc: 38.628% (16613/43008)\n",
      "Loss: 1.710 | Acc: 38.638% (16642/43072)\n",
      "Loss: 1.709 | Acc: 38.666% (16679/43136)\n",
      "Loss: 1.709 | Acc: 38.669% (16705/43200)\n",
      "Loss: 1.709 | Acc: 38.679% (16734/43264)\n",
      "Loss: 1.708 | Acc: 38.693% (16765/43328)\n",
      "Loss: 1.708 | Acc: 38.715% (16799/43392)\n",
      "Loss: 1.708 | Acc: 38.729% (16830/43456)\n",
      "Loss: 1.707 | Acc: 38.741% (16860/43520)\n",
      "Loss: 1.707 | Acc: 38.760% (16893/43584)\n",
      "Loss: 1.706 | Acc: 38.781% (16927/43648)\n",
      "Loss: 1.706 | Acc: 38.781% (16952/43712)\n",
      "Loss: 1.706 | Acc: 38.779% (16976/43776)\n",
      "Loss: 1.705 | Acc: 38.796% (17008/43840)\n",
      "Loss: 1.705 | Acc: 38.823% (17045/43904)\n",
      "Loss: 1.704 | Acc: 38.821% (17069/43968)\n",
      "Loss: 1.704 | Acc: 38.838% (17101/44032)\n",
      "Loss: 1.704 | Acc: 38.852% (17132/44096)\n",
      "Loss: 1.703 | Acc: 38.854% (17158/44160)\n",
      "Loss: 1.703 | Acc: 38.888% (17198/44224)\n",
      "Loss: 1.702 | Acc: 38.907% (17231/44288)\n",
      "Loss: 1.702 | Acc: 38.929% (17266/44352)\n",
      "Loss: 1.702 | Acc: 38.930% (17291/44416)\n",
      "Loss: 1.701 | Acc: 38.948% (17324/44480)\n",
      "Loss: 1.701 | Acc: 38.970% (17359/44544)\n",
      "Loss: 1.700 | Acc: 38.993% (17394/44608)\n",
      "Loss: 1.700 | Acc: 38.989% (17417/44672)\n",
      "Loss: 1.699 | Acc: 39.018% (17455/44736)\n",
      "Loss: 1.699 | Acc: 39.036% (17488/44800)\n",
      "Loss: 1.698 | Acc: 39.049% (17519/44864)\n",
      "Loss: 1.698 | Acc: 39.062% (17550/44928)\n",
      "Loss: 1.698 | Acc: 39.085% (17585/44992)\n",
      "Loss: 1.697 | Acc: 39.096% (17615/45056)\n",
      "Loss: 1.697 | Acc: 39.091% (17638/45120)\n",
      "Loss: 1.697 | Acc: 39.098% (17666/45184)\n",
      "Loss: 1.697 | Acc: 39.102% (17693/45248)\n",
      "Loss: 1.696 | Acc: 39.113% (17723/45312)\n",
      "Loss: 1.696 | Acc: 39.137% (17759/45376)\n",
      "Loss: 1.695 | Acc: 39.157% (17793/45440)\n",
      "Loss: 1.695 | Acc: 39.177% (17827/45504)\n",
      "Loss: 1.695 | Acc: 39.183% (17855/45568)\n",
      "Loss: 1.695 | Acc: 39.190% (17883/45632)\n",
      "Loss: 1.694 | Acc: 39.209% (17917/45696)\n",
      "Loss: 1.694 | Acc: 39.213% (17944/45760)\n",
      "Loss: 1.694 | Acc: 39.215% (17970/45824)\n",
      "Loss: 1.693 | Acc: 39.232% (18003/45888)\n",
      "Loss: 1.693 | Acc: 39.245% (18034/45952)\n",
      "Loss: 1.692 | Acc: 39.256% (18064/46016)\n",
      "Loss: 1.692 | Acc: 39.266% (18094/46080)\n",
      "Loss: 1.691 | Acc: 39.286% (18128/46144)\n",
      "Loss: 1.691 | Acc: 39.298% (18159/46208)\n",
      "Loss: 1.690 | Acc: 39.318% (18193/46272)\n",
      "Loss: 1.690 | Acc: 39.341% (18229/46336)\n",
      "Loss: 1.689 | Acc: 39.356% (18261/46400)\n",
      "Loss: 1.689 | Acc: 39.372% (18294/46464)\n",
      "Loss: 1.689 | Acc: 39.374% (18320/46528)\n",
      "Loss: 1.689 | Acc: 39.387% (18351/46592)\n",
      "Loss: 1.688 | Acc: 39.382% (18374/46656)\n",
      "Loss: 1.688 | Acc: 39.381% (18399/46720)\n",
      "Loss: 1.688 | Acc: 39.394% (18430/46784)\n",
      "Loss: 1.687 | Acc: 39.400% (18458/46848)\n",
      "Loss: 1.687 | Acc: 39.414% (18490/46912)\n",
      "Loss: 1.687 | Acc: 39.407% (18512/46976)\n",
      "Loss: 1.686 | Acc: 39.435% (18550/47040)\n",
      "Loss: 1.686 | Acc: 39.447% (18581/47104)\n",
      "Loss: 1.686 | Acc: 39.463% (18614/47168)\n",
      "Loss: 1.686 | Acc: 39.463% (18639/47232)\n",
      "Loss: 1.686 | Acc: 39.464% (18665/47296)\n",
      "Loss: 1.685 | Acc: 39.483% (18699/47360)\n",
      "Loss: 1.685 | Acc: 39.499% (18732/47424)\n",
      "Loss: 1.684 | Acc: 39.517% (18766/47488)\n",
      "Loss: 1.683 | Acc: 39.531% (18798/47552)\n",
      "Loss: 1.684 | Acc: 39.525% (18820/47616)\n",
      "Loss: 1.683 | Acc: 39.545% (18855/47680)\n",
      "Loss: 1.683 | Acc: 39.546% (18881/47744)\n",
      "Loss: 1.683 | Acc: 39.567% (18916/47808)\n",
      "Loss: 1.682 | Acc: 39.566% (18941/47872)\n",
      "Loss: 1.682 | Acc: 39.576% (18971/47936)\n",
      "Loss: 1.681 | Acc: 39.606% (19011/48000)\n",
      "Loss: 1.681 | Acc: 39.616% (19041/48064)\n",
      "Loss: 1.681 | Acc: 39.632% (19074/48128)\n",
      "Loss: 1.681 | Acc: 39.641% (19104/48192)\n",
      "Loss: 1.681 | Acc: 39.657% (19137/48256)\n",
      "Loss: 1.680 | Acc: 39.671% (19169/48320)\n",
      "Loss: 1.680 | Acc: 39.685% (19201/48384)\n",
      "Loss: 1.679 | Acc: 39.706% (19237/48448)\n",
      "Loss: 1.679 | Acc: 39.716% (19267/48512)\n",
      "Loss: 1.679 | Acc: 39.717% (19293/48576)\n",
      "Loss: 1.678 | Acc: 39.737% (19328/48640)\n",
      "Loss: 1.678 | Acc: 39.730% (19350/48704)\n",
      "Loss: 1.678 | Acc: 39.729% (19375/48768)\n",
      "Loss: 1.678 | Acc: 39.736% (19404/48832)\n",
      "Loss: 1.678 | Acc: 39.748% (19435/48896)\n",
      "Loss: 1.677 | Acc: 39.771% (19472/48960)\n",
      "Loss: 1.677 | Acc: 39.782% (19493/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 39.78163265306122\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.433 | Acc: 43.750% (28/64)\n",
      "Loss: 1.339 | Acc: 48.438% (62/128)\n",
      "Loss: 1.438 | Acc: 45.312% (87/192)\n",
      "Loss: 1.512 | Acc: 43.359% (111/256)\n",
      "Loss: 1.466 | Acc: 45.938% (147/320)\n",
      "Loss: 1.496 | Acc: 45.573% (175/384)\n",
      "Loss: 1.506 | Acc: 44.196% (198/448)\n",
      "Loss: 1.501 | Acc: 43.750% (224/512)\n",
      "Loss: 1.471 | Acc: 45.139% (260/576)\n",
      "Loss: 1.471 | Acc: 44.688% (286/640)\n",
      "Loss: 1.485 | Acc: 44.318% (312/704)\n",
      "Loss: 1.486 | Acc: 44.141% (339/768)\n",
      "Loss: 1.493 | Acc: 44.231% (368/832)\n",
      "Loss: 1.483 | Acc: 44.420% (398/896)\n",
      "Loss: 1.474 | Acc: 44.688% (429/960)\n",
      "Loss: 1.457 | Acc: 45.605% (467/1024)\n",
      "Loss: 1.454 | Acc: 46.232% (503/1088)\n",
      "Loss: 1.447 | Acc: 46.441% (535/1152)\n",
      "Loss: 1.444 | Acc: 46.875% (570/1216)\n",
      "Loss: 1.472 | Acc: 46.250% (592/1280)\n",
      "Loss: 1.475 | Acc: 45.685% (614/1344)\n",
      "Loss: 1.477 | Acc: 45.739% (644/1408)\n",
      "Loss: 1.466 | Acc: 46.264% (681/1472)\n",
      "Loss: 1.475 | Acc: 46.419% (713/1536)\n",
      "Loss: 1.474 | Acc: 46.312% (741/1600)\n",
      "Loss: 1.482 | Acc: 46.094% (767/1664)\n",
      "Loss: 1.481 | Acc: 46.123% (797/1728)\n",
      "Loss: 1.479 | Acc: 46.317% (830/1792)\n",
      "Loss: 1.482 | Acc: 46.175% (857/1856)\n",
      "Loss: 1.487 | Acc: 46.354% (890/1920)\n",
      "Loss: 1.483 | Acc: 46.623% (925/1984)\n",
      "Loss: 1.490 | Acc: 46.338% (949/2048)\n",
      "Loss: 1.481 | Acc: 46.496% (982/2112)\n",
      "Loss: 1.480 | Acc: 46.599% (1014/2176)\n",
      "Loss: 1.478 | Acc: 46.696% (1046/2240)\n",
      "Loss: 1.477 | Acc: 46.615% (1074/2304)\n",
      "Loss: 1.477 | Acc: 46.664% (1105/2368)\n",
      "Loss: 1.478 | Acc: 46.669% (1135/2432)\n",
      "Loss: 1.479 | Acc: 46.635% (1164/2496)\n",
      "Loss: 1.487 | Acc: 46.250% (1184/2560)\n",
      "Loss: 1.490 | Acc: 45.960% (1206/2624)\n",
      "Loss: 1.489 | Acc: 45.982% (1236/2688)\n",
      "Loss: 1.486 | Acc: 45.930% (1264/2752)\n",
      "Loss: 1.490 | Acc: 45.703% (1287/2816)\n",
      "Loss: 1.492 | Acc: 45.764% (1318/2880)\n",
      "Loss: 1.488 | Acc: 45.890% (1351/2944)\n",
      "Loss: 1.489 | Acc: 45.844% (1379/3008)\n",
      "Loss: 1.490 | Acc: 45.898% (1410/3072)\n",
      "Loss: 1.492 | Acc: 45.663% (1432/3136)\n",
      "Loss: 1.488 | Acc: 45.750% (1464/3200)\n",
      "Loss: 1.493 | Acc: 45.588% (1488/3264)\n",
      "Loss: 1.493 | Acc: 45.493% (1514/3328)\n",
      "Loss: 1.491 | Acc: 45.666% (1549/3392)\n",
      "Loss: 1.493 | Acc: 45.660% (1578/3456)\n",
      "Loss: 1.493 | Acc: 45.540% (1603/3520)\n",
      "Loss: 1.491 | Acc: 45.647% (1636/3584)\n",
      "Loss: 1.491 | Acc: 45.751% (1669/3648)\n",
      "Loss: 1.488 | Acc: 45.878% (1703/3712)\n",
      "Loss: 1.489 | Acc: 45.895% (1733/3776)\n",
      "Loss: 1.489 | Acc: 45.729% (1756/3840)\n",
      "Loss: 1.489 | Acc: 45.697% (1784/3904)\n",
      "Loss: 1.489 | Acc: 45.691% (1813/3968)\n",
      "Loss: 1.489 | Acc: 45.759% (1845/4032)\n",
      "Loss: 1.490 | Acc: 45.703% (1872/4096)\n",
      "Loss: 1.493 | Acc: 45.673% (1900/4160)\n",
      "Loss: 1.492 | Acc: 45.762% (1933/4224)\n",
      "Loss: 1.493 | Acc: 45.662% (1958/4288)\n",
      "Loss: 1.491 | Acc: 45.680% (1988/4352)\n",
      "Loss: 1.488 | Acc: 45.743% (2020/4416)\n",
      "Loss: 1.487 | Acc: 45.871% (2055/4480)\n",
      "Loss: 1.488 | Acc: 45.863% (2084/4544)\n",
      "Loss: 1.491 | Acc: 45.660% (2104/4608)\n",
      "Loss: 1.489 | Acc: 45.634% (2132/4672)\n",
      "Loss: 1.490 | Acc: 45.545% (2157/4736)\n",
      "Loss: 1.491 | Acc: 45.562% (2187/4800)\n",
      "Loss: 1.488 | Acc: 45.641% (2220/4864)\n",
      "Loss: 1.486 | Acc: 45.779% (2256/4928)\n",
      "Loss: 1.485 | Acc: 45.813% (2287/4992)\n",
      "Loss: 1.484 | Acc: 45.847% (2318/5056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.485 | Acc: 45.879% (2349/5120)\n",
      "Loss: 1.484 | Acc: 46.007% (2385/5184)\n",
      "Loss: 1.485 | Acc: 45.979% (2413/5248)\n",
      "Loss: 1.485 | Acc: 45.971% (2442/5312)\n",
      "Loss: 1.488 | Acc: 45.889% (2467/5376)\n",
      "Loss: 1.488 | Acc: 45.919% (2498/5440)\n",
      "Loss: 1.488 | Acc: 45.858% (2524/5504)\n",
      "Loss: 1.492 | Acc: 45.797% (2550/5568)\n",
      "Loss: 1.494 | Acc: 45.774% (2578/5632)\n",
      "Loss: 1.498 | Acc: 45.681% (2602/5696)\n",
      "Loss: 1.497 | Acc: 45.712% (2633/5760)\n",
      "Loss: 1.494 | Acc: 45.793% (2667/5824)\n",
      "Loss: 1.496 | Acc: 45.720% (2692/5888)\n",
      "Loss: 1.499 | Acc: 45.665% (2718/5952)\n",
      "Loss: 1.500 | Acc: 45.612% (2744/6016)\n",
      "Loss: 1.500 | Acc: 45.526% (2768/6080)\n",
      "Loss: 1.499 | Acc: 45.589% (2801/6144)\n",
      "Loss: 1.499 | Acc: 45.586% (2830/6208)\n",
      "Loss: 1.501 | Acc: 45.456% (2851/6272)\n",
      "Loss: 1.500 | Acc: 45.533% (2885/6336)\n",
      "Loss: 1.501 | Acc: 45.484% (2911/6400)\n",
      "Loss: 1.502 | Acc: 45.421% (2936/6464)\n",
      "Loss: 1.502 | Acc: 45.450% (2967/6528)\n",
      "Loss: 1.504 | Acc: 45.373% (2991/6592)\n",
      "Loss: 1.503 | Acc: 45.418% (3023/6656)\n",
      "Loss: 1.503 | Acc: 45.327% (3046/6720)\n",
      "Loss: 1.501 | Acc: 45.445% (3083/6784)\n",
      "Loss: 1.498 | Acc: 45.546% (3119/6848)\n",
      "Loss: 1.500 | Acc: 45.501% (3145/6912)\n",
      "Loss: 1.502 | Acc: 45.456% (3171/6976)\n",
      "Loss: 1.503 | Acc: 45.412% (3197/7040)\n",
      "Loss: 1.502 | Acc: 45.510% (3233/7104)\n",
      "Loss: 1.502 | Acc: 45.536% (3264/7168)\n",
      "Loss: 1.502 | Acc: 45.561% (3295/7232)\n",
      "Loss: 1.501 | Acc: 45.573% (3325/7296)\n",
      "Loss: 1.499 | Acc: 45.639% (3359/7360)\n",
      "Loss: 1.500 | Acc: 45.609% (3386/7424)\n",
      "Loss: 1.499 | Acc: 45.700% (3422/7488)\n",
      "Loss: 1.499 | Acc: 45.710% (3452/7552)\n",
      "Loss: 1.500 | Acc: 45.680% (3479/7616)\n",
      "Loss: 1.499 | Acc: 45.729% (3512/7680)\n",
      "Loss: 1.498 | Acc: 45.777% (3545/7744)\n",
      "Loss: 1.499 | Acc: 45.799% (3576/7808)\n",
      "Loss: 1.498 | Acc: 45.821% (3607/7872)\n",
      "Loss: 1.497 | Acc: 45.817% (3636/7936)\n",
      "Loss: 1.498 | Acc: 45.800% (3664/8000)\n",
      "Loss: 1.497 | Acc: 45.858% (3698/8064)\n",
      "Loss: 1.498 | Acc: 45.854% (3727/8128)\n",
      "Loss: 1.497 | Acc: 45.862% (3757/8192)\n",
      "Loss: 1.500 | Acc: 45.809% (3782/8256)\n",
      "Loss: 1.502 | Acc: 45.709% (3803/8320)\n",
      "Loss: 1.502 | Acc: 45.742% (3835/8384)\n",
      "Loss: 1.501 | Acc: 45.798% (3869/8448)\n",
      "Loss: 1.502 | Acc: 45.759% (3895/8512)\n",
      "Loss: 1.501 | Acc: 45.767% (3925/8576)\n",
      "Loss: 1.501 | Acc: 45.799% (3957/8640)\n",
      "Loss: 1.502 | Acc: 45.726% (3980/8704)\n",
      "Loss: 1.503 | Acc: 45.712% (4008/8768)\n",
      "Loss: 1.500 | Acc: 45.799% (4045/8832)\n",
      "Loss: 1.500 | Acc: 45.830% (4077/8896)\n",
      "Loss: 1.500 | Acc: 45.837% (4107/8960)\n",
      "Loss: 1.500 | Acc: 45.911% (4143/9024)\n",
      "Loss: 1.502 | Acc: 45.808% (4163/9088)\n",
      "Loss: 1.502 | Acc: 45.782% (4190/9152)\n",
      "Loss: 1.501 | Acc: 45.801% (4221/9216)\n",
      "Loss: 1.500 | Acc: 45.830% (4253/9280)\n",
      "Loss: 1.500 | Acc: 45.783% (4278/9344)\n",
      "Loss: 1.500 | Acc: 45.759% (4305/9408)\n",
      "Loss: 1.500 | Acc: 45.714% (4330/9472)\n",
      "Loss: 1.500 | Acc: 45.763% (4364/9536)\n",
      "Loss: 1.499 | Acc: 45.844% (4401/9600)\n",
      "Loss: 1.499 | Acc: 45.830% (4429/9664)\n",
      "Loss: 1.499 | Acc: 45.847% (4460/9728)\n",
      "Loss: 1.500 | Acc: 45.762% (4481/9792)\n",
      "Loss: 1.499 | Acc: 45.779% (4512/9856)\n",
      "Loss: 1.499 | Acc: 45.786% (4542/9920)\n",
      "Loss: 1.500 | Acc: 45.773% (4570/9984)\n",
      "Loss: 1.500 | Acc: 45.760% (4576/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 45.76\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.317 | Acc: 54.688% (35/64)\n",
      "Loss: 1.425 | Acc: 50.000% (64/128)\n",
      "Loss: 1.402 | Acc: 49.479% (95/192)\n",
      "Loss: 1.446 | Acc: 48.828% (125/256)\n",
      "Loss: 1.456 | Acc: 48.125% (154/320)\n",
      "Loss: 1.428 | Acc: 49.219% (189/384)\n",
      "Loss: 1.436 | Acc: 50.000% (224/448)\n",
      "Loss: 1.426 | Acc: 50.195% (257/512)\n",
      "Loss: 1.428 | Acc: 50.174% (289/576)\n",
      "Loss: 1.430 | Acc: 49.688% (318/640)\n",
      "Loss: 1.422 | Acc: 49.574% (349/704)\n",
      "Loss: 1.417 | Acc: 49.870% (383/768)\n",
      "Loss: 1.419 | Acc: 49.159% (409/832)\n",
      "Loss: 1.414 | Acc: 49.107% (440/896)\n",
      "Loss: 1.433 | Acc: 48.229% (463/960)\n",
      "Loss: 1.433 | Acc: 48.438% (496/1024)\n",
      "Loss: 1.445 | Acc: 48.070% (523/1088)\n",
      "Loss: 1.449 | Acc: 48.090% (554/1152)\n",
      "Loss: 1.451 | Acc: 47.615% (579/1216)\n",
      "Loss: 1.448 | Acc: 47.578% (609/1280)\n",
      "Loss: 1.439 | Acc: 47.917% (644/1344)\n",
      "Loss: 1.432 | Acc: 48.580% (684/1408)\n",
      "Loss: 1.430 | Acc: 48.438% (713/1472)\n",
      "Loss: 1.423 | Acc: 48.503% (745/1536)\n",
      "Loss: 1.410 | Acc: 49.312% (789/1600)\n",
      "Loss: 1.402 | Acc: 49.459% (823/1664)\n",
      "Loss: 1.410 | Acc: 49.363% (853/1728)\n",
      "Loss: 1.403 | Acc: 49.498% (887/1792)\n",
      "Loss: 1.411 | Acc: 49.084% (911/1856)\n",
      "Loss: 1.411 | Acc: 49.010% (941/1920)\n",
      "Loss: 1.416 | Acc: 48.740% (967/1984)\n",
      "Loss: 1.418 | Acc: 48.389% (991/2048)\n",
      "Loss: 1.413 | Acc: 48.438% (1023/2112)\n",
      "Loss: 1.411 | Acc: 48.529% (1056/2176)\n",
      "Loss: 1.412 | Acc: 48.348% (1083/2240)\n",
      "Loss: 1.410 | Acc: 48.264% (1112/2304)\n",
      "Loss: 1.409 | Acc: 48.226% (1142/2368)\n",
      "Loss: 1.408 | Acc: 48.314% (1175/2432)\n",
      "Loss: 1.407 | Acc: 48.317% (1206/2496)\n",
      "Loss: 1.406 | Acc: 48.594% (1244/2560)\n",
      "Loss: 1.405 | Acc: 48.628% (1276/2624)\n",
      "Loss: 1.406 | Acc: 48.772% (1311/2688)\n",
      "Loss: 1.406 | Acc: 48.765% (1342/2752)\n",
      "Loss: 1.402 | Acc: 48.864% (1376/2816)\n",
      "Loss: 1.403 | Acc: 48.785% (1405/2880)\n",
      "Loss: 1.400 | Acc: 49.015% (1443/2944)\n",
      "Loss: 1.395 | Acc: 49.069% (1476/3008)\n",
      "Loss: 1.396 | Acc: 48.991% (1505/3072)\n",
      "Loss: 1.391 | Acc: 49.043% (1538/3136)\n",
      "Loss: 1.391 | Acc: 48.969% (1567/3200)\n",
      "Loss: 1.388 | Acc: 49.173% (1605/3264)\n",
      "Loss: 1.385 | Acc: 49.279% (1640/3328)\n",
      "Loss: 1.384 | Acc: 49.204% (1669/3392)\n",
      "Loss: 1.388 | Acc: 49.132% (1698/3456)\n",
      "Loss: 1.387 | Acc: 49.205% (1732/3520)\n",
      "Loss: 1.387 | Acc: 49.275% (1766/3584)\n",
      "Loss: 1.386 | Acc: 49.342% (1800/3648)\n",
      "Loss: 1.382 | Acc: 49.623% (1842/3712)\n",
      "Loss: 1.381 | Acc: 49.709% (1877/3776)\n",
      "Loss: 1.380 | Acc: 49.688% (1908/3840)\n",
      "Loss: 1.379 | Acc: 49.821% (1945/3904)\n",
      "Loss: 1.375 | Acc: 50.000% (1984/3968)\n",
      "Loss: 1.374 | Acc: 50.074% (2019/4032)\n",
      "Loss: 1.377 | Acc: 49.951% (2046/4096)\n",
      "Loss: 1.378 | Acc: 49.856% (2074/4160)\n",
      "Loss: 1.380 | Acc: 49.787% (2103/4224)\n",
      "Loss: 1.381 | Acc: 49.697% (2131/4288)\n",
      "Loss: 1.385 | Acc: 49.655% (2161/4352)\n",
      "Loss: 1.386 | Acc: 49.592% (2190/4416)\n",
      "Loss: 1.390 | Acc: 49.509% (2218/4480)\n",
      "Loss: 1.388 | Acc: 49.670% (2257/4544)\n",
      "Loss: 1.388 | Acc: 49.631% (2287/4608)\n",
      "Loss: 1.390 | Acc: 49.551% (2315/4672)\n",
      "Loss: 1.389 | Acc: 49.557% (2347/4736)\n",
      "Loss: 1.390 | Acc: 49.479% (2375/4800)\n",
      "Loss: 1.387 | Acc: 49.548% (2410/4864)\n",
      "Loss: 1.390 | Acc: 49.330% (2431/4928)\n",
      "Loss: 1.389 | Acc: 49.399% (2466/4992)\n",
      "Loss: 1.387 | Acc: 49.506% (2503/5056)\n",
      "Loss: 1.386 | Acc: 49.551% (2537/5120)\n",
      "Loss: 1.385 | Acc: 49.556% (2569/5184)\n",
      "Loss: 1.385 | Acc: 49.600% (2603/5248)\n",
      "Loss: 1.389 | Acc: 49.511% (2630/5312)\n",
      "Loss: 1.385 | Acc: 49.572% (2665/5376)\n",
      "Loss: 1.384 | Acc: 49.504% (2693/5440)\n",
      "Loss: 1.386 | Acc: 49.419% (2720/5504)\n",
      "Loss: 1.388 | Acc: 49.335% (2747/5568)\n",
      "Loss: 1.386 | Acc: 49.396% (2782/5632)\n",
      "Loss: 1.387 | Acc: 49.456% (2817/5696)\n",
      "Loss: 1.390 | Acc: 49.375% (2844/5760)\n",
      "Loss: 1.390 | Acc: 49.296% (2871/5824)\n",
      "Loss: 1.390 | Acc: 49.287% (2902/5888)\n",
      "Loss: 1.389 | Acc: 49.311% (2935/5952)\n",
      "Loss: 1.391 | Acc: 49.352% (2969/6016)\n",
      "Loss: 1.393 | Acc: 49.293% (2997/6080)\n",
      "Loss: 1.393 | Acc: 49.300% (3029/6144)\n",
      "Loss: 1.393 | Acc: 49.356% (3064/6208)\n",
      "Loss: 1.395 | Acc: 49.267% (3090/6272)\n",
      "Loss: 1.396 | Acc: 49.227% (3119/6336)\n",
      "Loss: 1.400 | Acc: 49.125% (3144/6400)\n",
      "Loss: 1.399 | Acc: 49.118% (3175/6464)\n",
      "Loss: 1.398 | Acc: 49.142% (3208/6528)\n",
      "Loss: 1.399 | Acc: 49.059% (3234/6592)\n",
      "Loss: 1.399 | Acc: 49.069% (3266/6656)\n",
      "Loss: 1.399 | Acc: 49.107% (3300/6720)\n",
      "Loss: 1.398 | Acc: 49.145% (3334/6784)\n",
      "Loss: 1.399 | Acc: 49.080% (3361/6848)\n",
      "Loss: 1.400 | Acc: 49.060% (3391/6912)\n",
      "Loss: 1.400 | Acc: 49.083% (3424/6976)\n",
      "Loss: 1.399 | Acc: 49.091% (3456/7040)\n",
      "Loss: 1.398 | Acc: 49.169% (3493/7104)\n",
      "Loss: 1.397 | Acc: 49.149% (3523/7168)\n",
      "Loss: 1.398 | Acc: 49.129% (3553/7232)\n",
      "Loss: 1.398 | Acc: 49.137% (3585/7296)\n",
      "Loss: 1.400 | Acc: 49.076% (3612/7360)\n",
      "Loss: 1.401 | Acc: 49.003% (3638/7424)\n",
      "Loss: 1.402 | Acc: 48.985% (3668/7488)\n",
      "Loss: 1.404 | Acc: 48.941% (3696/7552)\n",
      "Loss: 1.403 | Acc: 48.963% (3729/7616)\n",
      "Loss: 1.403 | Acc: 48.945% (3759/7680)\n",
      "Loss: 1.404 | Acc: 48.941% (3790/7744)\n",
      "Loss: 1.404 | Acc: 48.963% (3823/7808)\n",
      "Loss: 1.405 | Acc: 48.946% (3853/7872)\n",
      "Loss: 1.403 | Acc: 49.042% (3892/7936)\n",
      "Loss: 1.402 | Acc: 49.075% (3926/8000)\n",
      "Loss: 1.401 | Acc: 49.144% (3963/8064)\n",
      "Loss: 1.402 | Acc: 49.163% (3996/8128)\n",
      "Loss: 1.402 | Acc: 49.170% (4028/8192)\n",
      "Loss: 1.402 | Acc: 49.237% (4065/8256)\n",
      "Loss: 1.403 | Acc: 49.147% (4089/8320)\n",
      "Loss: 1.402 | Acc: 49.165% (4122/8384)\n",
      "Loss: 1.402 | Acc: 49.242% (4160/8448)\n",
      "Loss: 1.401 | Acc: 49.307% (4197/8512)\n",
      "Loss: 1.401 | Acc: 49.324% (4230/8576)\n",
      "Loss: 1.401 | Acc: 49.317% (4261/8640)\n",
      "Loss: 1.401 | Acc: 49.311% (4292/8704)\n",
      "Loss: 1.401 | Acc: 49.339% (4326/8768)\n",
      "Loss: 1.401 | Acc: 49.298% (4354/8832)\n",
      "Loss: 1.400 | Acc: 49.348% (4390/8896)\n",
      "Loss: 1.399 | Acc: 49.397% (4426/8960)\n",
      "Loss: 1.400 | Acc: 49.379% (4456/9024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.400 | Acc: 49.406% (4490/9088)\n",
      "Loss: 1.399 | Acc: 49.432% (4524/9152)\n",
      "Loss: 1.400 | Acc: 49.425% (4555/9216)\n",
      "Loss: 1.401 | Acc: 49.418% (4586/9280)\n",
      "Loss: 1.400 | Acc: 49.422% (4618/9344)\n",
      "Loss: 1.400 | Acc: 49.384% (4646/9408)\n",
      "Loss: 1.400 | Acc: 49.377% (4677/9472)\n",
      "Loss: 1.400 | Acc: 49.339% (4705/9536)\n",
      "Loss: 1.401 | Acc: 49.344% (4737/9600)\n",
      "Loss: 1.401 | Acc: 49.265% (4761/9664)\n",
      "Loss: 1.400 | Acc: 49.332% (4799/9728)\n",
      "Loss: 1.400 | Acc: 49.316% (4829/9792)\n",
      "Loss: 1.399 | Acc: 49.330% (4862/9856)\n",
      "Loss: 1.397 | Acc: 49.405% (4901/9920)\n",
      "Loss: 1.398 | Acc: 49.359% (4928/9984)\n",
      "Loss: 1.399 | Acc: 49.353% (4959/10048)\n",
      "Loss: 1.399 | Acc: 49.347% (4990/10112)\n",
      "Loss: 1.397 | Acc: 49.401% (5027/10176)\n",
      "Loss: 1.396 | Acc: 49.385% (5057/10240)\n",
      "Loss: 1.397 | Acc: 49.369% (5087/10304)\n",
      "Loss: 1.398 | Acc: 49.373% (5119/10368)\n",
      "Loss: 1.398 | Acc: 49.387% (5152/10432)\n",
      "Loss: 1.397 | Acc: 49.419% (5187/10496)\n",
      "Loss: 1.397 | Acc: 49.394% (5216/10560)\n",
      "Loss: 1.397 | Acc: 49.416% (5250/10624)\n",
      "Loss: 1.397 | Acc: 49.429% (5283/10688)\n",
      "Loss: 1.397 | Acc: 49.423% (5314/10752)\n",
      "Loss: 1.397 | Acc: 49.399% (5343/10816)\n",
      "Loss: 1.398 | Acc: 49.347% (5369/10880)\n",
      "Loss: 1.398 | Acc: 49.324% (5398/10944)\n",
      "Loss: 1.398 | Acc: 49.291% (5426/11008)\n",
      "Loss: 1.398 | Acc: 49.296% (5458/11072)\n",
      "Loss: 1.398 | Acc: 49.300% (5490/11136)\n",
      "Loss: 1.398 | Acc: 49.295% (5521/11200)\n",
      "Loss: 1.397 | Acc: 49.316% (5555/11264)\n",
      "Loss: 1.397 | Acc: 49.320% (5587/11328)\n",
      "Loss: 1.397 | Acc: 49.315% (5618/11392)\n",
      "Loss: 1.398 | Acc: 49.258% (5643/11456)\n",
      "Loss: 1.398 | Acc: 49.227% (5671/11520)\n",
      "Loss: 1.399 | Acc: 49.232% (5703/11584)\n",
      "Loss: 1.397 | Acc: 49.287% (5741/11648)\n",
      "Loss: 1.399 | Acc: 49.249% (5768/11712)\n",
      "Loss: 1.398 | Acc: 49.312% (5807/11776)\n",
      "Loss: 1.398 | Acc: 49.316% (5839/11840)\n",
      "Loss: 1.398 | Acc: 49.370% (5877/11904)\n",
      "Loss: 1.398 | Acc: 49.373% (5909/11968)\n",
      "Loss: 1.399 | Acc: 49.335% (5936/12032)\n",
      "Loss: 1.399 | Acc: 49.339% (5968/12096)\n",
      "Loss: 1.399 | Acc: 49.400% (6007/12160)\n",
      "Loss: 1.399 | Acc: 49.411% (6040/12224)\n",
      "Loss: 1.397 | Acc: 49.495% (6082/12288)\n",
      "Loss: 1.397 | Acc: 49.498% (6114/12352)\n",
      "Loss: 1.397 | Acc: 49.468% (6142/12416)\n",
      "Loss: 1.397 | Acc: 49.511% (6179/12480)\n",
      "Loss: 1.399 | Acc: 49.506% (6210/12544)\n",
      "Loss: 1.398 | Acc: 49.508% (6242/12608)\n",
      "Loss: 1.397 | Acc: 49.590% (6284/12672)\n",
      "Loss: 1.398 | Acc: 49.560% (6312/12736)\n",
      "Loss: 1.399 | Acc: 49.508% (6337/12800)\n",
      "Loss: 1.399 | Acc: 49.502% (6368/12864)\n",
      "Loss: 1.399 | Acc: 49.497% (6399/12928)\n",
      "Loss: 1.399 | Acc: 49.477% (6428/12992)\n",
      "Loss: 1.398 | Acc: 49.479% (6460/13056)\n",
      "Loss: 1.401 | Acc: 49.413% (6483/13120)\n",
      "Loss: 1.399 | Acc: 49.439% (6518/13184)\n",
      "Loss: 1.399 | Acc: 49.449% (6551/13248)\n",
      "Loss: 1.399 | Acc: 49.452% (6583/13312)\n",
      "Loss: 1.398 | Acc: 49.492% (6620/13376)\n",
      "Loss: 1.398 | Acc: 49.472% (6649/13440)\n",
      "Loss: 1.399 | Acc: 49.459% (6679/13504)\n",
      "Loss: 1.400 | Acc: 49.440% (6708/13568)\n",
      "Loss: 1.399 | Acc: 49.464% (6743/13632)\n",
      "Loss: 1.399 | Acc: 49.474% (6776/13696)\n",
      "Loss: 1.400 | Acc: 49.433% (6802/13760)\n",
      "Loss: 1.400 | Acc: 49.443% (6835/13824)\n",
      "Loss: 1.399 | Acc: 49.474% (6871/13888)\n",
      "Loss: 1.399 | Acc: 49.498% (6906/13952)\n",
      "Loss: 1.399 | Acc: 49.486% (6936/14016)\n",
      "Loss: 1.399 | Acc: 49.489% (6968/14080)\n",
      "Loss: 1.400 | Acc: 49.463% (6996/14144)\n",
      "Loss: 1.399 | Acc: 49.479% (7030/14208)\n",
      "Loss: 1.399 | Acc: 49.453% (7058/14272)\n",
      "Loss: 1.399 | Acc: 49.421% (7085/14336)\n",
      "Loss: 1.399 | Acc: 49.417% (7116/14400)\n",
      "Loss: 1.399 | Acc: 49.433% (7150/14464)\n",
      "Loss: 1.400 | Acc: 49.387% (7175/14528)\n",
      "Loss: 1.400 | Acc: 49.390% (7207/14592)\n",
      "Loss: 1.400 | Acc: 49.386% (7238/14656)\n",
      "Loss: 1.401 | Acc: 49.355% (7265/14720)\n",
      "Loss: 1.401 | Acc: 49.357% (7297/14784)\n",
      "Loss: 1.400 | Acc: 49.333% (7325/14848)\n",
      "Loss: 1.400 | Acc: 49.350% (7359/14912)\n",
      "Loss: 1.400 | Acc: 49.339% (7389/14976)\n",
      "Loss: 1.401 | Acc: 49.302% (7415/15040)\n",
      "Loss: 1.402 | Acc: 49.285% (7444/15104)\n",
      "Loss: 1.400 | Acc: 49.328% (7482/15168)\n",
      "Loss: 1.400 | Acc: 49.324% (7513/15232)\n",
      "Loss: 1.400 | Acc: 49.353% (7549/15296)\n",
      "Loss: 1.401 | Acc: 49.297% (7572/15360)\n",
      "Loss: 1.400 | Acc: 49.313% (7606/15424)\n",
      "Loss: 1.401 | Acc: 49.277% (7632/15488)\n",
      "Loss: 1.400 | Acc: 49.293% (7666/15552)\n",
      "Loss: 1.400 | Acc: 49.308% (7700/15616)\n",
      "Loss: 1.401 | Acc: 49.279% (7727/15680)\n",
      "Loss: 1.401 | Acc: 49.276% (7758/15744)\n",
      "Loss: 1.401 | Acc: 49.241% (7784/15808)\n",
      "Loss: 1.401 | Acc: 49.225% (7813/15872)\n",
      "Loss: 1.401 | Acc: 49.209% (7842/15936)\n",
      "Loss: 1.401 | Acc: 49.206% (7873/16000)\n",
      "Loss: 1.400 | Acc: 49.247% (7911/16064)\n",
      "Loss: 1.400 | Acc: 49.268% (7946/16128)\n",
      "Loss: 1.400 | Acc: 49.253% (7975/16192)\n",
      "Loss: 1.399 | Acc: 49.250% (8006/16256)\n",
      "Loss: 1.400 | Acc: 49.216% (8032/16320)\n",
      "Loss: 1.400 | Acc: 49.219% (8064/16384)\n",
      "Loss: 1.399 | Acc: 49.277% (8105/16448)\n",
      "Loss: 1.399 | Acc: 49.285% (8138/16512)\n",
      "Loss: 1.399 | Acc: 49.300% (8172/16576)\n",
      "Loss: 1.399 | Acc: 49.303% (8204/16640)\n",
      "Loss: 1.398 | Acc: 49.335% (8241/16704)\n",
      "Loss: 1.398 | Acc: 49.362% (8277/16768)\n",
      "Loss: 1.397 | Acc: 49.358% (8308/16832)\n",
      "Loss: 1.397 | Acc: 49.355% (8339/16896)\n",
      "Loss: 1.397 | Acc: 49.340% (8368/16960)\n",
      "Loss: 1.396 | Acc: 49.395% (8409/17024)\n",
      "Loss: 1.396 | Acc: 49.397% (8441/17088)\n",
      "Loss: 1.395 | Acc: 49.452% (8482/17152)\n",
      "Loss: 1.394 | Acc: 49.512% (8524/17216)\n",
      "Loss: 1.394 | Acc: 49.520% (8557/17280)\n",
      "Loss: 1.393 | Acc: 49.533% (8591/17344)\n",
      "Loss: 1.393 | Acc: 49.558% (8627/17408)\n",
      "Loss: 1.393 | Acc: 49.594% (8665/17472)\n",
      "Loss: 1.394 | Acc: 49.578% (8694/17536)\n",
      "Loss: 1.394 | Acc: 49.562% (8723/17600)\n",
      "Loss: 1.393 | Acc: 49.609% (8763/17664)\n",
      "Loss: 1.393 | Acc: 49.605% (8794/17728)\n",
      "Loss: 1.393 | Acc: 49.623% (8829/17792)\n",
      "Loss: 1.392 | Acc: 49.658% (8867/17856)\n",
      "Loss: 1.393 | Acc: 49.660% (8899/17920)\n",
      "Loss: 1.394 | Acc: 49.611% (8922/17984)\n",
      "Loss: 1.394 | Acc: 49.612% (8954/18048)\n",
      "Loss: 1.393 | Acc: 49.602% (8984/18112)\n",
      "Loss: 1.394 | Acc: 49.615% (9018/18176)\n",
      "Loss: 1.394 | Acc: 49.589% (9045/18240)\n",
      "Loss: 1.393 | Acc: 49.601% (9079/18304)\n",
      "Loss: 1.393 | Acc: 49.624% (9115/18368)\n",
      "Loss: 1.392 | Acc: 49.642% (9150/18432)\n",
      "Loss: 1.392 | Acc: 49.632% (9180/18496)\n",
      "Loss: 1.392 | Acc: 49.639% (9213/18560)\n",
      "Loss: 1.392 | Acc: 49.651% (9247/18624)\n",
      "Loss: 1.392 | Acc: 49.652% (9279/18688)\n",
      "Loss: 1.392 | Acc: 49.659% (9312/18752)\n",
      "Loss: 1.391 | Acc: 49.702% (9352/18816)\n",
      "Loss: 1.391 | Acc: 49.714% (9386/18880)\n",
      "Loss: 1.391 | Acc: 49.715% (9418/18944)\n",
      "Loss: 1.391 | Acc: 49.726% (9452/19008)\n",
      "Loss: 1.391 | Acc: 49.712% (9481/19072)\n",
      "Loss: 1.390 | Acc: 49.739% (9518/19136)\n",
      "Loss: 1.390 | Acc: 49.724% (9547/19200)\n",
      "Loss: 1.389 | Acc: 49.730% (9580/19264)\n",
      "Loss: 1.388 | Acc: 49.767% (9619/19328)\n",
      "Loss: 1.388 | Acc: 49.809% (9659/19392)\n",
      "Loss: 1.388 | Acc: 49.779% (9685/19456)\n",
      "Loss: 1.388 | Acc: 49.780% (9717/19520)\n",
      "Loss: 1.388 | Acc: 49.765% (9746/19584)\n",
      "Loss: 1.388 | Acc: 49.791% (9783/19648)\n",
      "Loss: 1.387 | Acc: 49.792% (9815/19712)\n",
      "Loss: 1.387 | Acc: 49.793% (9847/19776)\n",
      "Loss: 1.387 | Acc: 49.808% (9882/19840)\n",
      "Loss: 1.387 | Acc: 49.829% (9918/19904)\n",
      "Loss: 1.387 | Acc: 49.790% (9942/19968)\n",
      "Loss: 1.388 | Acc: 49.795% (9975/20032)\n",
      "Loss: 1.387 | Acc: 49.801% (10008/20096)\n",
      "Loss: 1.387 | Acc: 49.821% (10044/20160)\n",
      "Loss: 1.387 | Acc: 49.812% (10074/20224)\n",
      "Loss: 1.387 | Acc: 49.788% (10101/20288)\n",
      "Loss: 1.387 | Acc: 49.794% (10134/20352)\n",
      "Loss: 1.388 | Acc: 49.765% (10160/20416)\n",
      "Loss: 1.388 | Acc: 49.761% (10191/20480)\n",
      "Loss: 1.388 | Acc: 49.791% (10229/20544)\n",
      "Loss: 1.388 | Acc: 49.791% (10261/20608)\n",
      "Loss: 1.388 | Acc: 49.797% (10294/20672)\n",
      "Loss: 1.388 | Acc: 49.783% (10323/20736)\n",
      "Loss: 1.388 | Acc: 49.760% (10350/20800)\n",
      "Loss: 1.388 | Acc: 49.784% (10387/20864)\n",
      "Loss: 1.388 | Acc: 49.775% (10417/20928)\n",
      "Loss: 1.387 | Acc: 49.809% (10456/20992)\n",
      "Loss: 1.387 | Acc: 49.834% (10493/21056)\n",
      "Loss: 1.386 | Acc: 49.858% (10530/21120)\n",
      "Loss: 1.385 | Acc: 49.896% (10570/21184)\n",
      "Loss: 1.385 | Acc: 49.896% (10602/21248)\n",
      "Loss: 1.385 | Acc: 49.878% (10630/21312)\n",
      "Loss: 1.385 | Acc: 49.878% (10662/21376)\n",
      "Loss: 1.385 | Acc: 49.921% (10703/21440)\n",
      "Loss: 1.385 | Acc: 49.921% (10735/21504)\n",
      "Loss: 1.384 | Acc: 49.926% (10768/21568)\n",
      "Loss: 1.385 | Acc: 49.898% (10794/21632)\n",
      "Loss: 1.384 | Acc: 49.908% (10828/21696)\n",
      "Loss: 1.384 | Acc: 49.913% (10861/21760)\n",
      "Loss: 1.384 | Acc: 49.918% (10894/21824)\n",
      "Loss: 1.384 | Acc: 49.931% (10929/21888)\n",
      "Loss: 1.384 | Acc: 49.936% (10962/21952)\n",
      "Loss: 1.384 | Acc: 49.945% (10996/22016)\n",
      "Loss: 1.383 | Acc: 49.955% (11030/22080)\n",
      "Loss: 1.382 | Acc: 49.973% (11066/22144)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.383 | Acc: 49.959% (11095/22208)\n",
      "Loss: 1.383 | Acc: 49.946% (11124/22272)\n",
      "Loss: 1.383 | Acc: 49.937% (11154/22336)\n",
      "Loss: 1.383 | Acc: 49.951% (11189/22400)\n",
      "Loss: 1.382 | Acc: 49.969% (11225/22464)\n",
      "Loss: 1.382 | Acc: 49.991% (11262/22528)\n",
      "Loss: 1.382 | Acc: 49.987% (11293/22592)\n",
      "Loss: 1.382 | Acc: 49.996% (11327/22656)\n",
      "Loss: 1.383 | Acc: 49.996% (11359/22720)\n",
      "Loss: 1.382 | Acc: 49.991% (11390/22784)\n",
      "Loss: 1.382 | Acc: 50.000% (11424/22848)\n",
      "Loss: 1.382 | Acc: 50.004% (11457/22912)\n",
      "Loss: 1.381 | Acc: 50.017% (11492/22976)\n",
      "Loss: 1.381 | Acc: 50.017% (11524/23040)\n",
      "Loss: 1.381 | Acc: 50.013% (11555/23104)\n",
      "Loss: 1.382 | Acc: 50.004% (11585/23168)\n",
      "Loss: 1.382 | Acc: 49.996% (11615/23232)\n",
      "Loss: 1.382 | Acc: 50.021% (11653/23296)\n",
      "Loss: 1.382 | Acc: 50.004% (11681/23360)\n",
      "Loss: 1.382 | Acc: 50.017% (11716/23424)\n",
      "Loss: 1.382 | Acc: 50.017% (11748/23488)\n",
      "Loss: 1.381 | Acc: 50.042% (11786/23552)\n",
      "Loss: 1.381 | Acc: 50.030% (11815/23616)\n",
      "Loss: 1.381 | Acc: 50.046% (11851/23680)\n",
      "Loss: 1.381 | Acc: 50.042% (11882/23744)\n",
      "Loss: 1.381 | Acc: 50.038% (11913/23808)\n",
      "Loss: 1.380 | Acc: 50.046% (11947/23872)\n",
      "Loss: 1.380 | Acc: 50.042% (11978/23936)\n",
      "Loss: 1.380 | Acc: 50.062% (12015/24000)\n",
      "Loss: 1.380 | Acc: 50.054% (12045/24064)\n",
      "Loss: 1.380 | Acc: 50.058% (12078/24128)\n",
      "Loss: 1.380 | Acc: 50.058% (12110/24192)\n",
      "Loss: 1.380 | Acc: 50.074% (12146/24256)\n",
      "Loss: 1.380 | Acc: 50.095% (12183/24320)\n",
      "Loss: 1.379 | Acc: 50.123% (12222/24384)\n",
      "Loss: 1.379 | Acc: 50.151% (12261/24448)\n",
      "Loss: 1.379 | Acc: 50.126% (12287/24512)\n",
      "Loss: 1.379 | Acc: 50.126% (12319/24576)\n",
      "Loss: 1.379 | Acc: 50.122% (12350/24640)\n",
      "Loss: 1.379 | Acc: 50.121% (12382/24704)\n",
      "Loss: 1.379 | Acc: 50.125% (12415/24768)\n",
      "Loss: 1.379 | Acc: 50.125% (12447/24832)\n",
      "Loss: 1.379 | Acc: 50.125% (12479/24896)\n",
      "Loss: 1.379 | Acc: 50.128% (12512/24960)\n",
      "Loss: 1.380 | Acc: 50.100% (12537/25024)\n",
      "Loss: 1.380 | Acc: 50.108% (12571/25088)\n",
      "Loss: 1.380 | Acc: 50.080% (12596/25152)\n",
      "Loss: 1.381 | Acc: 50.059% (12623/25216)\n",
      "Loss: 1.381 | Acc: 50.051% (12653/25280)\n",
      "Loss: 1.381 | Acc: 50.051% (12685/25344)\n",
      "Loss: 1.381 | Acc: 50.063% (12720/25408)\n",
      "Loss: 1.381 | Acc: 50.051% (12749/25472)\n",
      "Loss: 1.381 | Acc: 50.039% (12778/25536)\n",
      "Loss: 1.381 | Acc: 50.023% (12806/25600)\n",
      "Loss: 1.382 | Acc: 49.992% (12830/25664)\n",
      "Loss: 1.383 | Acc: 49.961% (12854/25728)\n",
      "Loss: 1.383 | Acc: 49.953% (12884/25792)\n",
      "Loss: 1.383 | Acc: 49.938% (12912/25856)\n",
      "Loss: 1.383 | Acc: 49.958% (12949/25920)\n",
      "Loss: 1.382 | Acc: 49.977% (12986/25984)\n",
      "Loss: 1.382 | Acc: 49.988% (13021/26048)\n",
      "Loss: 1.382 | Acc: 50.004% (13057/26112)\n",
      "Loss: 1.381 | Acc: 50.000% (13088/26176)\n",
      "Loss: 1.381 | Acc: 50.004% (13121/26240)\n",
      "Loss: 1.381 | Acc: 50.015% (13156/26304)\n",
      "Loss: 1.381 | Acc: 50.008% (13186/26368)\n",
      "Loss: 1.381 | Acc: 50.011% (13219/26432)\n",
      "Loss: 1.381 | Acc: 50.030% (13256/26496)\n",
      "Loss: 1.381 | Acc: 50.038% (13290/26560)\n",
      "Loss: 1.381 | Acc: 50.045% (13324/26624)\n",
      "Loss: 1.380 | Acc: 50.056% (13359/26688)\n",
      "Loss: 1.380 | Acc: 50.067% (13394/26752)\n",
      "Loss: 1.380 | Acc: 50.078% (13429/26816)\n",
      "Loss: 1.379 | Acc: 50.089% (13464/26880)\n",
      "Loss: 1.379 | Acc: 50.078% (13493/26944)\n",
      "Loss: 1.379 | Acc: 50.096% (13530/27008)\n",
      "Loss: 1.379 | Acc: 50.092% (13561/27072)\n",
      "Loss: 1.379 | Acc: 50.096% (13594/27136)\n",
      "Loss: 1.379 | Acc: 50.125% (13634/27200)\n",
      "Loss: 1.379 | Acc: 50.095% (13658/27264)\n",
      "Loss: 1.379 | Acc: 50.106% (13693/27328)\n",
      "Loss: 1.378 | Acc: 50.117% (13728/27392)\n",
      "Loss: 1.378 | Acc: 50.098% (13755/27456)\n",
      "Loss: 1.379 | Acc: 50.113% (13791/27520)\n",
      "Loss: 1.379 | Acc: 50.116% (13824/27584)\n",
      "Loss: 1.378 | Acc: 50.127% (13859/27648)\n",
      "Loss: 1.378 | Acc: 50.119% (13889/27712)\n",
      "Loss: 1.378 | Acc: 50.101% (13916/27776)\n",
      "Loss: 1.378 | Acc: 50.115% (13952/27840)\n",
      "Loss: 1.378 | Acc: 50.108% (13982/27904)\n",
      "Loss: 1.378 | Acc: 50.122% (14018/27968)\n",
      "Loss: 1.378 | Acc: 50.093% (14042/28032)\n",
      "Loss: 1.378 | Acc: 50.085% (14072/28096)\n",
      "Loss: 1.378 | Acc: 50.085% (14104/28160)\n",
      "Loss: 1.379 | Acc: 50.060% (14129/28224)\n",
      "Loss: 1.379 | Acc: 50.053% (14159/28288)\n",
      "Loss: 1.379 | Acc: 50.056% (14192/28352)\n",
      "Loss: 1.380 | Acc: 50.063% (14226/28416)\n",
      "Loss: 1.379 | Acc: 50.088% (14265/28480)\n",
      "Loss: 1.379 | Acc: 50.098% (14300/28544)\n",
      "Loss: 1.379 | Acc: 50.077% (14326/28608)\n",
      "Loss: 1.379 | Acc: 50.066% (14355/28672)\n",
      "Loss: 1.379 | Acc: 50.042% (14380/28736)\n",
      "Loss: 1.379 | Acc: 50.062% (14418/28800)\n",
      "Loss: 1.378 | Acc: 50.090% (14458/28864)\n",
      "Loss: 1.378 | Acc: 50.090% (14490/28928)\n",
      "Loss: 1.379 | Acc: 50.072% (14517/28992)\n",
      "Loss: 1.378 | Acc: 50.089% (14554/29056)\n",
      "Loss: 1.378 | Acc: 50.079% (14583/29120)\n",
      "Loss: 1.378 | Acc: 50.099% (14621/29184)\n",
      "Loss: 1.379 | Acc: 50.089% (14650/29248)\n",
      "Loss: 1.378 | Acc: 50.109% (14688/29312)\n",
      "Loss: 1.379 | Acc: 50.102% (14718/29376)\n",
      "Loss: 1.379 | Acc: 50.112% (14753/29440)\n",
      "Loss: 1.378 | Acc: 50.129% (14790/29504)\n",
      "Loss: 1.378 | Acc: 50.135% (14824/29568)\n",
      "Loss: 1.378 | Acc: 50.138% (14857/29632)\n",
      "Loss: 1.378 | Acc: 50.118% (14883/29696)\n",
      "Loss: 1.378 | Acc: 50.134% (14920/29760)\n",
      "Loss: 1.378 | Acc: 50.131% (14951/29824)\n",
      "Loss: 1.377 | Acc: 50.144% (14987/29888)\n",
      "Loss: 1.378 | Acc: 50.134% (15016/29952)\n",
      "Loss: 1.378 | Acc: 50.133% (15048/30016)\n",
      "Loss: 1.378 | Acc: 50.116% (15075/30080)\n",
      "Loss: 1.378 | Acc: 50.113% (15106/30144)\n",
      "Loss: 1.377 | Acc: 50.132% (15144/30208)\n",
      "Loss: 1.377 | Acc: 50.149% (15181/30272)\n",
      "Loss: 1.377 | Acc: 50.145% (15212/30336)\n",
      "Loss: 1.377 | Acc: 50.141% (15243/30400)\n",
      "Loss: 1.377 | Acc: 50.131% (15272/30464)\n",
      "Loss: 1.378 | Acc: 50.124% (15302/30528)\n",
      "Loss: 1.378 | Acc: 50.134% (15337/30592)\n",
      "Loss: 1.378 | Acc: 50.124% (15366/30656)\n",
      "Loss: 1.378 | Acc: 50.127% (15399/30720)\n",
      "Loss: 1.378 | Acc: 50.120% (15429/30784)\n",
      "Loss: 1.378 | Acc: 50.143% (15468/30848)\n",
      "Loss: 1.378 | Acc: 50.149% (15502/30912)\n",
      "Loss: 1.378 | Acc: 50.152% (15535/30976)\n",
      "Loss: 1.378 | Acc: 50.161% (15570/31040)\n",
      "Loss: 1.378 | Acc: 50.158% (15601/31104)\n",
      "Loss: 1.377 | Acc: 50.170% (15637/31168)\n",
      "Loss: 1.377 | Acc: 50.173% (15670/31232)\n",
      "Loss: 1.377 | Acc: 50.179% (15704/31296)\n",
      "Loss: 1.377 | Acc: 50.201% (15743/31360)\n",
      "Loss: 1.377 | Acc: 50.197% (15774/31424)\n",
      "Loss: 1.377 | Acc: 50.194% (15805/31488)\n",
      "Loss: 1.377 | Acc: 50.209% (15842/31552)\n",
      "Loss: 1.377 | Acc: 50.212% (15875/31616)\n",
      "Loss: 1.377 | Acc: 50.211% (15907/31680)\n",
      "Loss: 1.377 | Acc: 50.214% (15940/31744)\n",
      "Loss: 1.377 | Acc: 50.207% (15970/31808)\n",
      "Loss: 1.377 | Acc: 50.182% (15994/31872)\n",
      "Loss: 1.378 | Acc: 50.169% (16022/31936)\n",
      "Loss: 1.378 | Acc: 50.150% (16048/32000)\n",
      "Loss: 1.378 | Acc: 50.128% (16073/32064)\n",
      "Loss: 1.378 | Acc: 50.159% (16115/32128)\n",
      "Loss: 1.378 | Acc: 50.143% (16142/32192)\n",
      "Loss: 1.378 | Acc: 50.140% (16173/32256)\n",
      "Loss: 1.378 | Acc: 50.149% (16208/32320)\n",
      "Loss: 1.378 | Acc: 50.154% (16242/32384)\n",
      "Loss: 1.377 | Acc: 50.170% (16279/32448)\n",
      "Loss: 1.377 | Acc: 50.172% (16312/32512)\n",
      "Loss: 1.378 | Acc: 50.175% (16345/32576)\n",
      "Loss: 1.378 | Acc: 50.162% (16373/32640)\n",
      "Loss: 1.378 | Acc: 50.131% (16395/32704)\n",
      "Loss: 1.378 | Acc: 50.134% (16428/32768)\n",
      "Loss: 1.378 | Acc: 50.137% (16461/32832)\n",
      "Loss: 1.377 | Acc: 50.152% (16498/32896)\n",
      "Loss: 1.377 | Acc: 50.167% (16535/32960)\n",
      "Loss: 1.377 | Acc: 50.182% (16572/33024)\n",
      "Loss: 1.377 | Acc: 50.181% (16604/33088)\n",
      "Loss: 1.377 | Acc: 50.178% (16635/33152)\n",
      "Loss: 1.377 | Acc: 50.187% (16670/33216)\n",
      "Loss: 1.377 | Acc: 50.192% (16704/33280)\n",
      "Loss: 1.377 | Acc: 50.216% (16744/33344)\n",
      "Loss: 1.378 | Acc: 50.201% (16771/33408)\n",
      "Loss: 1.378 | Acc: 50.170% (16793/33472)\n",
      "Loss: 1.378 | Acc: 50.167% (16824/33536)\n",
      "Loss: 1.378 | Acc: 50.167% (16856/33600)\n",
      "Loss: 1.378 | Acc: 50.154% (16884/33664)\n",
      "Loss: 1.378 | Acc: 50.160% (16918/33728)\n",
      "Loss: 1.378 | Acc: 50.175% (16955/33792)\n",
      "Loss: 1.377 | Acc: 50.189% (16992/33856)\n",
      "Loss: 1.377 | Acc: 50.192% (17025/33920)\n",
      "Loss: 1.377 | Acc: 50.194% (17058/33984)\n",
      "Loss: 1.377 | Acc: 50.182% (17086/34048)\n",
      "Loss: 1.377 | Acc: 50.182% (17118/34112)\n",
      "Loss: 1.377 | Acc: 50.173% (17147/34176)\n",
      "Loss: 1.378 | Acc: 50.158% (17174/34240)\n",
      "Loss: 1.377 | Acc: 50.169% (17210/34304)\n",
      "Loss: 1.378 | Acc: 50.160% (17239/34368)\n",
      "Loss: 1.377 | Acc: 50.163% (17272/34432)\n",
      "Loss: 1.377 | Acc: 50.151% (17300/34496)\n",
      "Loss: 1.377 | Acc: 50.150% (17332/34560)\n",
      "Loss: 1.377 | Acc: 50.150% (17364/34624)\n",
      "Loss: 1.377 | Acc: 50.173% (17404/34688)\n",
      "Loss: 1.377 | Acc: 50.164% (17433/34752)\n",
      "Loss: 1.377 | Acc: 50.187% (17473/34816)\n",
      "Loss: 1.376 | Acc: 50.195% (17508/34880)\n",
      "Loss: 1.376 | Acc: 50.200% (17542/34944)\n",
      "Loss: 1.376 | Acc: 50.189% (17570/35008)\n",
      "Loss: 1.377 | Acc: 50.180% (17599/35072)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.376 | Acc: 50.199% (17638/35136)\n",
      "Loss: 1.376 | Acc: 50.216% (17676/35200)\n",
      "Loss: 1.376 | Acc: 50.221% (17710/35264)\n",
      "Loss: 1.375 | Acc: 50.221% (17742/35328)\n",
      "Loss: 1.376 | Acc: 50.218% (17773/35392)\n",
      "Loss: 1.376 | Acc: 50.228% (17809/35456)\n",
      "Loss: 1.376 | Acc: 50.228% (17841/35520)\n",
      "Loss: 1.376 | Acc: 50.225% (17872/35584)\n",
      "Loss: 1.376 | Acc: 50.216% (17901/35648)\n",
      "Loss: 1.376 | Acc: 50.218% (17934/35712)\n",
      "Loss: 1.376 | Acc: 50.240% (17974/35776)\n",
      "Loss: 1.376 | Acc: 50.237% (18005/35840)\n",
      "Loss: 1.376 | Acc: 50.228% (18034/35904)\n",
      "Loss: 1.376 | Acc: 50.242% (18071/35968)\n",
      "Loss: 1.376 | Acc: 50.247% (18105/36032)\n",
      "Loss: 1.375 | Acc: 50.266% (18144/36096)\n",
      "Loss: 1.375 | Acc: 50.252% (18171/36160)\n",
      "Loss: 1.375 | Acc: 50.276% (18212/36224)\n",
      "Loss: 1.374 | Acc: 50.292% (18250/36288)\n",
      "Loss: 1.374 | Acc: 50.311% (18289/36352)\n",
      "Loss: 1.374 | Acc: 50.321% (18325/36416)\n",
      "Loss: 1.374 | Acc: 50.310% (18353/36480)\n",
      "Loss: 1.374 | Acc: 50.301% (18382/36544)\n",
      "Loss: 1.374 | Acc: 50.284% (18408/36608)\n",
      "Loss: 1.374 | Acc: 50.308% (18449/36672)\n",
      "Loss: 1.374 | Acc: 50.313% (18483/36736)\n",
      "Loss: 1.374 | Acc: 50.293% (18508/36800)\n",
      "Loss: 1.374 | Acc: 50.296% (18541/36864)\n",
      "Loss: 1.375 | Acc: 50.284% (18569/36928)\n",
      "Loss: 1.374 | Acc: 50.297% (18606/36992)\n",
      "Loss: 1.374 | Acc: 50.305% (18641/37056)\n",
      "Loss: 1.374 | Acc: 50.302% (18672/37120)\n",
      "Loss: 1.374 | Acc: 50.331% (18715/37184)\n",
      "Loss: 1.374 | Acc: 50.322% (18744/37248)\n",
      "Loss: 1.374 | Acc: 50.322% (18776/37312)\n",
      "Loss: 1.374 | Acc: 50.326% (18810/37376)\n",
      "Loss: 1.374 | Acc: 50.329% (18843/37440)\n",
      "Loss: 1.374 | Acc: 50.317% (18871/37504)\n",
      "Loss: 1.373 | Acc: 50.335% (18910/37568)\n",
      "Loss: 1.373 | Acc: 50.332% (18941/37632)\n",
      "Loss: 1.373 | Acc: 50.342% (18977/37696)\n",
      "Loss: 1.373 | Acc: 50.336% (19007/37760)\n",
      "Loss: 1.374 | Acc: 50.336% (19039/37824)\n",
      "Loss: 1.373 | Acc: 50.346% (19075/37888)\n",
      "Loss: 1.373 | Acc: 50.374% (19118/37952)\n",
      "Loss: 1.373 | Acc: 50.387% (19155/38016)\n",
      "Loss: 1.372 | Acc: 50.397% (19191/38080)\n",
      "Loss: 1.373 | Acc: 50.380% (19217/38144)\n",
      "Loss: 1.373 | Acc: 50.382% (19250/38208)\n",
      "Loss: 1.373 | Acc: 50.395% (19287/38272)\n",
      "Loss: 1.372 | Acc: 50.381% (19314/38336)\n",
      "Loss: 1.372 | Acc: 50.383% (19347/38400)\n",
      "Loss: 1.373 | Acc: 50.374% (19376/38464)\n",
      "Loss: 1.373 | Acc: 50.363% (19404/38528)\n",
      "Loss: 1.373 | Acc: 50.368% (19438/38592)\n",
      "Loss: 1.373 | Acc: 50.367% (19470/38656)\n",
      "Loss: 1.373 | Acc: 50.382% (19508/38720)\n",
      "Loss: 1.373 | Acc: 50.382% (19540/38784)\n",
      "Loss: 1.373 | Acc: 50.363% (19565/38848)\n",
      "Loss: 1.373 | Acc: 50.360% (19596/38912)\n",
      "Loss: 1.373 | Acc: 50.354% (19626/38976)\n",
      "Loss: 1.373 | Acc: 50.348% (19656/39040)\n",
      "Loss: 1.373 | Acc: 50.358% (19692/39104)\n",
      "Loss: 1.373 | Acc: 50.350% (19721/39168)\n",
      "Loss: 1.373 | Acc: 50.357% (19756/39232)\n",
      "Loss: 1.373 | Acc: 50.349% (19785/39296)\n",
      "Loss: 1.373 | Acc: 50.346% (19816/39360)\n",
      "Loss: 1.374 | Acc: 50.337% (19845/39424)\n",
      "Loss: 1.373 | Acc: 50.347% (19881/39488)\n",
      "Loss: 1.374 | Acc: 50.336% (19909/39552)\n",
      "Loss: 1.374 | Acc: 50.331% (19939/39616)\n",
      "Loss: 1.374 | Acc: 50.318% (19966/39680)\n",
      "Loss: 1.374 | Acc: 50.317% (19998/39744)\n",
      "Loss: 1.374 | Acc: 50.317% (20030/39808)\n",
      "Loss: 1.374 | Acc: 50.331% (20068/39872)\n",
      "Loss: 1.374 | Acc: 50.321% (20096/39936)\n",
      "Loss: 1.374 | Acc: 50.310% (20124/40000)\n",
      "Loss: 1.374 | Acc: 50.322% (20161/40064)\n",
      "Loss: 1.374 | Acc: 50.319% (20192/40128)\n",
      "Loss: 1.374 | Acc: 50.323% (20226/40192)\n",
      "Loss: 1.374 | Acc: 50.328% (20260/40256)\n",
      "Loss: 1.374 | Acc: 50.312% (20286/40320)\n",
      "Loss: 1.374 | Acc: 50.295% (20311/40384)\n",
      "Loss: 1.374 | Acc: 50.309% (20349/40448)\n",
      "Loss: 1.374 | Acc: 50.301% (20378/40512)\n",
      "Loss: 1.374 | Acc: 50.288% (20405/40576)\n",
      "Loss: 1.374 | Acc: 50.288% (20437/40640)\n",
      "Loss: 1.374 | Acc: 50.285% (20468/40704)\n",
      "Loss: 1.374 | Acc: 50.292% (20503/40768)\n",
      "Loss: 1.374 | Acc: 50.284% (20532/40832)\n",
      "Loss: 1.373 | Acc: 50.296% (20569/40896)\n",
      "Loss: 1.373 | Acc: 50.303% (20604/40960)\n",
      "Loss: 1.373 | Acc: 50.312% (20640/41024)\n",
      "Loss: 1.373 | Acc: 50.312% (20672/41088)\n",
      "Loss: 1.373 | Acc: 50.313% (20705/41152)\n",
      "Loss: 1.373 | Acc: 50.306% (20734/41216)\n",
      "Loss: 1.373 | Acc: 50.303% (20765/41280)\n",
      "Loss: 1.373 | Acc: 50.302% (20797/41344)\n",
      "Loss: 1.373 | Acc: 50.319% (20836/41408)\n",
      "Loss: 1.373 | Acc: 50.330% (20873/41472)\n",
      "Loss: 1.372 | Acc: 50.347% (20912/41536)\n",
      "Loss: 1.372 | Acc: 50.358% (20949/41600)\n",
      "Loss: 1.373 | Acc: 50.360% (20982/41664)\n",
      "Loss: 1.373 | Acc: 50.369% (21018/41728)\n",
      "Loss: 1.372 | Acc: 50.385% (21057/41792)\n",
      "Loss: 1.372 | Acc: 50.397% (21094/41856)\n",
      "Loss: 1.372 | Acc: 50.401% (21128/41920)\n",
      "Loss: 1.372 | Acc: 50.400% (21160/41984)\n",
      "Loss: 1.372 | Acc: 50.404% (21194/42048)\n",
      "Loss: 1.372 | Acc: 50.401% (21225/42112)\n",
      "Loss: 1.371 | Acc: 50.420% (21265/42176)\n",
      "Loss: 1.371 | Acc: 50.417% (21296/42240)\n",
      "Loss: 1.371 | Acc: 50.416% (21328/42304)\n",
      "Loss: 1.371 | Acc: 50.401% (21354/42368)\n",
      "Loss: 1.371 | Acc: 50.405% (21388/42432)\n",
      "Loss: 1.371 | Acc: 50.417% (21425/42496)\n",
      "Loss: 1.371 | Acc: 50.416% (21457/42560)\n",
      "Loss: 1.371 | Acc: 50.420% (21491/42624)\n",
      "Loss: 1.371 | Acc: 50.417% (21522/42688)\n",
      "Loss: 1.371 | Acc: 50.400% (21547/42752)\n",
      "Loss: 1.371 | Acc: 50.392% (21576/42816)\n",
      "Loss: 1.371 | Acc: 50.387% (21606/42880)\n",
      "Loss: 1.371 | Acc: 50.391% (21640/42944)\n",
      "Loss: 1.370 | Acc: 50.402% (21677/43008)\n",
      "Loss: 1.370 | Acc: 50.420% (21717/43072)\n",
      "Loss: 1.370 | Acc: 50.427% (21752/43136)\n",
      "Loss: 1.370 | Acc: 50.421% (21782/43200)\n",
      "Loss: 1.370 | Acc: 50.425% (21816/43264)\n",
      "Loss: 1.370 | Acc: 50.420% (21846/43328)\n",
      "Loss: 1.370 | Acc: 50.419% (21878/43392)\n",
      "Loss: 1.370 | Acc: 50.410% (21906/43456)\n",
      "Loss: 1.370 | Acc: 50.409% (21938/43520)\n",
      "Loss: 1.370 | Acc: 50.404% (21968/43584)\n",
      "Loss: 1.370 | Acc: 50.406% (22001/43648)\n",
      "Loss: 1.369 | Acc: 50.412% (22036/43712)\n",
      "Loss: 1.369 | Acc: 50.413% (22069/43776)\n",
      "Loss: 1.369 | Acc: 50.420% (22104/43840)\n",
      "Loss: 1.369 | Acc: 50.408% (22131/43904)\n",
      "Loss: 1.369 | Acc: 50.403% (22161/43968)\n",
      "Loss: 1.369 | Acc: 50.407% (22195/44032)\n",
      "Loss: 1.370 | Acc: 50.386% (22218/44096)\n",
      "Loss: 1.369 | Acc: 50.399% (22256/44160)\n",
      "Loss: 1.369 | Acc: 50.391% (22285/44224)\n",
      "Loss: 1.369 | Acc: 50.393% (22318/44288)\n",
      "Loss: 1.369 | Acc: 50.399% (22353/44352)\n",
      "Loss: 1.368 | Acc: 50.421% (22395/44416)\n",
      "Loss: 1.368 | Acc: 50.416% (22425/44480)\n",
      "Loss: 1.368 | Acc: 50.424% (22461/44544)\n",
      "Loss: 1.368 | Acc: 50.417% (22490/44608)\n",
      "Loss: 1.369 | Acc: 50.416% (22522/44672)\n",
      "Loss: 1.369 | Acc: 50.420% (22556/44736)\n",
      "Loss: 1.369 | Acc: 50.413% (22585/44800)\n",
      "Loss: 1.369 | Acc: 50.410% (22616/44864)\n",
      "Loss: 1.369 | Acc: 50.414% (22650/44928)\n",
      "Loss: 1.369 | Acc: 50.400% (22676/44992)\n",
      "Loss: 1.369 | Acc: 50.395% (22706/45056)\n",
      "Loss: 1.369 | Acc: 50.399% (22740/45120)\n",
      "Loss: 1.369 | Acc: 50.405% (22775/45184)\n",
      "Loss: 1.369 | Acc: 50.407% (22808/45248)\n",
      "Loss: 1.369 | Acc: 50.399% (22837/45312)\n",
      "Loss: 1.369 | Acc: 50.410% (22874/45376)\n",
      "Loss: 1.369 | Acc: 50.416% (22909/45440)\n",
      "Loss: 1.369 | Acc: 50.415% (22941/45504)\n",
      "Loss: 1.369 | Acc: 50.415% (22973/45568)\n",
      "Loss: 1.369 | Acc: 50.408% (23002/45632)\n",
      "Loss: 1.369 | Acc: 50.407% (23034/45696)\n",
      "Loss: 1.369 | Acc: 50.400% (23063/45760)\n",
      "Loss: 1.369 | Acc: 50.404% (23097/45824)\n",
      "Loss: 1.369 | Acc: 50.405% (23130/45888)\n",
      "Loss: 1.368 | Acc: 50.424% (23171/45952)\n",
      "Loss: 1.368 | Acc: 50.415% (23199/46016)\n",
      "Loss: 1.368 | Acc: 50.417% (23232/46080)\n",
      "Loss: 1.368 | Acc: 50.418% (23265/46144)\n",
      "Loss: 1.368 | Acc: 50.439% (23307/46208)\n",
      "Loss: 1.368 | Acc: 50.452% (23345/46272)\n",
      "Loss: 1.368 | Acc: 50.447% (23375/46336)\n",
      "Loss: 1.367 | Acc: 50.466% (23416/46400)\n",
      "Loss: 1.367 | Acc: 50.465% (23448/46464)\n",
      "Loss: 1.368 | Acc: 50.466% (23481/46528)\n",
      "Loss: 1.368 | Acc: 50.470% (23515/46592)\n",
      "Loss: 1.368 | Acc: 50.476% (23550/46656)\n",
      "Loss: 1.368 | Acc: 50.477% (23583/46720)\n",
      "Loss: 1.368 | Acc: 50.481% (23617/46784)\n",
      "Loss: 1.367 | Acc: 50.489% (23653/46848)\n",
      "Loss: 1.367 | Acc: 50.495% (23688/46912)\n",
      "Loss: 1.367 | Acc: 50.492% (23719/46976)\n",
      "Loss: 1.367 | Acc: 50.504% (23757/47040)\n",
      "Loss: 1.367 | Acc: 50.524% (23799/47104)\n",
      "Loss: 1.367 | Acc: 50.539% (23838/47168)\n",
      "Loss: 1.366 | Acc: 50.553% (23877/47232)\n",
      "Loss: 1.366 | Acc: 50.560% (23913/47296)\n",
      "Loss: 1.366 | Acc: 50.568% (23949/47360)\n",
      "Loss: 1.366 | Acc: 50.561% (23978/47424)\n",
      "Loss: 1.366 | Acc: 50.564% (24012/47488)\n",
      "Loss: 1.366 | Acc: 50.576% (24050/47552)\n",
      "Loss: 1.366 | Acc: 50.582% (24085/47616)\n",
      "Loss: 1.365 | Acc: 50.583% (24118/47680)\n",
      "Loss: 1.365 | Acc: 50.582% (24150/47744)\n",
      "Loss: 1.365 | Acc: 50.581% (24182/47808)\n",
      "Loss: 1.365 | Acc: 50.587% (24217/47872)\n",
      "Loss: 1.365 | Acc: 50.582% (24247/47936)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.365 | Acc: 50.571% (24274/48000)\n",
      "Loss: 1.365 | Acc: 50.570% (24306/48064)\n",
      "Loss: 1.365 | Acc: 50.573% (24340/48128)\n",
      "Loss: 1.365 | Acc: 50.583% (24377/48192)\n",
      "Loss: 1.366 | Acc: 50.576% (24406/48256)\n",
      "Loss: 1.366 | Acc: 50.569% (24435/48320)\n",
      "Loss: 1.365 | Acc: 50.585% (24475/48384)\n",
      "Loss: 1.365 | Acc: 50.590% (24510/48448)\n",
      "Loss: 1.365 | Acc: 50.590% (24542/48512)\n",
      "Loss: 1.365 | Acc: 50.581% (24570/48576)\n",
      "Loss: 1.365 | Acc: 50.574% (24599/48640)\n",
      "Loss: 1.365 | Acc: 50.587% (24638/48704)\n",
      "Loss: 1.365 | Acc: 50.591% (24672/48768)\n",
      "Loss: 1.365 | Acc: 50.582% (24700/48832)\n",
      "Loss: 1.365 | Acc: 50.593% (24738/48896)\n",
      "Loss: 1.365 | Acc: 50.590% (24769/48960)\n",
      "Loss: 1.365 | Acc: 50.594% (24791/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 50.593877551020405\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.377 | Acc: 56.250% (36/64)\n",
      "Loss: 1.289 | Acc: 57.031% (73/128)\n",
      "Loss: 1.342 | Acc: 52.083% (100/192)\n",
      "Loss: 1.411 | Acc: 51.562% (132/256)\n",
      "Loss: 1.378 | Acc: 51.875% (166/320)\n",
      "Loss: 1.409 | Acc: 50.260% (193/384)\n",
      "Loss: 1.410 | Acc: 49.330% (221/448)\n",
      "Loss: 1.402 | Acc: 49.805% (255/512)\n",
      "Loss: 1.383 | Acc: 49.653% (286/576)\n",
      "Loss: 1.363 | Acc: 50.312% (322/640)\n",
      "Loss: 1.367 | Acc: 50.000% (352/704)\n",
      "Loss: 1.368 | Acc: 50.000% (384/768)\n",
      "Loss: 1.378 | Acc: 49.519% (412/832)\n",
      "Loss: 1.375 | Acc: 49.888% (447/896)\n",
      "Loss: 1.358 | Acc: 50.104% (481/960)\n",
      "Loss: 1.345 | Acc: 50.684% (519/1024)\n",
      "Loss: 1.345 | Acc: 50.735% (552/1088)\n",
      "Loss: 1.340 | Acc: 51.215% (590/1152)\n",
      "Loss: 1.334 | Acc: 51.480% (626/1216)\n",
      "Loss: 1.350 | Acc: 51.328% (657/1280)\n",
      "Loss: 1.347 | Acc: 51.562% (693/1344)\n",
      "Loss: 1.344 | Acc: 51.420% (724/1408)\n",
      "Loss: 1.333 | Acc: 51.970% (765/1472)\n",
      "Loss: 1.336 | Acc: 52.148% (801/1536)\n",
      "Loss: 1.333 | Acc: 52.188% (835/1600)\n",
      "Loss: 1.333 | Acc: 52.043% (866/1664)\n",
      "Loss: 1.331 | Acc: 52.373% (905/1728)\n",
      "Loss: 1.331 | Acc: 52.065% (933/1792)\n",
      "Loss: 1.333 | Acc: 52.047% (966/1856)\n",
      "Loss: 1.332 | Acc: 52.188% (1002/1920)\n",
      "Loss: 1.330 | Acc: 52.319% (1038/1984)\n",
      "Loss: 1.329 | Acc: 52.295% (1071/2048)\n",
      "Loss: 1.321 | Acc: 52.746% (1114/2112)\n",
      "Loss: 1.322 | Acc: 52.574% (1144/2176)\n",
      "Loss: 1.319 | Acc: 52.723% (1181/2240)\n",
      "Loss: 1.318 | Acc: 52.691% (1214/2304)\n",
      "Loss: 1.320 | Acc: 52.449% (1242/2368)\n",
      "Loss: 1.326 | Acc: 52.426% (1275/2432)\n",
      "Loss: 1.324 | Acc: 52.404% (1308/2496)\n",
      "Loss: 1.333 | Acc: 52.070% (1333/2560)\n",
      "Loss: 1.334 | Acc: 51.867% (1361/2624)\n",
      "Loss: 1.337 | Acc: 51.749% (1391/2688)\n",
      "Loss: 1.336 | Acc: 51.526% (1418/2752)\n",
      "Loss: 1.338 | Acc: 51.598% (1453/2816)\n",
      "Loss: 1.338 | Acc: 51.701% (1489/2880)\n",
      "Loss: 1.336 | Acc: 51.800% (1525/2944)\n",
      "Loss: 1.337 | Acc: 51.895% (1561/3008)\n",
      "Loss: 1.337 | Acc: 51.921% (1595/3072)\n",
      "Loss: 1.340 | Acc: 51.818% (1625/3136)\n",
      "Loss: 1.339 | Acc: 51.875% (1660/3200)\n",
      "Loss: 1.342 | Acc: 51.838% (1692/3264)\n",
      "Loss: 1.342 | Acc: 51.713% (1721/3328)\n",
      "Loss: 1.343 | Acc: 51.710% (1754/3392)\n",
      "Loss: 1.346 | Acc: 51.534% (1781/3456)\n",
      "Loss: 1.348 | Acc: 51.420% (1810/3520)\n",
      "Loss: 1.345 | Acc: 51.535% (1847/3584)\n",
      "Loss: 1.347 | Acc: 51.398% (1875/3648)\n",
      "Loss: 1.346 | Acc: 51.428% (1909/3712)\n",
      "Loss: 1.348 | Acc: 51.457% (1943/3776)\n",
      "Loss: 1.346 | Acc: 51.432% (1975/3840)\n",
      "Loss: 1.347 | Acc: 51.383% (2006/3904)\n",
      "Loss: 1.345 | Acc: 51.462% (2042/3968)\n",
      "Loss: 1.346 | Acc: 51.389% (2072/4032)\n",
      "Loss: 1.350 | Acc: 51.392% (2105/4096)\n",
      "Loss: 1.349 | Acc: 51.418% (2139/4160)\n",
      "Loss: 1.347 | Acc: 51.562% (2178/4224)\n",
      "Loss: 1.346 | Acc: 51.562% (2211/4288)\n",
      "Loss: 1.346 | Acc: 51.631% (2247/4352)\n",
      "Loss: 1.344 | Acc: 51.766% (2286/4416)\n",
      "Loss: 1.341 | Acc: 51.763% (2319/4480)\n",
      "Loss: 1.339 | Acc: 51.915% (2359/4544)\n",
      "Loss: 1.339 | Acc: 51.997% (2396/4608)\n",
      "Loss: 1.338 | Acc: 52.055% (2432/4672)\n",
      "Loss: 1.337 | Acc: 52.048% (2465/4736)\n",
      "Loss: 1.337 | Acc: 52.042% (2498/4800)\n",
      "Loss: 1.336 | Acc: 52.035% (2531/4864)\n",
      "Loss: 1.335 | Acc: 52.171% (2571/4928)\n",
      "Loss: 1.335 | Acc: 52.123% (2602/4992)\n",
      "Loss: 1.333 | Acc: 52.176% (2638/5056)\n",
      "Loss: 1.336 | Acc: 52.129% (2669/5120)\n",
      "Loss: 1.336 | Acc: 52.160% (2704/5184)\n",
      "Loss: 1.337 | Acc: 52.172% (2738/5248)\n",
      "Loss: 1.336 | Acc: 52.203% (2773/5312)\n",
      "Loss: 1.339 | Acc: 52.121% (2802/5376)\n",
      "Loss: 1.339 | Acc: 52.096% (2834/5440)\n",
      "Loss: 1.339 | Acc: 51.999% (2862/5504)\n",
      "Loss: 1.342 | Acc: 51.814% (2885/5568)\n",
      "Loss: 1.344 | Acc: 51.776% (2916/5632)\n",
      "Loss: 1.346 | Acc: 51.650% (2942/5696)\n",
      "Loss: 1.344 | Acc: 51.684% (2977/5760)\n",
      "Loss: 1.344 | Acc: 51.648% (3008/5824)\n",
      "Loss: 1.345 | Acc: 51.546% (3035/5888)\n",
      "Loss: 1.347 | Acc: 51.445% (3062/5952)\n",
      "Loss: 1.347 | Acc: 51.463% (3096/6016)\n",
      "Loss: 1.346 | Acc: 51.382% (3124/6080)\n",
      "Loss: 1.346 | Acc: 51.383% (3157/6144)\n",
      "Loss: 1.346 | Acc: 51.434% (3193/6208)\n",
      "Loss: 1.349 | Acc: 51.307% (3218/6272)\n",
      "Loss: 1.348 | Acc: 51.405% (3257/6336)\n",
      "Loss: 1.350 | Acc: 51.359% (3287/6400)\n",
      "Loss: 1.351 | Acc: 51.269% (3314/6464)\n",
      "Loss: 1.353 | Acc: 51.241% (3345/6528)\n",
      "Loss: 1.355 | Acc: 51.259% (3379/6592)\n",
      "Loss: 1.355 | Acc: 51.262% (3412/6656)\n",
      "Loss: 1.354 | Acc: 51.250% (3444/6720)\n",
      "Loss: 1.353 | Acc: 51.341% (3483/6784)\n",
      "Loss: 1.350 | Acc: 51.431% (3522/6848)\n",
      "Loss: 1.352 | Acc: 51.389% (3552/6912)\n",
      "Loss: 1.353 | Acc: 51.347% (3582/6976)\n",
      "Loss: 1.354 | Acc: 51.307% (3612/7040)\n",
      "Loss: 1.354 | Acc: 51.323% (3646/7104)\n",
      "Loss: 1.353 | Acc: 51.367% (3682/7168)\n",
      "Loss: 1.353 | Acc: 51.341% (3713/7232)\n",
      "Loss: 1.351 | Acc: 51.288% (3742/7296)\n",
      "Loss: 1.350 | Acc: 51.372% (3781/7360)\n",
      "Loss: 1.349 | Acc: 51.401% (3816/7424)\n",
      "Loss: 1.347 | Acc: 51.469% (3854/7488)\n",
      "Loss: 1.346 | Acc: 51.510% (3890/7552)\n",
      "Loss: 1.348 | Acc: 51.471% (3920/7616)\n",
      "Loss: 1.347 | Acc: 51.510% (3956/7680)\n",
      "Loss: 1.345 | Acc: 51.653% (4000/7744)\n",
      "Loss: 1.343 | Acc: 51.755% (4041/7808)\n",
      "Loss: 1.345 | Acc: 51.728% (4072/7872)\n",
      "Loss: 1.344 | Acc: 51.777% (4109/7936)\n",
      "Loss: 1.344 | Acc: 51.737% (4139/8000)\n",
      "Loss: 1.344 | Acc: 51.736% (4172/8064)\n",
      "Loss: 1.345 | Acc: 51.759% (4207/8128)\n",
      "Loss: 1.344 | Acc: 51.807% (4244/8192)\n",
      "Loss: 1.344 | Acc: 51.805% (4277/8256)\n",
      "Loss: 1.345 | Acc: 51.767% (4307/8320)\n",
      "Loss: 1.346 | Acc: 51.765% (4340/8384)\n",
      "Loss: 1.345 | Acc: 51.787% (4375/8448)\n",
      "Loss: 1.346 | Acc: 51.739% (4404/8512)\n",
      "Loss: 1.347 | Acc: 51.726% (4436/8576)\n",
      "Loss: 1.346 | Acc: 51.794% (4475/8640)\n",
      "Loss: 1.347 | Acc: 51.677% (4498/8704)\n",
      "Loss: 1.347 | Acc: 51.688% (4532/8768)\n",
      "Loss: 1.347 | Acc: 51.710% (4567/8832)\n",
      "Loss: 1.346 | Acc: 51.731% (4602/8896)\n",
      "Loss: 1.346 | Acc: 51.763% (4638/8960)\n",
      "Loss: 1.346 | Acc: 51.795% (4674/9024)\n",
      "Loss: 1.347 | Acc: 51.728% (4701/9088)\n",
      "Loss: 1.347 | Acc: 51.705% (4732/9152)\n",
      "Loss: 1.346 | Acc: 51.769% (4771/9216)\n",
      "Loss: 1.346 | Acc: 51.789% (4806/9280)\n",
      "Loss: 1.346 | Acc: 51.766% (4837/9344)\n",
      "Loss: 1.346 | Acc: 51.754% (4869/9408)\n",
      "Loss: 1.347 | Acc: 51.700% (4897/9472)\n",
      "Loss: 1.346 | Acc: 51.751% (4935/9536)\n",
      "Loss: 1.345 | Acc: 51.781% (4971/9600)\n",
      "Loss: 1.345 | Acc: 51.780% (5004/9664)\n",
      "Loss: 1.345 | Acc: 51.768% (5036/9728)\n",
      "Loss: 1.346 | Acc: 51.705% (5063/9792)\n",
      "Loss: 1.345 | Acc: 51.705% (5096/9856)\n",
      "Loss: 1.345 | Acc: 51.704% (5129/9920)\n",
      "Loss: 1.346 | Acc: 51.693% (5161/9984)\n",
      "Loss: 1.347 | Acc: 51.660% (5166/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 51.66\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.391 | Acc: 46.875% (30/64)\n",
      "Loss: 1.328 | Acc: 52.344% (67/128)\n",
      "Loss: 1.428 | Acc: 48.438% (93/192)\n",
      "Loss: 1.427 | Acc: 48.438% (124/256)\n",
      "Loss: 1.392 | Acc: 49.062% (157/320)\n",
      "Loss: 1.355 | Acc: 50.000% (192/384)\n",
      "Loss: 1.365 | Acc: 50.446% (226/448)\n",
      "Loss: 1.387 | Acc: 50.781% (260/512)\n",
      "Loss: 1.376 | Acc: 51.215% (295/576)\n",
      "Loss: 1.369 | Acc: 51.250% (328/640)\n",
      "Loss: 1.358 | Acc: 51.420% (362/704)\n",
      "Loss: 1.364 | Acc: 50.911% (391/768)\n",
      "Loss: 1.360 | Acc: 51.082% (425/832)\n",
      "Loss: 1.351 | Acc: 51.228% (459/896)\n",
      "Loss: 1.334 | Acc: 52.188% (501/960)\n",
      "Loss: 1.324 | Acc: 53.125% (544/1024)\n",
      "Loss: 1.316 | Acc: 53.493% (582/1088)\n",
      "Loss: 1.319 | Acc: 53.299% (614/1152)\n",
      "Loss: 1.321 | Acc: 53.125% (646/1216)\n",
      "Loss: 1.328 | Acc: 52.500% (672/1280)\n",
      "Loss: 1.330 | Acc: 52.381% (704/1344)\n",
      "Loss: 1.331 | Acc: 52.344% (737/1408)\n",
      "Loss: 1.318 | Acc: 53.057% (781/1472)\n",
      "Loss: 1.327 | Acc: 52.734% (810/1536)\n",
      "Loss: 1.334 | Acc: 52.312% (837/1600)\n",
      "Loss: 1.323 | Acc: 52.584% (875/1664)\n",
      "Loss: 1.316 | Acc: 53.009% (916/1728)\n",
      "Loss: 1.316 | Acc: 52.734% (945/1792)\n",
      "Loss: 1.310 | Acc: 52.963% (983/1856)\n",
      "Loss: 1.305 | Acc: 53.125% (1020/1920)\n",
      "Loss: 1.303 | Acc: 53.478% (1061/1984)\n",
      "Loss: 1.297 | Acc: 53.906% (1104/2048)\n",
      "Loss: 1.295 | Acc: 54.072% (1142/2112)\n",
      "Loss: 1.301 | Acc: 53.814% (1171/2176)\n",
      "Loss: 1.302 | Acc: 53.705% (1203/2240)\n",
      "Loss: 1.298 | Acc: 53.863% (1241/2304)\n",
      "Loss: 1.295 | Acc: 53.885% (1276/2368)\n",
      "Loss: 1.301 | Acc: 53.783% (1308/2432)\n",
      "Loss: 1.305 | Acc: 53.726% (1341/2496)\n",
      "Loss: 1.304 | Acc: 53.672% (1374/2560)\n",
      "Loss: 1.300 | Acc: 53.811% (1412/2624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.299 | Acc: 53.869% (1448/2688)\n",
      "Loss: 1.301 | Acc: 53.743% (1479/2752)\n",
      "Loss: 1.299 | Acc: 53.800% (1515/2816)\n",
      "Loss: 1.298 | Acc: 53.681% (1546/2880)\n",
      "Loss: 1.297 | Acc: 53.635% (1579/2944)\n",
      "Loss: 1.296 | Acc: 53.690% (1615/3008)\n",
      "Loss: 1.296 | Acc: 53.646% (1648/3072)\n",
      "Loss: 1.294 | Acc: 53.444% (1676/3136)\n",
      "Loss: 1.293 | Acc: 53.438% (1710/3200)\n",
      "Loss: 1.293 | Acc: 53.493% (1746/3264)\n",
      "Loss: 1.289 | Acc: 53.636% (1785/3328)\n",
      "Loss: 1.289 | Acc: 53.538% (1816/3392)\n",
      "Loss: 1.288 | Acc: 53.704% (1856/3456)\n",
      "Loss: 1.289 | Acc: 53.636% (1888/3520)\n",
      "Loss: 1.288 | Acc: 53.627% (1922/3584)\n",
      "Loss: 1.288 | Acc: 53.564% (1954/3648)\n",
      "Loss: 1.285 | Acc: 53.691% (1993/3712)\n",
      "Loss: 1.282 | Acc: 53.787% (2031/3776)\n",
      "Loss: 1.285 | Acc: 53.750% (2064/3840)\n",
      "Loss: 1.283 | Acc: 53.817% (2101/3904)\n",
      "Loss: 1.282 | Acc: 53.881% (2138/3968)\n",
      "Loss: 1.278 | Acc: 53.943% (2175/4032)\n",
      "Loss: 1.274 | Acc: 54.077% (2215/4096)\n",
      "Loss: 1.274 | Acc: 54.014% (2247/4160)\n",
      "Loss: 1.273 | Acc: 54.096% (2285/4224)\n",
      "Loss: 1.274 | Acc: 54.104% (2320/4288)\n",
      "Loss: 1.275 | Acc: 54.113% (2355/4352)\n",
      "Loss: 1.275 | Acc: 54.121% (2390/4416)\n",
      "Loss: 1.276 | Acc: 53.929% (2416/4480)\n",
      "Loss: 1.277 | Acc: 53.851% (2447/4544)\n",
      "Loss: 1.278 | Acc: 53.733% (2476/4608)\n",
      "Loss: 1.277 | Acc: 53.746% (2511/4672)\n",
      "Loss: 1.278 | Acc: 53.843% (2550/4736)\n",
      "Loss: 1.276 | Acc: 53.938% (2589/4800)\n",
      "Loss: 1.277 | Acc: 53.988% (2626/4864)\n",
      "Loss: 1.278 | Acc: 53.977% (2660/4928)\n",
      "Loss: 1.279 | Acc: 53.926% (2692/4992)\n",
      "Loss: 1.280 | Acc: 53.956% (2728/5056)\n",
      "Loss: 1.280 | Acc: 53.906% (2760/5120)\n",
      "Loss: 1.282 | Acc: 53.819% (2790/5184)\n",
      "Loss: 1.283 | Acc: 53.773% (2822/5248)\n",
      "Loss: 1.281 | Acc: 53.916% (2864/5312)\n",
      "Loss: 1.280 | Acc: 53.962% (2901/5376)\n",
      "Loss: 1.280 | Acc: 53.915% (2933/5440)\n",
      "Loss: 1.278 | Acc: 54.015% (2973/5504)\n",
      "Loss: 1.280 | Acc: 53.897% (3001/5568)\n",
      "Loss: 1.278 | Acc: 54.031% (3043/5632)\n",
      "Loss: 1.279 | Acc: 54.003% (3076/5696)\n",
      "Loss: 1.278 | Acc: 54.045% (3113/5760)\n",
      "Loss: 1.277 | Acc: 54.018% (3146/5824)\n",
      "Loss: 1.278 | Acc: 53.974% (3178/5888)\n",
      "Loss: 1.277 | Acc: 53.982% (3213/5952)\n",
      "Loss: 1.276 | Acc: 54.039% (3251/6016)\n",
      "Loss: 1.276 | Acc: 53.964% (3281/6080)\n",
      "Loss: 1.273 | Acc: 54.036% (3320/6144)\n",
      "Loss: 1.271 | Acc: 54.124% (3360/6208)\n",
      "Loss: 1.274 | Acc: 54.034% (3389/6272)\n",
      "Loss: 1.273 | Acc: 54.104% (3428/6336)\n",
      "Loss: 1.274 | Acc: 54.078% (3461/6400)\n",
      "Loss: 1.273 | Acc: 54.022% (3492/6464)\n",
      "Loss: 1.273 | Acc: 53.983% (3524/6528)\n",
      "Loss: 1.274 | Acc: 53.929% (3555/6592)\n",
      "Loss: 1.273 | Acc: 53.936% (3590/6656)\n",
      "Loss: 1.277 | Acc: 53.795% (3615/6720)\n",
      "Loss: 1.277 | Acc: 53.833% (3652/6784)\n",
      "Loss: 1.275 | Acc: 53.884% (3690/6848)\n",
      "Loss: 1.277 | Acc: 53.848% (3722/6912)\n",
      "Loss: 1.274 | Acc: 53.999% (3767/6976)\n",
      "Loss: 1.270 | Acc: 54.134% (3811/7040)\n",
      "Loss: 1.271 | Acc: 54.124% (3845/7104)\n",
      "Loss: 1.270 | Acc: 54.129% (3880/7168)\n",
      "Loss: 1.271 | Acc: 54.148% (3916/7232)\n",
      "Loss: 1.270 | Acc: 54.167% (3952/7296)\n",
      "Loss: 1.268 | Acc: 54.212% (3990/7360)\n",
      "Loss: 1.267 | Acc: 54.297% (4031/7424)\n",
      "Loss: 1.267 | Acc: 54.314% (4067/7488)\n",
      "Loss: 1.265 | Acc: 54.409% (4109/7552)\n",
      "Loss: 1.264 | Acc: 54.530% (4153/7616)\n",
      "Loss: 1.265 | Acc: 54.492% (4185/7680)\n",
      "Loss: 1.263 | Acc: 54.571% (4226/7744)\n",
      "Loss: 1.261 | Acc: 54.611% (4264/7808)\n",
      "Loss: 1.261 | Acc: 54.611% (4299/7872)\n",
      "Loss: 1.262 | Acc: 54.574% (4331/7936)\n",
      "Loss: 1.261 | Acc: 54.587% (4367/8000)\n",
      "Loss: 1.262 | Acc: 54.539% (4398/8064)\n",
      "Loss: 1.260 | Acc: 54.601% (4438/8128)\n",
      "Loss: 1.259 | Acc: 54.639% (4476/8192)\n",
      "Loss: 1.259 | Acc: 54.651% (4512/8256)\n",
      "Loss: 1.258 | Acc: 54.675% (4549/8320)\n",
      "Loss: 1.256 | Acc: 54.795% (4594/8384)\n",
      "Loss: 1.256 | Acc: 54.830% (4632/8448)\n",
      "Loss: 1.254 | Acc: 54.911% (4674/8512)\n",
      "Loss: 1.254 | Acc: 54.909% (4709/8576)\n",
      "Loss: 1.254 | Acc: 54.942% (4747/8640)\n",
      "Loss: 1.254 | Acc: 55.021% (4789/8704)\n",
      "Loss: 1.252 | Acc: 55.075% (4829/8768)\n",
      "Loss: 1.250 | Acc: 55.140% (4870/8832)\n",
      "Loss: 1.252 | Acc: 55.103% (4902/8896)\n",
      "Loss: 1.251 | Acc: 55.112% (4938/8960)\n",
      "Loss: 1.249 | Acc: 55.164% (4978/9024)\n",
      "Loss: 1.250 | Acc: 55.139% (5011/9088)\n",
      "Loss: 1.249 | Acc: 55.179% (5050/9152)\n",
      "Loss: 1.248 | Acc: 55.263% (5093/9216)\n",
      "Loss: 1.248 | Acc: 55.259% (5128/9280)\n",
      "Loss: 1.249 | Acc: 55.255% (5163/9344)\n",
      "Loss: 1.250 | Acc: 55.230% (5196/9408)\n",
      "Loss: 1.249 | Acc: 55.247% (5233/9472)\n",
      "Loss: 1.250 | Acc: 55.233% (5267/9536)\n",
      "Loss: 1.250 | Acc: 55.177% (5297/9600)\n",
      "Loss: 1.250 | Acc: 55.226% (5337/9664)\n",
      "Loss: 1.251 | Acc: 55.160% (5366/9728)\n",
      "Loss: 1.250 | Acc: 55.208% (5406/9792)\n",
      "Loss: 1.249 | Acc: 55.235% (5444/9856)\n",
      "Loss: 1.251 | Acc: 55.192% (5475/9920)\n",
      "Loss: 1.251 | Acc: 55.208% (5512/9984)\n",
      "Loss: 1.250 | Acc: 55.205% (5547/10048)\n",
      "Loss: 1.250 | Acc: 55.212% (5583/10112)\n",
      "Loss: 1.252 | Acc: 55.208% (5618/10176)\n",
      "Loss: 1.252 | Acc: 55.146% (5647/10240)\n",
      "Loss: 1.253 | Acc: 55.153% (5683/10304)\n",
      "Loss: 1.253 | Acc: 55.141% (5717/10368)\n",
      "Loss: 1.253 | Acc: 55.138% (5752/10432)\n",
      "Loss: 1.253 | Acc: 55.164% (5790/10496)\n",
      "Loss: 1.252 | Acc: 55.161% (5825/10560)\n",
      "Loss: 1.251 | Acc: 55.139% (5858/10624)\n",
      "Loss: 1.251 | Acc: 55.109% (5890/10688)\n",
      "Loss: 1.251 | Acc: 55.087% (5923/10752)\n",
      "Loss: 1.251 | Acc: 55.030% (5952/10816)\n",
      "Loss: 1.250 | Acc: 55.083% (5993/10880)\n",
      "Loss: 1.251 | Acc: 55.080% (6028/10944)\n",
      "Loss: 1.250 | Acc: 55.051% (6060/11008)\n",
      "Loss: 1.251 | Acc: 55.004% (6090/11072)\n",
      "Loss: 1.251 | Acc: 54.984% (6123/11136)\n",
      "Loss: 1.251 | Acc: 55.036% (6164/11200)\n",
      "Loss: 1.252 | Acc: 54.945% (6189/11264)\n",
      "Loss: 1.254 | Acc: 54.855% (6214/11328)\n",
      "Loss: 1.254 | Acc: 54.863% (6250/11392)\n",
      "Loss: 1.254 | Acc: 54.845% (6283/11456)\n",
      "Loss: 1.254 | Acc: 54.870% (6321/11520)\n",
      "Loss: 1.253 | Acc: 54.860% (6355/11584)\n",
      "Loss: 1.254 | Acc: 54.816% (6385/11648)\n",
      "Loss: 1.253 | Acc: 54.833% (6422/11712)\n",
      "Loss: 1.254 | Acc: 54.806% (6454/11776)\n",
      "Loss: 1.253 | Acc: 54.848% (6494/11840)\n",
      "Loss: 1.252 | Acc: 54.864% (6531/11904)\n",
      "Loss: 1.253 | Acc: 54.813% (6560/11968)\n",
      "Loss: 1.253 | Acc: 54.779% (6591/12032)\n",
      "Loss: 1.253 | Acc: 54.787% (6627/12096)\n",
      "Loss: 1.254 | Acc: 54.737% (6656/12160)\n",
      "Loss: 1.255 | Acc: 54.720% (6689/12224)\n",
      "Loss: 1.255 | Acc: 54.720% (6724/12288)\n",
      "Loss: 1.255 | Acc: 54.720% (6759/12352)\n",
      "Loss: 1.255 | Acc: 54.736% (6796/12416)\n",
      "Loss: 1.256 | Acc: 54.752% (6833/12480)\n",
      "Loss: 1.256 | Acc: 54.735% (6866/12544)\n",
      "Loss: 1.256 | Acc: 54.711% (6898/12608)\n",
      "Loss: 1.257 | Acc: 54.688% (6930/12672)\n",
      "Loss: 1.258 | Acc: 54.648% (6960/12736)\n",
      "Loss: 1.258 | Acc: 54.672% (6998/12800)\n",
      "Loss: 1.257 | Acc: 54.688% (7035/12864)\n",
      "Loss: 1.258 | Acc: 54.672% (7068/12928)\n",
      "Loss: 1.259 | Acc: 54.626% (7097/12992)\n",
      "Loss: 1.259 | Acc: 54.619% (7131/13056)\n",
      "Loss: 1.259 | Acc: 54.573% (7160/13120)\n",
      "Loss: 1.258 | Acc: 54.627% (7202/13184)\n",
      "Loss: 1.258 | Acc: 54.635% (7238/13248)\n",
      "Loss: 1.258 | Acc: 54.620% (7271/13312)\n",
      "Loss: 1.258 | Acc: 54.635% (7308/13376)\n",
      "Loss: 1.258 | Acc: 54.643% (7344/13440)\n",
      "Loss: 1.257 | Acc: 54.695% (7386/13504)\n",
      "Loss: 1.257 | Acc: 54.658% (7416/13568)\n",
      "Loss: 1.257 | Acc: 54.673% (7453/13632)\n",
      "Loss: 1.258 | Acc: 54.614% (7480/13696)\n",
      "Loss: 1.257 | Acc: 54.637% (7518/13760)\n",
      "Loss: 1.257 | Acc: 54.637% (7553/13824)\n",
      "Loss: 1.257 | Acc: 54.651% (7590/13888)\n",
      "Loss: 1.255 | Acc: 54.716% (7634/13952)\n",
      "Loss: 1.255 | Acc: 54.695% (7666/14016)\n",
      "Loss: 1.255 | Acc: 54.716% (7704/14080)\n",
      "Loss: 1.254 | Acc: 54.723% (7740/14144)\n",
      "Loss: 1.255 | Acc: 54.709% (7773/14208)\n",
      "Loss: 1.255 | Acc: 54.695% (7806/14272)\n",
      "Loss: 1.256 | Acc: 54.632% (7832/14336)\n",
      "Loss: 1.256 | Acc: 54.639% (7868/14400)\n",
      "Loss: 1.256 | Acc: 54.632% (7902/14464)\n",
      "Loss: 1.255 | Acc: 54.688% (7945/14528)\n",
      "Loss: 1.255 | Acc: 54.660% (7976/14592)\n",
      "Loss: 1.255 | Acc: 54.660% (8011/14656)\n",
      "Loss: 1.256 | Acc: 54.620% (8040/14720)\n",
      "Loss: 1.257 | Acc: 54.600% (8072/14784)\n",
      "Loss: 1.255 | Acc: 54.627% (8111/14848)\n",
      "Loss: 1.255 | Acc: 54.647% (8149/14912)\n",
      "Loss: 1.255 | Acc: 54.654% (8185/14976)\n",
      "Loss: 1.255 | Acc: 54.641% (8218/15040)\n",
      "Loss: 1.255 | Acc: 54.635% (8252/15104)\n",
      "Loss: 1.255 | Acc: 54.661% (8291/15168)\n",
      "Loss: 1.254 | Acc: 54.688% (8330/15232)\n",
      "Loss: 1.254 | Acc: 54.681% (8364/15296)\n",
      "Loss: 1.255 | Acc: 54.681% (8399/15360)\n",
      "Loss: 1.255 | Acc: 54.694% (8436/15424)\n",
      "Loss: 1.255 | Acc: 54.726% (8476/15488)\n",
      "Loss: 1.255 | Acc: 54.733% (8512/15552)\n",
      "Loss: 1.255 | Acc: 54.726% (8546/15616)\n",
      "Loss: 1.254 | Acc: 54.758% (8586/15680)\n",
      "Loss: 1.255 | Acc: 54.713% (8614/15744)\n",
      "Loss: 1.255 | Acc: 54.688% (8645/15808)\n",
      "Loss: 1.254 | Acc: 54.706% (8683/15872)\n",
      "Loss: 1.255 | Acc: 54.700% (8717/15936)\n",
      "Loss: 1.253 | Acc: 54.719% (8755/16000)\n",
      "Loss: 1.253 | Acc: 54.737% (8793/16064)\n",
      "Loss: 1.253 | Acc: 54.731% (8827/16128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.253 | Acc: 54.755% (8866/16192)\n",
      "Loss: 1.253 | Acc: 54.774% (8904/16256)\n",
      "Loss: 1.253 | Acc: 54.792% (8942/16320)\n",
      "Loss: 1.254 | Acc: 54.803% (8979/16384)\n",
      "Loss: 1.253 | Acc: 54.797% (9013/16448)\n",
      "Loss: 1.253 | Acc: 54.803% (9049/16512)\n",
      "Loss: 1.253 | Acc: 54.814% (9086/16576)\n",
      "Loss: 1.253 | Acc: 54.856% (9128/16640)\n",
      "Loss: 1.252 | Acc: 54.873% (9166/16704)\n",
      "Loss: 1.252 | Acc: 54.896% (9205/16768)\n",
      "Loss: 1.252 | Acc: 54.907% (9242/16832)\n",
      "Loss: 1.252 | Acc: 54.906% (9277/16896)\n",
      "Loss: 1.252 | Acc: 54.935% (9317/16960)\n",
      "Loss: 1.253 | Acc: 54.940% (9353/17024)\n",
      "Loss: 1.252 | Acc: 54.933% (9387/17088)\n",
      "Loss: 1.253 | Acc: 54.886% (9414/17152)\n",
      "Loss: 1.253 | Acc: 54.885% (9449/17216)\n",
      "Loss: 1.253 | Acc: 54.890% (9485/17280)\n",
      "Loss: 1.252 | Acc: 54.912% (9524/17344)\n",
      "Loss: 1.252 | Acc: 54.940% (9564/17408)\n",
      "Loss: 1.252 | Acc: 54.945% (9600/17472)\n",
      "Loss: 1.252 | Acc: 54.933% (9633/17536)\n",
      "Loss: 1.252 | Acc: 54.909% (9664/17600)\n",
      "Loss: 1.252 | Acc: 54.869% (9692/17664)\n",
      "Loss: 1.252 | Acc: 54.868% (9727/17728)\n",
      "Loss: 1.251 | Acc: 54.895% (9767/17792)\n",
      "Loss: 1.251 | Acc: 54.872% (9798/17856)\n",
      "Loss: 1.252 | Acc: 54.855% (9830/17920)\n",
      "Loss: 1.252 | Acc: 54.871% (9868/17984)\n",
      "Loss: 1.252 | Acc: 54.876% (9904/18048)\n",
      "Loss: 1.252 | Acc: 54.881% (9940/18112)\n",
      "Loss: 1.252 | Acc: 54.891% (9977/18176)\n",
      "Loss: 1.251 | Acc: 54.912% (10016/18240)\n",
      "Loss: 1.251 | Acc: 54.911% (10051/18304)\n",
      "Loss: 1.251 | Acc: 54.938% (10091/18368)\n",
      "Loss: 1.250 | Acc: 54.970% (10132/18432)\n",
      "Loss: 1.251 | Acc: 54.925% (10159/18496)\n",
      "Loss: 1.251 | Acc: 54.903% (10190/18560)\n",
      "Loss: 1.251 | Acc: 54.945% (10233/18624)\n",
      "Loss: 1.251 | Acc: 54.960% (10271/18688)\n",
      "Loss: 1.251 | Acc: 54.949% (10304/18752)\n",
      "Loss: 1.252 | Acc: 54.943% (10338/18816)\n",
      "Loss: 1.251 | Acc: 54.952% (10375/18880)\n",
      "Loss: 1.252 | Acc: 54.946% (10409/18944)\n",
      "Loss: 1.251 | Acc: 54.961% (10447/19008)\n",
      "Loss: 1.251 | Acc: 54.939% (10478/19072)\n",
      "Loss: 1.251 | Acc: 54.917% (10509/19136)\n",
      "Loss: 1.252 | Acc: 54.911% (10543/19200)\n",
      "Loss: 1.251 | Acc: 54.926% (10581/19264)\n",
      "Loss: 1.251 | Acc: 54.910% (10613/19328)\n",
      "Loss: 1.251 | Acc: 54.899% (10646/19392)\n",
      "Loss: 1.252 | Acc: 54.862% (10674/19456)\n",
      "Loss: 1.252 | Acc: 54.867% (10710/19520)\n",
      "Loss: 1.251 | Acc: 54.856% (10743/19584)\n",
      "Loss: 1.252 | Acc: 54.840% (10775/19648)\n",
      "Loss: 1.253 | Acc: 54.809% (10804/19712)\n",
      "Loss: 1.253 | Acc: 54.809% (10839/19776)\n",
      "Loss: 1.253 | Acc: 54.808% (10874/19840)\n",
      "Loss: 1.253 | Acc: 54.823% (10912/19904)\n",
      "Loss: 1.253 | Acc: 54.838% (10950/19968)\n",
      "Loss: 1.253 | Acc: 54.862% (10990/20032)\n",
      "Loss: 1.253 | Acc: 54.822% (11017/20096)\n",
      "Loss: 1.253 | Acc: 54.807% (11049/20160)\n",
      "Loss: 1.254 | Acc: 54.772% (11077/20224)\n",
      "Loss: 1.254 | Acc: 54.776% (11113/20288)\n",
      "Loss: 1.254 | Acc: 54.766% (11146/20352)\n",
      "Loss: 1.254 | Acc: 54.751% (11178/20416)\n",
      "Loss: 1.254 | Acc: 54.775% (11218/20480)\n",
      "Loss: 1.253 | Acc: 54.795% (11257/20544)\n",
      "Loss: 1.253 | Acc: 54.794% (11292/20608)\n",
      "Loss: 1.253 | Acc: 54.804% (11329/20672)\n",
      "Loss: 1.253 | Acc: 54.798% (11363/20736)\n",
      "Loss: 1.254 | Acc: 54.764% (11391/20800)\n",
      "Loss: 1.254 | Acc: 54.759% (11425/20864)\n",
      "Loss: 1.253 | Acc: 54.759% (11460/20928)\n",
      "Loss: 1.253 | Acc: 54.764% (11496/20992)\n",
      "Loss: 1.253 | Acc: 54.763% (11531/21056)\n",
      "Loss: 1.253 | Acc: 54.754% (11564/21120)\n",
      "Loss: 1.254 | Acc: 54.721% (11592/21184)\n",
      "Loss: 1.254 | Acc: 54.688% (11620/21248)\n",
      "Loss: 1.254 | Acc: 54.692% (11656/21312)\n",
      "Loss: 1.254 | Acc: 54.692% (11691/21376)\n",
      "Loss: 1.254 | Acc: 54.669% (11721/21440)\n",
      "Loss: 1.254 | Acc: 54.683% (11759/21504)\n",
      "Loss: 1.253 | Acc: 54.729% (11804/21568)\n",
      "Loss: 1.253 | Acc: 54.743% (11842/21632)\n",
      "Loss: 1.252 | Acc: 54.766% (11882/21696)\n",
      "Loss: 1.252 | Acc: 54.752% (11914/21760)\n",
      "Loss: 1.252 | Acc: 54.779% (11955/21824)\n",
      "Loss: 1.253 | Acc: 54.756% (11985/21888)\n",
      "Loss: 1.253 | Acc: 54.756% (12020/21952)\n",
      "Loss: 1.253 | Acc: 54.751% (12054/22016)\n",
      "Loss: 1.253 | Acc: 54.755% (12090/22080)\n",
      "Loss: 1.253 | Acc: 54.728% (12119/22144)\n",
      "Loss: 1.254 | Acc: 54.706% (12149/22208)\n",
      "Loss: 1.255 | Acc: 54.679% (12178/22272)\n",
      "Loss: 1.255 | Acc: 54.661% (12209/22336)\n",
      "Loss: 1.254 | Acc: 54.701% (12253/22400)\n",
      "Loss: 1.254 | Acc: 54.696% (12287/22464)\n",
      "Loss: 1.254 | Acc: 54.727% (12329/22528)\n",
      "Loss: 1.253 | Acc: 54.749% (12369/22592)\n",
      "Loss: 1.253 | Acc: 54.763% (12407/22656)\n",
      "Loss: 1.253 | Acc: 54.771% (12444/22720)\n",
      "Loss: 1.254 | Acc: 54.736% (12471/22784)\n",
      "Loss: 1.254 | Acc: 54.731% (12505/22848)\n",
      "Loss: 1.253 | Acc: 54.718% (12537/22912)\n",
      "Loss: 1.253 | Acc: 54.709% (12570/22976)\n",
      "Loss: 1.253 | Acc: 54.718% (12607/23040)\n",
      "Loss: 1.253 | Acc: 54.718% (12642/23104)\n",
      "Loss: 1.254 | Acc: 54.679% (12668/23168)\n",
      "Loss: 1.254 | Acc: 54.683% (12704/23232)\n",
      "Loss: 1.255 | Acc: 54.666% (12735/23296)\n",
      "Loss: 1.254 | Acc: 54.688% (12775/23360)\n",
      "Loss: 1.254 | Acc: 54.713% (12816/23424)\n",
      "Loss: 1.253 | Acc: 54.726% (12854/23488)\n",
      "Loss: 1.254 | Acc: 54.721% (12888/23552)\n",
      "Loss: 1.254 | Acc: 54.726% (12924/23616)\n",
      "Loss: 1.254 | Acc: 54.726% (12959/23680)\n",
      "Loss: 1.254 | Acc: 54.721% (12993/23744)\n",
      "Loss: 1.254 | Acc: 54.717% (13027/23808)\n",
      "Loss: 1.255 | Acc: 54.713% (13061/23872)\n",
      "Loss: 1.255 | Acc: 54.713% (13096/23936)\n",
      "Loss: 1.255 | Acc: 54.704% (13129/24000)\n",
      "Loss: 1.256 | Acc: 54.700% (13163/24064)\n",
      "Loss: 1.255 | Acc: 54.717% (13202/24128)\n",
      "Loss: 1.255 | Acc: 54.729% (13240/24192)\n",
      "Loss: 1.255 | Acc: 54.733% (13276/24256)\n",
      "Loss: 1.255 | Acc: 54.741% (13313/24320)\n",
      "Loss: 1.255 | Acc: 54.737% (13347/24384)\n",
      "Loss: 1.255 | Acc: 54.724% (13379/24448)\n",
      "Loss: 1.255 | Acc: 54.753% (13421/24512)\n",
      "Loss: 1.255 | Acc: 54.744% (13454/24576)\n",
      "Loss: 1.255 | Acc: 54.720% (13483/24640)\n",
      "Loss: 1.255 | Acc: 54.736% (13522/24704)\n",
      "Loss: 1.254 | Acc: 54.752% (13561/24768)\n",
      "Loss: 1.255 | Acc: 54.728% (13590/24832)\n",
      "Loss: 1.255 | Acc: 54.744% (13629/24896)\n",
      "Loss: 1.255 | Acc: 54.732% (13661/24960)\n",
      "Loss: 1.255 | Acc: 54.747% (13700/25024)\n",
      "Loss: 1.254 | Acc: 54.771% (13741/25088)\n",
      "Loss: 1.254 | Acc: 54.763% (13774/25152)\n",
      "Loss: 1.255 | Acc: 54.771% (13811/25216)\n",
      "Loss: 1.254 | Acc: 54.786% (13850/25280)\n",
      "Loss: 1.254 | Acc: 54.794% (13887/25344)\n",
      "Loss: 1.255 | Acc: 54.794% (13922/25408)\n",
      "Loss: 1.254 | Acc: 54.778% (13953/25472)\n",
      "Loss: 1.254 | Acc: 54.770% (13986/25536)\n",
      "Loss: 1.254 | Acc: 54.773% (14022/25600)\n",
      "Loss: 1.254 | Acc: 54.781% (14059/25664)\n",
      "Loss: 1.254 | Acc: 54.789% (14096/25728)\n",
      "Loss: 1.254 | Acc: 54.788% (14131/25792)\n",
      "Loss: 1.253 | Acc: 54.800% (14169/25856)\n",
      "Loss: 1.253 | Acc: 54.776% (14198/25920)\n",
      "Loss: 1.254 | Acc: 54.780% (14234/25984)\n",
      "Loss: 1.254 | Acc: 54.776% (14268/26048)\n",
      "Loss: 1.254 | Acc: 54.779% (14304/26112)\n",
      "Loss: 1.254 | Acc: 54.779% (14339/26176)\n",
      "Loss: 1.254 | Acc: 54.775% (14373/26240)\n",
      "Loss: 1.254 | Acc: 54.779% (14409/26304)\n",
      "Loss: 1.254 | Acc: 54.775% (14443/26368)\n",
      "Loss: 1.254 | Acc: 54.748% (14471/26432)\n",
      "Loss: 1.254 | Acc: 54.744% (14505/26496)\n",
      "Loss: 1.254 | Acc: 54.729% (14536/26560)\n",
      "Loss: 1.254 | Acc: 54.729% (14571/26624)\n",
      "Loss: 1.254 | Acc: 54.751% (14612/26688)\n",
      "Loss: 1.253 | Acc: 54.770% (14652/26752)\n",
      "Loss: 1.254 | Acc: 54.755% (14683/26816)\n",
      "Loss: 1.254 | Acc: 54.754% (14718/26880)\n",
      "Loss: 1.254 | Acc: 54.758% (14754/26944)\n",
      "Loss: 1.254 | Acc: 54.765% (14791/27008)\n",
      "Loss: 1.255 | Acc: 54.758% (14824/27072)\n",
      "Loss: 1.255 | Acc: 54.772% (14863/27136)\n",
      "Loss: 1.255 | Acc: 54.761% (14895/27200)\n",
      "Loss: 1.255 | Acc: 54.750% (14927/27264)\n",
      "Loss: 1.255 | Acc: 54.746% (14961/27328)\n",
      "Loss: 1.255 | Acc: 54.775% (15004/27392)\n",
      "Loss: 1.255 | Acc: 54.779% (15040/27456)\n",
      "Loss: 1.255 | Acc: 54.771% (15073/27520)\n",
      "Loss: 1.255 | Acc: 54.796% (15115/27584)\n",
      "Loss: 1.255 | Acc: 54.782% (15146/27648)\n",
      "Loss: 1.255 | Acc: 54.774% (15179/27712)\n",
      "Loss: 1.255 | Acc: 54.774% (15214/27776)\n",
      "Loss: 1.254 | Acc: 54.788% (15253/27840)\n",
      "Loss: 1.254 | Acc: 54.788% (15288/27904)\n",
      "Loss: 1.254 | Acc: 54.791% (15324/27968)\n",
      "Loss: 1.254 | Acc: 54.809% (15364/28032)\n",
      "Loss: 1.254 | Acc: 54.805% (15398/28096)\n",
      "Loss: 1.254 | Acc: 54.812% (15435/28160)\n",
      "Loss: 1.254 | Acc: 54.808% (15469/28224)\n",
      "Loss: 1.254 | Acc: 54.808% (15504/28288)\n",
      "Loss: 1.254 | Acc: 54.822% (15543/28352)\n",
      "Loss: 1.255 | Acc: 54.779% (15566/28416)\n",
      "Loss: 1.255 | Acc: 54.782% (15602/28480)\n",
      "Loss: 1.255 | Acc: 54.768% (15633/28544)\n",
      "Loss: 1.255 | Acc: 54.778% (15671/28608)\n",
      "Loss: 1.255 | Acc: 54.775% (15705/28672)\n",
      "Loss: 1.255 | Acc: 54.774% (15740/28736)\n",
      "Loss: 1.255 | Acc: 54.753% (15769/28800)\n",
      "Loss: 1.255 | Acc: 54.729% (15797/28864)\n",
      "Loss: 1.255 | Acc: 54.708% (15826/28928)\n",
      "Loss: 1.255 | Acc: 54.712% (15862/28992)\n",
      "Loss: 1.255 | Acc: 54.701% (15894/29056)\n",
      "Loss: 1.255 | Acc: 54.681% (15923/29120)\n",
      "Loss: 1.255 | Acc: 54.664% (15953/29184)\n",
      "Loss: 1.256 | Acc: 54.636% (15980/29248)\n",
      "Loss: 1.256 | Acc: 54.636% (16015/29312)\n",
      "Loss: 1.256 | Acc: 54.640% (16051/29376)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.256 | Acc: 54.647% (16088/29440)\n",
      "Loss: 1.255 | Acc: 54.643% (16122/29504)\n",
      "Loss: 1.255 | Acc: 54.660% (16162/29568)\n",
      "Loss: 1.255 | Acc: 54.667% (16199/29632)\n",
      "Loss: 1.255 | Acc: 54.677% (16237/29696)\n",
      "Loss: 1.255 | Acc: 54.657% (16266/29760)\n",
      "Loss: 1.255 | Acc: 54.654% (16300/29824)\n",
      "Loss: 1.255 | Acc: 54.661% (16337/29888)\n",
      "Loss: 1.255 | Acc: 54.677% (16377/29952)\n",
      "Loss: 1.255 | Acc: 54.674% (16411/30016)\n",
      "Loss: 1.254 | Acc: 54.694% (16452/30080)\n",
      "Loss: 1.254 | Acc: 54.684% (16484/30144)\n",
      "Loss: 1.254 | Acc: 54.691% (16521/30208)\n",
      "Loss: 1.254 | Acc: 54.701% (16559/30272)\n",
      "Loss: 1.254 | Acc: 54.701% (16594/30336)\n",
      "Loss: 1.254 | Acc: 54.720% (16635/30400)\n",
      "Loss: 1.254 | Acc: 54.727% (16672/30464)\n",
      "Loss: 1.254 | Acc: 54.730% (16708/30528)\n",
      "Loss: 1.253 | Acc: 54.743% (16747/30592)\n",
      "Loss: 1.253 | Acc: 54.749% (16784/30656)\n",
      "Loss: 1.253 | Acc: 54.730% (16813/30720)\n",
      "Loss: 1.253 | Acc: 54.726% (16847/30784)\n",
      "Loss: 1.253 | Acc: 54.733% (16884/30848)\n",
      "Loss: 1.253 | Acc: 54.730% (16918/30912)\n",
      "Loss: 1.253 | Acc: 54.733% (16954/30976)\n",
      "Loss: 1.253 | Acc: 54.742% (16992/31040)\n",
      "Loss: 1.253 | Acc: 54.749% (17029/31104)\n",
      "Loss: 1.252 | Acc: 54.761% (17068/31168)\n",
      "Loss: 1.253 | Acc: 54.758% (17102/31232)\n",
      "Loss: 1.253 | Acc: 54.751% (17135/31296)\n",
      "Loss: 1.252 | Acc: 54.777% (17178/31360)\n",
      "Loss: 1.252 | Acc: 54.793% (17218/31424)\n",
      "Loss: 1.252 | Acc: 54.786% (17251/31488)\n",
      "Loss: 1.252 | Acc: 54.795% (17289/31552)\n",
      "Loss: 1.252 | Acc: 54.770% (17316/31616)\n",
      "Loss: 1.253 | Acc: 54.779% (17354/31680)\n",
      "Loss: 1.253 | Acc: 54.779% (17389/31744)\n",
      "Loss: 1.253 | Acc: 54.766% (17420/31808)\n",
      "Loss: 1.253 | Acc: 54.782% (17460/31872)\n",
      "Loss: 1.253 | Acc: 54.766% (17490/31936)\n",
      "Loss: 1.253 | Acc: 54.769% (17526/32000)\n",
      "Loss: 1.253 | Acc: 54.756% (17557/32064)\n",
      "Loss: 1.254 | Acc: 54.750% (17590/32128)\n",
      "Loss: 1.254 | Acc: 54.740% (17622/32192)\n",
      "Loss: 1.254 | Acc: 54.756% (17662/32256)\n",
      "Loss: 1.253 | Acc: 54.759% (17698/32320)\n",
      "Loss: 1.253 | Acc: 54.762% (17734/32384)\n",
      "Loss: 1.253 | Acc: 54.755% (17767/32448)\n",
      "Loss: 1.253 | Acc: 54.746% (17799/32512)\n",
      "Loss: 1.253 | Acc: 54.770% (17842/32576)\n",
      "Loss: 1.253 | Acc: 54.752% (17871/32640)\n",
      "Loss: 1.254 | Acc: 54.746% (17904/32704)\n",
      "Loss: 1.254 | Acc: 54.724% (17932/32768)\n",
      "Loss: 1.254 | Acc: 54.730% (17969/32832)\n",
      "Loss: 1.254 | Acc: 54.724% (18002/32896)\n",
      "Loss: 1.255 | Acc: 54.706% (18031/32960)\n",
      "Loss: 1.255 | Acc: 54.697% (18063/33024)\n",
      "Loss: 1.255 | Acc: 54.712% (18103/33088)\n",
      "Loss: 1.255 | Acc: 54.703% (18135/33152)\n",
      "Loss: 1.255 | Acc: 54.709% (18172/33216)\n",
      "Loss: 1.255 | Acc: 54.712% (18208/33280)\n",
      "Loss: 1.255 | Acc: 54.696% (18238/33344)\n",
      "Loss: 1.255 | Acc: 54.685% (18269/33408)\n",
      "Loss: 1.255 | Acc: 54.685% (18304/33472)\n",
      "Loss: 1.256 | Acc: 54.667% (18333/33536)\n",
      "Loss: 1.256 | Acc: 54.685% (18374/33600)\n",
      "Loss: 1.256 | Acc: 54.693% (18412/33664)\n",
      "Loss: 1.256 | Acc: 54.673% (18440/33728)\n",
      "Loss: 1.256 | Acc: 54.688% (18480/33792)\n",
      "Loss: 1.255 | Acc: 54.693% (18517/33856)\n",
      "Loss: 1.255 | Acc: 54.688% (18550/33920)\n",
      "Loss: 1.255 | Acc: 54.685% (18584/33984)\n",
      "Loss: 1.254 | Acc: 54.726% (18633/34048)\n",
      "Loss: 1.254 | Acc: 54.731% (18670/34112)\n",
      "Loss: 1.254 | Acc: 54.746% (18710/34176)\n",
      "Loss: 1.253 | Acc: 54.763% (18751/34240)\n",
      "Loss: 1.253 | Acc: 54.752% (18782/34304)\n",
      "Loss: 1.253 | Acc: 54.749% (18816/34368)\n",
      "Loss: 1.253 | Acc: 54.766% (18857/34432)\n",
      "Loss: 1.253 | Acc: 54.777% (18896/34496)\n",
      "Loss: 1.253 | Acc: 54.780% (18932/34560)\n",
      "Loss: 1.253 | Acc: 54.791% (18971/34624)\n",
      "Loss: 1.252 | Acc: 54.797% (19008/34688)\n",
      "Loss: 1.252 | Acc: 54.791% (19041/34752)\n",
      "Loss: 1.252 | Acc: 54.794% (19077/34816)\n",
      "Loss: 1.252 | Acc: 54.794% (19112/34880)\n",
      "Loss: 1.252 | Acc: 54.788% (19145/34944)\n",
      "Loss: 1.252 | Acc: 54.787% (19180/35008)\n",
      "Loss: 1.252 | Acc: 54.804% (19221/35072)\n",
      "Loss: 1.252 | Acc: 54.818% (19261/35136)\n",
      "Loss: 1.252 | Acc: 54.815% (19295/35200)\n",
      "Loss: 1.252 | Acc: 54.818% (19331/35264)\n",
      "Loss: 1.251 | Acc: 54.821% (19367/35328)\n",
      "Loss: 1.251 | Acc: 54.820% (19402/35392)\n",
      "Loss: 1.251 | Acc: 54.826% (19439/35456)\n",
      "Loss: 1.252 | Acc: 54.814% (19470/35520)\n",
      "Loss: 1.252 | Acc: 54.797% (19499/35584)\n",
      "Loss: 1.251 | Acc: 54.811% (19539/35648)\n",
      "Loss: 1.251 | Acc: 54.814% (19575/35712)\n",
      "Loss: 1.252 | Acc: 54.810% (19609/35776)\n",
      "Loss: 1.252 | Acc: 54.805% (19642/35840)\n",
      "Loss: 1.252 | Acc: 54.788% (19671/35904)\n",
      "Loss: 1.252 | Acc: 54.799% (19710/35968)\n",
      "Loss: 1.252 | Acc: 54.785% (19740/36032)\n",
      "Loss: 1.252 | Acc: 54.784% (19775/36096)\n",
      "Loss: 1.252 | Acc: 54.806% (19818/36160)\n",
      "Loss: 1.252 | Acc: 54.803% (19852/36224)\n",
      "Loss: 1.251 | Acc: 54.831% (19897/36288)\n",
      "Loss: 1.251 | Acc: 54.831% (19932/36352)\n",
      "Loss: 1.251 | Acc: 54.822% (19964/36416)\n",
      "Loss: 1.251 | Acc: 54.825% (20000/36480)\n",
      "Loss: 1.251 | Acc: 54.824% (20035/36544)\n",
      "Loss: 1.252 | Acc: 54.810% (20065/36608)\n",
      "Loss: 1.252 | Acc: 54.813% (20101/36672)\n",
      "Loss: 1.251 | Acc: 54.815% (20137/36736)\n",
      "Loss: 1.251 | Acc: 54.807% (20169/36800)\n",
      "Loss: 1.251 | Acc: 54.810% (20205/36864)\n",
      "Loss: 1.251 | Acc: 54.807% (20239/36928)\n",
      "Loss: 1.252 | Acc: 54.790% (20268/36992)\n",
      "Loss: 1.252 | Acc: 54.777% (20298/37056)\n",
      "Loss: 1.252 | Acc: 54.744% (20321/37120)\n",
      "Loss: 1.252 | Acc: 54.731% (20351/37184)\n",
      "Loss: 1.253 | Acc: 54.717% (20381/37248)\n",
      "Loss: 1.253 | Acc: 54.717% (20416/37312)\n",
      "Loss: 1.253 | Acc: 54.709% (20448/37376)\n",
      "Loss: 1.253 | Acc: 54.701% (20480/37440)\n",
      "Loss: 1.253 | Acc: 54.714% (20520/37504)\n",
      "Loss: 1.252 | Acc: 54.709% (20553/37568)\n",
      "Loss: 1.252 | Acc: 54.706% (20587/37632)\n",
      "Loss: 1.252 | Acc: 54.703% (20621/37696)\n",
      "Loss: 1.253 | Acc: 54.706% (20657/37760)\n",
      "Loss: 1.253 | Acc: 54.703% (20691/37824)\n",
      "Loss: 1.252 | Acc: 54.711% (20729/37888)\n",
      "Loss: 1.252 | Acc: 54.724% (20769/37952)\n",
      "Loss: 1.252 | Acc: 54.727% (20805/38016)\n",
      "Loss: 1.252 | Acc: 54.730% (20841/38080)\n",
      "Loss: 1.252 | Acc: 54.743% (20881/38144)\n",
      "Loss: 1.252 | Acc: 54.748% (20918/38208)\n",
      "Loss: 1.251 | Acc: 54.753% (20955/38272)\n",
      "Loss: 1.251 | Acc: 54.758% (20992/38336)\n",
      "Loss: 1.251 | Acc: 54.768% (21031/38400)\n",
      "Loss: 1.251 | Acc: 54.784% (21072/38464)\n",
      "Loss: 1.251 | Acc: 54.776% (21104/38528)\n",
      "Loss: 1.252 | Acc: 54.765% (21135/38592)\n",
      "Loss: 1.252 | Acc: 54.770% (21172/38656)\n",
      "Loss: 1.252 | Acc: 54.762% (21204/38720)\n",
      "Loss: 1.251 | Acc: 54.773% (21243/38784)\n",
      "Loss: 1.251 | Acc: 54.775% (21279/38848)\n",
      "Loss: 1.251 | Acc: 54.788% (21319/38912)\n",
      "Loss: 1.251 | Acc: 54.795% (21357/38976)\n",
      "Loss: 1.251 | Acc: 54.800% (21394/39040)\n",
      "Loss: 1.251 | Acc: 54.800% (21429/39104)\n",
      "Loss: 1.251 | Acc: 54.797% (21463/39168)\n",
      "Loss: 1.251 | Acc: 54.812% (21504/39232)\n",
      "Loss: 1.251 | Acc: 54.822% (21543/39296)\n",
      "Loss: 1.251 | Acc: 54.817% (21576/39360)\n",
      "Loss: 1.251 | Acc: 54.812% (21609/39424)\n",
      "Loss: 1.251 | Acc: 54.801% (21640/39488)\n",
      "Loss: 1.251 | Acc: 54.796% (21673/39552)\n",
      "Loss: 1.251 | Acc: 54.799% (21709/39616)\n",
      "Loss: 1.251 | Acc: 54.793% (21742/39680)\n",
      "Loss: 1.251 | Acc: 54.806% (21782/39744)\n",
      "Loss: 1.251 | Acc: 54.803% (21816/39808)\n",
      "Loss: 1.251 | Acc: 54.798% (21849/39872)\n",
      "Loss: 1.251 | Acc: 54.800% (21885/39936)\n",
      "Loss: 1.251 | Acc: 54.812% (21925/40000)\n",
      "Loss: 1.251 | Acc: 54.822% (21964/40064)\n",
      "Loss: 1.250 | Acc: 54.835% (22004/40128)\n",
      "Loss: 1.250 | Acc: 54.842% (22042/40192)\n",
      "Loss: 1.250 | Acc: 54.849% (22080/40256)\n",
      "Loss: 1.250 | Acc: 54.864% (22121/40320)\n",
      "Loss: 1.250 | Acc: 54.881% (22163/40384)\n",
      "Loss: 1.250 | Acc: 54.885% (22200/40448)\n",
      "Loss: 1.250 | Acc: 54.892% (22238/40512)\n",
      "Loss: 1.250 | Acc: 54.882% (22269/40576)\n",
      "Loss: 1.250 | Acc: 54.897% (22310/40640)\n",
      "Loss: 1.250 | Acc: 54.886% (22341/40704)\n",
      "Loss: 1.250 | Acc: 54.884% (22375/40768)\n",
      "Loss: 1.250 | Acc: 54.893% (22414/40832)\n",
      "Loss: 1.250 | Acc: 54.888% (22447/40896)\n",
      "Loss: 1.250 | Acc: 54.878% (22478/40960)\n",
      "Loss: 1.250 | Acc: 54.873% (22511/41024)\n",
      "Loss: 1.249 | Acc: 54.877% (22548/41088)\n",
      "Loss: 1.249 | Acc: 54.877% (22583/41152)\n",
      "Loss: 1.249 | Acc: 54.869% (22615/41216)\n",
      "Loss: 1.249 | Acc: 54.874% (22652/41280)\n",
      "Loss: 1.249 | Acc: 54.874% (22687/41344)\n",
      "Loss: 1.249 | Acc: 54.866% (22719/41408)\n",
      "Loss: 1.249 | Acc: 54.861% (22752/41472)\n",
      "Loss: 1.249 | Acc: 54.854% (22784/41536)\n",
      "Loss: 1.249 | Acc: 54.856% (22820/41600)\n",
      "Loss: 1.250 | Acc: 54.839% (22848/41664)\n",
      "Loss: 1.250 | Acc: 54.829% (22879/41728)\n",
      "Loss: 1.250 | Acc: 54.819% (22910/41792)\n",
      "Loss: 1.250 | Acc: 54.821% (22946/41856)\n",
      "Loss: 1.250 | Acc: 54.826% (22983/41920)\n",
      "Loss: 1.250 | Acc: 54.826% (23018/41984)\n",
      "Loss: 1.250 | Acc: 54.823% (23052/42048)\n",
      "Loss: 1.250 | Acc: 54.823% (23087/42112)\n",
      "Loss: 1.250 | Acc: 54.823% (23122/42176)\n",
      "Loss: 1.250 | Acc: 54.820% (23156/42240)\n",
      "Loss: 1.249 | Acc: 54.832% (23196/42304)\n",
      "Loss: 1.250 | Acc: 54.820% (23226/42368)\n",
      "Loss: 1.250 | Acc: 54.812% (23258/42432)\n",
      "Loss: 1.250 | Acc: 54.812% (23293/42496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.250 | Acc: 54.810% (23327/42560)\n",
      "Loss: 1.250 | Acc: 54.814% (23364/42624)\n",
      "Loss: 1.250 | Acc: 54.823% (23403/42688)\n",
      "Loss: 1.249 | Acc: 54.835% (23443/42752)\n",
      "Loss: 1.249 | Acc: 54.832% (23477/42816)\n",
      "Loss: 1.249 | Acc: 54.839% (23515/42880)\n",
      "Loss: 1.249 | Acc: 54.851% (23555/42944)\n",
      "Loss: 1.249 | Acc: 54.855% (23592/43008)\n",
      "Loss: 1.249 | Acc: 54.850% (23625/43072)\n",
      "Loss: 1.249 | Acc: 54.841% (23656/43136)\n",
      "Loss: 1.249 | Acc: 54.850% (23695/43200)\n",
      "Loss: 1.249 | Acc: 54.847% (23729/43264)\n",
      "Loss: 1.250 | Acc: 54.838% (23760/43328)\n",
      "Loss: 1.250 | Acc: 54.844% (23798/43392)\n",
      "Loss: 1.250 | Acc: 54.846% (23834/43456)\n",
      "Loss: 1.250 | Acc: 54.844% (23868/43520)\n",
      "Loss: 1.250 | Acc: 54.850% (23906/43584)\n",
      "Loss: 1.249 | Acc: 54.859% (23945/43648)\n",
      "Loss: 1.249 | Acc: 54.866% (23983/43712)\n",
      "Loss: 1.249 | Acc: 54.868% (24019/43776)\n",
      "Loss: 1.249 | Acc: 54.868% (24054/43840)\n",
      "Loss: 1.249 | Acc: 54.870% (24090/43904)\n",
      "Loss: 1.249 | Acc: 54.874% (24127/43968)\n",
      "Loss: 1.249 | Acc: 54.885% (24167/44032)\n",
      "Loss: 1.249 | Acc: 54.883% (24201/44096)\n",
      "Loss: 1.249 | Acc: 54.866% (24229/44160)\n",
      "Loss: 1.249 | Acc: 54.862% (24262/44224)\n",
      "Loss: 1.249 | Acc: 54.859% (24296/44288)\n",
      "Loss: 1.249 | Acc: 54.854% (24329/44352)\n",
      "Loss: 1.249 | Acc: 54.854% (24364/44416)\n",
      "Loss: 1.249 | Acc: 54.856% (24400/44480)\n",
      "Loss: 1.250 | Acc: 54.842% (24429/44544)\n",
      "Loss: 1.249 | Acc: 54.851% (24468/44608)\n",
      "Loss: 1.249 | Acc: 54.855% (24505/44672)\n",
      "Loss: 1.249 | Acc: 54.869% (24546/44736)\n",
      "Loss: 1.249 | Acc: 54.864% (24579/44800)\n",
      "Loss: 1.249 | Acc: 54.868% (24616/44864)\n",
      "Loss: 1.249 | Acc: 54.870% (24652/44928)\n",
      "Loss: 1.249 | Acc: 54.879% (24691/44992)\n",
      "Loss: 1.249 | Acc: 54.872% (24723/45056)\n",
      "Loss: 1.249 | Acc: 54.876% (24760/45120)\n",
      "Loss: 1.249 | Acc: 54.880% (24797/45184)\n",
      "Loss: 1.249 | Acc: 54.889% (24836/45248)\n",
      "Loss: 1.248 | Acc: 54.895% (24874/45312)\n",
      "Loss: 1.248 | Acc: 54.899% (24911/45376)\n",
      "Loss: 1.248 | Acc: 54.894% (24944/45440)\n",
      "Loss: 1.249 | Acc: 54.890% (24977/45504)\n",
      "Loss: 1.249 | Acc: 54.881% (25008/45568)\n",
      "Loss: 1.249 | Acc: 54.880% (25043/45632)\n",
      "Loss: 1.249 | Acc: 54.889% (25082/45696)\n",
      "Loss: 1.249 | Acc: 54.880% (25113/45760)\n",
      "Loss: 1.249 | Acc: 54.880% (25148/45824)\n",
      "Loss: 1.249 | Acc: 54.890% (25188/45888)\n",
      "Loss: 1.249 | Acc: 54.886% (25221/45952)\n",
      "Loss: 1.249 | Acc: 54.881% (25254/46016)\n",
      "Loss: 1.249 | Acc: 54.876% (25287/46080)\n",
      "Loss: 1.249 | Acc: 54.887% (25327/46144)\n",
      "Loss: 1.249 | Acc: 54.880% (25359/46208)\n",
      "Loss: 1.249 | Acc: 54.871% (25390/46272)\n",
      "Loss: 1.249 | Acc: 54.875% (25427/46336)\n",
      "Loss: 1.249 | Acc: 54.879% (25464/46400)\n",
      "Loss: 1.248 | Acc: 54.890% (25504/46464)\n",
      "Loss: 1.248 | Acc: 54.890% (25539/46528)\n",
      "Loss: 1.249 | Acc: 54.883% (25571/46592)\n",
      "Loss: 1.249 | Acc: 54.876% (25603/46656)\n",
      "Loss: 1.249 | Acc: 54.874% (25637/46720)\n",
      "Loss: 1.248 | Acc: 54.893% (25681/46784)\n",
      "Loss: 1.249 | Acc: 54.886% (25713/46848)\n",
      "Loss: 1.248 | Acc: 54.896% (25753/46912)\n",
      "Loss: 1.248 | Acc: 54.894% (25787/46976)\n",
      "Loss: 1.248 | Acc: 54.881% (25816/47040)\n",
      "Loss: 1.248 | Acc: 54.887% (25854/47104)\n",
      "Loss: 1.248 | Acc: 54.883% (25887/47168)\n",
      "Loss: 1.248 | Acc: 54.880% (25921/47232)\n",
      "Loss: 1.248 | Acc: 54.880% (25956/47296)\n",
      "Loss: 1.248 | Acc: 54.869% (25986/47360)\n",
      "Loss: 1.248 | Acc: 54.850% (26012/47424)\n",
      "Loss: 1.249 | Acc: 54.852% (26048/47488)\n",
      "Loss: 1.249 | Acc: 54.847% (26081/47552)\n",
      "Loss: 1.249 | Acc: 54.849% (26117/47616)\n",
      "Loss: 1.249 | Acc: 54.845% (26150/47680)\n",
      "Loss: 1.249 | Acc: 54.838% (26182/47744)\n",
      "Loss: 1.248 | Acc: 54.853% (26224/47808)\n",
      "Loss: 1.248 | Acc: 54.855% (26260/47872)\n",
      "Loss: 1.248 | Acc: 54.865% (26300/47936)\n",
      "Loss: 1.248 | Acc: 54.852% (26329/48000)\n",
      "Loss: 1.248 | Acc: 54.858% (26367/48064)\n",
      "Loss: 1.248 | Acc: 54.868% (26407/48128)\n",
      "Loss: 1.248 | Acc: 54.864% (26440/48192)\n",
      "Loss: 1.248 | Acc: 54.855% (26471/48256)\n",
      "Loss: 1.248 | Acc: 54.853% (26505/48320)\n",
      "Loss: 1.248 | Acc: 54.855% (26541/48384)\n",
      "Loss: 1.248 | Acc: 54.863% (26580/48448)\n",
      "Loss: 1.248 | Acc: 54.865% (26616/48512)\n",
      "Loss: 1.248 | Acc: 54.860% (26649/48576)\n",
      "Loss: 1.248 | Acc: 54.870% (26689/48640)\n",
      "Loss: 1.248 | Acc: 54.876% (26727/48704)\n",
      "Loss: 1.248 | Acc: 54.884% (26766/48768)\n",
      "Loss: 1.248 | Acc: 54.884% (26801/48832)\n",
      "Loss: 1.248 | Acc: 54.872% (26830/48896)\n",
      "Loss: 1.248 | Acc: 54.867% (26863/48960)\n",
      "Loss: 1.248 | Acc: 54.859% (26881/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 54.85918367346939\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.184 | Acc: 57.812% (37/64)\n",
      "Loss: 1.123 | Acc: 57.031% (73/128)\n",
      "Loss: 1.206 | Acc: 54.688% (105/192)\n",
      "Loss: 1.249 | Acc: 53.125% (136/256)\n",
      "Loss: 1.211 | Acc: 55.312% (177/320)\n",
      "Loss: 1.239 | Acc: 54.167% (208/384)\n",
      "Loss: 1.274 | Acc: 52.679% (236/448)\n",
      "Loss: 1.262 | Acc: 52.539% (269/512)\n",
      "Loss: 1.239 | Acc: 53.125% (306/576)\n",
      "Loss: 1.227 | Acc: 53.281% (341/640)\n",
      "Loss: 1.239 | Acc: 53.267% (375/704)\n",
      "Loss: 1.241 | Acc: 53.125% (408/768)\n",
      "Loss: 1.255 | Acc: 51.923% (432/832)\n",
      "Loss: 1.253 | Acc: 52.455% (470/896)\n",
      "Loss: 1.241 | Acc: 52.812% (507/960)\n",
      "Loss: 1.229 | Acc: 53.027% (543/1024)\n",
      "Loss: 1.231 | Acc: 52.482% (571/1088)\n",
      "Loss: 1.227 | Acc: 52.778% (608/1152)\n",
      "Loss: 1.216 | Acc: 53.701% (653/1216)\n",
      "Loss: 1.230 | Acc: 53.203% (681/1280)\n",
      "Loss: 1.230 | Acc: 52.902% (711/1344)\n",
      "Loss: 1.226 | Acc: 52.983% (746/1408)\n",
      "Loss: 1.217 | Acc: 53.329% (785/1472)\n",
      "Loss: 1.223 | Acc: 53.320% (819/1536)\n",
      "Loss: 1.223 | Acc: 53.438% (855/1600)\n",
      "Loss: 1.222 | Acc: 53.606% (892/1664)\n",
      "Loss: 1.218 | Acc: 53.819% (930/1728)\n",
      "Loss: 1.218 | Acc: 53.850% (965/1792)\n",
      "Loss: 1.218 | Acc: 53.933% (1001/1856)\n",
      "Loss: 1.217 | Acc: 54.271% (1042/1920)\n",
      "Loss: 1.215 | Acc: 54.435% (1080/1984)\n",
      "Loss: 1.215 | Acc: 54.443% (1115/2048)\n",
      "Loss: 1.211 | Acc: 54.403% (1149/2112)\n",
      "Loss: 1.212 | Acc: 54.182% (1179/2176)\n",
      "Loss: 1.209 | Acc: 54.286% (1216/2240)\n",
      "Loss: 1.209 | Acc: 54.427% (1254/2304)\n",
      "Loss: 1.212 | Acc: 54.392% (1288/2368)\n",
      "Loss: 1.218 | Acc: 54.359% (1322/2432)\n",
      "Loss: 1.216 | Acc: 54.247% (1354/2496)\n",
      "Loss: 1.228 | Acc: 54.023% (1383/2560)\n",
      "Loss: 1.228 | Acc: 53.925% (1415/2624)\n",
      "Loss: 1.227 | Acc: 54.092% (1454/2688)\n",
      "Loss: 1.227 | Acc: 53.997% (1486/2752)\n",
      "Loss: 1.229 | Acc: 53.977% (1520/2816)\n",
      "Loss: 1.232 | Acc: 54.028% (1556/2880)\n",
      "Loss: 1.229 | Acc: 54.246% (1597/2944)\n",
      "Loss: 1.227 | Acc: 54.521% (1640/3008)\n",
      "Loss: 1.225 | Acc: 54.590% (1677/3072)\n",
      "Loss: 1.226 | Acc: 54.592% (1712/3136)\n",
      "Loss: 1.227 | Acc: 54.438% (1742/3200)\n",
      "Loss: 1.230 | Acc: 54.412% (1776/3264)\n",
      "Loss: 1.230 | Acc: 54.477% (1813/3328)\n",
      "Loss: 1.229 | Acc: 54.452% (1847/3392)\n",
      "Loss: 1.231 | Acc: 54.398% (1880/3456)\n",
      "Loss: 1.233 | Acc: 54.290% (1911/3520)\n",
      "Loss: 1.231 | Acc: 54.353% (1948/3584)\n",
      "Loss: 1.234 | Acc: 54.304% (1981/3648)\n",
      "Loss: 1.232 | Acc: 54.526% (2024/3712)\n",
      "Loss: 1.236 | Acc: 54.396% (2054/3776)\n",
      "Loss: 1.236 | Acc: 54.401% (2089/3840)\n",
      "Loss: 1.234 | Acc: 54.380% (2123/3904)\n",
      "Loss: 1.233 | Acc: 54.335% (2156/3968)\n",
      "Loss: 1.235 | Acc: 54.365% (2192/4032)\n",
      "Loss: 1.236 | Acc: 54.443% (2230/4096)\n",
      "Loss: 1.237 | Acc: 54.423% (2264/4160)\n",
      "Loss: 1.234 | Acc: 54.522% (2303/4224)\n",
      "Loss: 1.236 | Acc: 54.361% (2331/4288)\n",
      "Loss: 1.235 | Acc: 54.389% (2367/4352)\n",
      "Loss: 1.233 | Acc: 54.484% (2406/4416)\n",
      "Loss: 1.232 | Acc: 54.487% (2441/4480)\n",
      "Loss: 1.230 | Acc: 54.511% (2477/4544)\n",
      "Loss: 1.232 | Acc: 54.492% (2511/4608)\n",
      "Loss: 1.229 | Acc: 54.602% (2551/4672)\n",
      "Loss: 1.225 | Acc: 54.730% (2592/4736)\n",
      "Loss: 1.225 | Acc: 54.667% (2624/4800)\n",
      "Loss: 1.224 | Acc: 54.770% (2664/4864)\n",
      "Loss: 1.223 | Acc: 54.870% (2704/4928)\n",
      "Loss: 1.223 | Acc: 54.848% (2738/4992)\n",
      "Loss: 1.221 | Acc: 54.964% (2779/5056)\n",
      "Loss: 1.225 | Acc: 54.941% (2813/5120)\n",
      "Loss: 1.225 | Acc: 54.938% (2848/5184)\n",
      "Loss: 1.226 | Acc: 54.821% (2877/5248)\n",
      "Loss: 1.225 | Acc: 54.838% (2913/5312)\n",
      "Loss: 1.227 | Acc: 54.818% (2947/5376)\n",
      "Loss: 1.228 | Acc: 54.688% (2975/5440)\n",
      "Loss: 1.230 | Acc: 54.615% (3006/5504)\n",
      "Loss: 1.231 | Acc: 54.490% (3034/5568)\n",
      "Loss: 1.233 | Acc: 54.528% (3071/5632)\n",
      "Loss: 1.234 | Acc: 54.565% (3108/5696)\n",
      "Loss: 1.231 | Acc: 54.653% (3148/5760)\n",
      "Loss: 1.230 | Acc: 54.705% (3186/5824)\n",
      "Loss: 1.231 | Acc: 54.620% (3216/5888)\n",
      "Loss: 1.232 | Acc: 54.587% (3249/5952)\n",
      "Loss: 1.232 | Acc: 54.604% (3285/6016)\n",
      "Loss: 1.232 | Acc: 54.556% (3317/6080)\n",
      "Loss: 1.231 | Acc: 54.557% (3352/6144)\n",
      "Loss: 1.231 | Acc: 54.655% (3393/6208)\n",
      "Loss: 1.234 | Acc: 54.608% (3425/6272)\n",
      "Loss: 1.234 | Acc: 54.656% (3463/6336)\n",
      "Loss: 1.235 | Acc: 54.562% (3492/6400)\n",
      "Loss: 1.236 | Acc: 54.564% (3527/6464)\n",
      "Loss: 1.236 | Acc: 54.596% (3564/6528)\n",
      "Loss: 1.238 | Acc: 54.566% (3597/6592)\n",
      "Loss: 1.237 | Acc: 54.537% (3630/6656)\n",
      "Loss: 1.236 | Acc: 54.554% (3666/6720)\n",
      "Loss: 1.236 | Acc: 54.570% (3702/6784)\n",
      "Loss: 1.233 | Acc: 54.614% (3740/6848)\n",
      "Loss: 1.237 | Acc: 54.557% (3771/6912)\n",
      "Loss: 1.239 | Acc: 54.501% (3802/6976)\n",
      "Loss: 1.240 | Acc: 54.418% (3831/7040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.240 | Acc: 54.406% (3865/7104)\n",
      "Loss: 1.240 | Acc: 54.422% (3901/7168)\n",
      "Loss: 1.240 | Acc: 54.397% (3934/7232)\n",
      "Loss: 1.238 | Acc: 54.468% (3974/7296)\n",
      "Loss: 1.236 | Acc: 54.552% (4015/7360)\n",
      "Loss: 1.235 | Acc: 54.512% (4047/7424)\n",
      "Loss: 1.234 | Acc: 54.487% (4080/7488)\n",
      "Loss: 1.234 | Acc: 54.529% (4118/7552)\n",
      "Loss: 1.235 | Acc: 54.451% (4147/7616)\n",
      "Loss: 1.234 | Acc: 54.479% (4184/7680)\n",
      "Loss: 1.233 | Acc: 54.571% (4226/7744)\n",
      "Loss: 1.233 | Acc: 54.611% (4264/7808)\n",
      "Loss: 1.235 | Acc: 54.548% (4294/7872)\n",
      "Loss: 1.234 | Acc: 54.574% (4331/7936)\n",
      "Loss: 1.235 | Acc: 54.625% (4370/8000)\n",
      "Loss: 1.235 | Acc: 54.601% (4403/8064)\n",
      "Loss: 1.235 | Acc: 54.577% (4436/8128)\n",
      "Loss: 1.235 | Acc: 54.614% (4474/8192)\n",
      "Loss: 1.235 | Acc: 54.603% (4508/8256)\n",
      "Loss: 1.237 | Acc: 54.555% (4539/8320)\n",
      "Loss: 1.237 | Acc: 54.568% (4575/8384)\n",
      "Loss: 1.237 | Acc: 54.534% (4607/8448)\n",
      "Loss: 1.238 | Acc: 54.500% (4639/8512)\n",
      "Loss: 1.239 | Acc: 54.478% (4672/8576)\n",
      "Loss: 1.238 | Acc: 54.479% (4707/8640)\n",
      "Loss: 1.239 | Acc: 54.412% (4736/8704)\n",
      "Loss: 1.239 | Acc: 54.391% (4769/8768)\n",
      "Loss: 1.239 | Acc: 54.404% (4805/8832)\n",
      "Loss: 1.238 | Acc: 54.429% (4842/8896)\n",
      "Loss: 1.238 | Acc: 54.464% (4880/8960)\n",
      "Loss: 1.238 | Acc: 54.444% (4913/9024)\n",
      "Loss: 1.240 | Acc: 54.335% (4938/9088)\n",
      "Loss: 1.240 | Acc: 54.360% (4975/9152)\n",
      "Loss: 1.239 | Acc: 54.405% (5014/9216)\n",
      "Loss: 1.238 | Acc: 54.440% (5052/9280)\n",
      "Loss: 1.237 | Acc: 54.463% (5089/9344)\n",
      "Loss: 1.238 | Acc: 54.496% (5127/9408)\n",
      "Loss: 1.239 | Acc: 54.466% (5159/9472)\n",
      "Loss: 1.238 | Acc: 54.509% (5198/9536)\n",
      "Loss: 1.237 | Acc: 54.583% (5240/9600)\n",
      "Loss: 1.237 | Acc: 54.584% (5275/9664)\n",
      "Loss: 1.237 | Acc: 54.595% (5311/9728)\n",
      "Loss: 1.238 | Acc: 54.555% (5342/9792)\n",
      "Loss: 1.238 | Acc: 54.485% (5370/9856)\n",
      "Loss: 1.239 | Acc: 54.456% (5402/9920)\n",
      "Loss: 1.239 | Acc: 54.457% (5437/9984)\n",
      "Loss: 1.238 | Acc: 54.460% (5446/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 54.46\n",
      "\n",
      "Final train set accuracy is 54.85918367346939\n",
      "Final test set accuracy is 54.46\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers=num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, \n",
    "            bias=False).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d0400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
